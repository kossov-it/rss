{"title":"Hacker News","items":[{"id":"https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/","title":"Maybe the default settings are too high","link":"https://www.raptitude.com/2025/12/maybe-the-default-settings-are-too-high/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46387657","content":"<a href=\"https://news.ycombinator.com/item?id=46387657\">Comments</a>","date":1766704401000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false},{"id":"https://www.minimaxi.com/news/minimax-m21","title":"MiniMax M2.1: Built for Real-World Complex Tasks, Multi-Language Programming","link":"https://www.minimaxi.com/news/minimax-m21","hnCommentsUrl":"https://news.ycombinator.com/item?id=46388213","content":"<a href=\"https://news.ycombinator.com/item?id=46388213\">Comments</a>","date":1766710973000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div><div><p><a href=\"https://www.minimax.io/\"><img alt=\"MiniMax\" fetchpriority=\"high\" width=\"140\" height=\"32\" decoding=\"async\" data-nimg=\"1\" src=\"https://filecdn.minimax.chat/public/969d635c-cab6-45cc-8d61-47c9fe40c81f.png?x-oss-process=image/format,webp\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><div><p><img alt=\"https://filecdn.minimax.chat/public/1d46c2a2-6c36-4739-b820-cee832d1ecf9.png\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://filecdn.minimax.chat/public/1d46c2a2-6c36-4739-b820-cee832d1ecf9.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><div><p> MiniMax has been continuously transforming itself in a more AI-native way. The core driving forces of this process are models, Agent scaffolding, and organization. Throughout the exploration process, we have gained increasingly deeper understanding of these three aspects. Today we are releasing updates to the model component, namely MiniMax M2.1, hoping to help more enterprises and individuals find more AI-native ways of working (and living) sooner.</p></div><div><p>In M2, we primarily addressed issues of model cost and model accessibility. In M2.1, we are committed to improving performance in real-world complex tasks: focusing particularly on usability across more programming languages and office scenarios, and achieving the best level in this domain.</p></div><div><p><img alt=\"icon\" loading=\"lazy\" width=\"24\" height=\"24\" decoding=\"async\" data-nimg=\"1\" src=\"https://filecdn.minimax.chat/public/a533807b-9ff6-4f86-a72e-8eb31d0245c1.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><div><p><strong>Key Highlights of MiniMax M2.1:</strong></p><ul> <li><strong>Exceptional Multi-Programming Language Capabilities</strong><br>Many models in the past primarily focused on Python optimization, but real-world systems are often the result of multi-language collaboration.<br> In M2.1, we have systematically enhanced capabilities in Rust, Java, Golang, C++, Kotlin, Objective-C, TypeScript, JavaScript, and other languages. The overall performance on multi-language tasks has reached industry-leading levels, covering the complete chain from low-level system development to application layer development. </li> <li><strong>WebDev and AppDev: A Comprehensive Leap in Capability and Aesthetics</strong><br>Addressing the widely recognized weakness in mobile development across the industry, M2.1 significantly strengthens native Android and iOS development capabilities.<br> Meanwhile, we have systematically enhanced the model's design comprehension and aesthetic expression in Web and App scenarios, enabling excellent construction of complex interactions, 3D scientific scene simulations, and high-quality visualization, making vibe coding a sustainable and deliverable production practice. </li> <li><strong>Enhanced Composite Instruction Constraints, Enabling Office Scenarios</strong><br> As one of the first open-source model series to systematically introduce Interleaved Thinking, M2.1's systematic problem-solving capabilities have been further upgraded. The model not only focuses on code execution correctness but also emphasizes integrated execution of \"composite instruction constraints,\" providing higher usability in real office scenarios.</li> <li><strong>More Concise and Efficient Responses</strong><br> Compared to M2, MiniMax-M2.1 delivers more concise model responses and thought chains. In practical programming and interaction experiences, response speed has significantly improved and token consumption has notably decreased, resulting in smoother and more efficient performance in AI Coding and Agent-driven continuous workflows.</li> <li><strong>Outstanding Agent/Tool Scaffolding Generalization Capabilities</strong><br>M2.1 demonstrates excellent performance across various programming tools and Agent frameworks. It exhibits consistent and stable results in tools such as Claude Code, Droid (Factory AI), Cline, Kilo Code, Roo Code, and BlackBox, while providing reliable support for Context Management mechanisms including Skill.md, Claude.md/agent.md/cursorrule, and Slash Commands. </li> <li><strong>High-Quality Dialogue and Writing</strong><br> M2.1 is no longer just \"stronger in coding capabilities.\" In everyday conversation, technical documentation, and writing scenarios, it also provides more detailed and structured responses.</li> </ul></div><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/5c13032c-d33a-4e6f-be03-bac1606294a2.PNG\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><h3>First Impressions</h3><div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://factory.ai/\"><img alt=\"Factory AI (Droid)\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/87207db5-98a2-4236-83ef-5a124af574b5.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>\"We're excited for powerful open-source models like M2.1 that bring frontier performance (and in some cases exceed the frontier) for a wide variety of software development tasks. Developers deserve choice, and M2.1 provides that much needed choice!\"</p><div><p><span>E</span></p><div><p>Eno Reyes</p><p>Co-Founder, CTO of Factory AI</p></div></div></div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://fireworks.ai/\"><img alt=\"Fireworks\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/20ab951e-21b5-4144-925b-14f6597b5d1c.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>‚ÄúMiniMax M2.1 performed exceptionally well across our internal benchmarks, showing strong results in complex instruction following, reranking, and classification, especially within e-commerce tasks. Beyond its general versatility, it has proven to be an excellent model for coding. We are impressed by these results and look forward to a close collaboration with the MiniMax team as we continue to support their latest innovations on the Fireworks platform.‚Äù</p><div><p><span>B</span></p><div><p>Benny Chen</p><p>Co-founder of Fireworks</p></div></div></div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cline.bot/\"><img alt=\"Cline\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/12998d55-6f9c-4f97-be6a-37847f004609.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>‚ÄúMinimax M2 series has demonstrated powerful code generation capability, and has quickly became one of the most popular model on Cline platform during the past few months. We already see another huge advancement in capability for M2.1 and very excited to continue partner with minimax team to advance AI in coding‚Äù</p><div><p><span>S</span></p><div><p>Saoud Rizwan</p><p>Founder, CEO of Cline</p></div></div></div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://kilo.ai/\"><img alt=\"Kilo\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/e26bd78c-3ab9-4fd9-9c31-72d6d37d829d.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>‚ÄúWe could not be more excited about M2.1! Our users have come to rely on MiniMax for frontier-grade coding assistance at a fraction of the cost, and early testing shows M2.1 excelling at everything from architecture and orchestration to code reviews and deployment. The speed and efficiency are off the charts!‚Äù</p><div><p><span>S</span></p><div><p>Scott Breitenother</p><p>Co-Founder, CEO of Kilo</p></div></div></div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://roocode.com/\"><img alt=\"RooCode\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/0890153c-df2f-456b-bbb3-538cf95c3c36.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>\"Our users love MiniMax M2 for its strong coding ability and efficiency. The latest M2.1 release builds on that foundation with meaningful improvements in speed and reliability, performing well across a wider range of languages and frameworks. It's a great choice for high-throughput, agentic coding workflows where speed and affordability matter.\"</p><div><p><span>M</span></p><div><p>Matt Rubens</p><p>Co-Founder, CEO of RooCode</p></div></div></div><div><div><p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://www.blackbox.ai/\"><img alt=\"BlackBox AI\" loading=\"lazy\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/a6b76769-1851-4055-bb1d-ef7a809cccf4.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></div><p>‚ÄúIntegrating the MiniMax M2 series into our platform has been a significant win for our users, and M2.1 represents a clear step forward in what a coding-specific model can achieve. We‚Äôve found that M2.1 handles the nuances of complex, multi-step programming tasks with a level of consistency that is rare in this space. By providing high-quality reasoning and context awareness at scale, MiniMax has become a core component of how we help developers solve challenging problems faster. We look forward to seeing how our community continues to leverage these updated capabilities.‚Äù</p><div><p><span>R</span></p><div><p>Robert Rizk</p><p>Co-Founder, CEO of BlackBox AI</p></div></div></div></div><h3>Benchmarks</h3><p>MiniMax-M2.1 delivers a significant leap over M2 on core software engineering leaderboards. It shines particularly bright in multilingual scenarios, where it outperforms Claude Sonnet 4.5 and closely approaches Claude Opus 4.5.</p><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/758890d3-705b-49e2-add5-1c487dc1961f.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><p>We also evaluated MiniMax-M2.1 on SWE-bench Verified across a variety of coding agent frameworks. The results highlight the model's exceptional framework generalization and robust stability.<br> Furthermore, across specific benchmarks‚Äîincluding test case generation, code performance optimization, code review, and instruction following‚ÄîMiniMax-M2.1 demonstrates comprehensive improvements over M2. In these specialized domains, it consistently matches or exceeds the performance of Claude Sonnet 4.5.</p><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/fe802a54-e64f-489c-8a74-089d0d18c5b0.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/4ecb6bfc-b3a4-42bf-bb15-3cb5004529a6.PNG\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><p>To evaluate the model's full-stack capability to architect complete, functional applications \"from zero to one,\" we established a novel benchmark: VIBE (Visual &amp; Interactive Benchmark for Execution). This suite encompasses five core subsets: Web, Simulation, Android, iOS, and Backend. Distinguishing itself from traditional benchmarks, VIBE leverages an innovative Agent-as-a-Verifier (AaaV) paradigm to automatically assess the interactive logic and visual aesthetics of generated applications within a real runtime environment.<br> MiniMax-M2.1 delivers outstanding performance on the VIBE aggregate benchmark, achieving an average score of 88.6‚Äîdemonstrating robust full-stack development capabilities. It excels particularly in the VIBE-Web (91.5) and VIBE-Android (89.7) subsets.</p><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/aafc5ff4-c429-4b53-a224-ce9590804566.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><p>MiniMax-M2.1 also demonstrates steady improvements over M2 in both long-horizon tool use and comprehensive intelligence metrics.</p><div><p><img alt=\"\" fetchpriority=\"high\" decoding=\"async\" data-nimg=\"fill\" src=\"https://file.cdn.minimax.io/public/6aaadb26-4395-46ba-9ebe-f858f3bc0c6b.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></div><h2>Showcases</h2><div><div><h2>Multilingual Coding</h2></div><div><div><h2>3D Interactive Animation</h2><p>MiniMax M2.1 built a \"3D Dreamy Christmas Tree\" based on React Three Fiber and InstancedMesh, successfully rendering over 7,000 instances. It supports gesture interaction and complex particle animation, demonstrating advanced 3D rendering capabilities. <br>Try it out: <a href=\"https://yuyl27wq92.space.minimax.io/\" target=\"_blank\">https://yuyl27wq92.space.minimax.io/</a></p></div><div><h2>Avant-Garde Web UI Design</h2><p>M2.1 generated a minimalist photographer's personal homepage using an asymmetrical layout and a black-white-red contrasting color scheme. By combining immersive imagery with brutalist typography, it achieved a high-impact visual effect. <br>Try it out:<a href=\"https://m6xkaf07udss.space.minimax.io/\" target=\"_blank\">https://m6xkaf07udss.space.minimax.io/</a></p></div><div><h2>Website - Skincare Brand</h2><p>M2.1 designed a landing page for a high-end organic skincare brand. Adopting a \"Clean &amp; Minimalist\" style, it accurately presented the brand's premium identity and international visual appeal. <br>Try it out: <a href=\"https://2drpfocv00n9.space.minimax.io/\" target=\"_blank\">https://2drpfocv00n9.space.minimax.io/</a></p></div><div><h2>Web 3D Lego Sandbox</h2><p>M2.1 developed a high-freedom 3D brick building application based on Three.js, implementing precise grid snapping algorithms and collision detection mechanisms. The project perfectly replicates the glossy texture of plastic bricks, supporting multi-angle rotation, drag-and-drop assembly, and instant color switching, providing users with an immersive 3D creative building experience. <br>Try it out: <a href=\"https://8e6nunemyuzh.space.minimax.io/\" target=\"_blank\">https://8e6nunemyuzh.space.minimax.io/</a></p></div><div><h2>Native App Development - Android</h2><p>M2.1 used Kotlin to develop a native Android gravity sensor simulator. Utilizing the gyroscope for a silky-smooth control experience, it features clever visual easter eggs that elegantly present the \"MERRY XMAS MiniMax M2.1\" message through natural UI transitions and collision effects.</p></div><div><h2>Native App Development - iOS</h2><p>M2.1 wrote an interactive iOS Home Screen widget, designing a \"Sleeping Santa\" click-to-wake mechanism. The logic is complete with native-level animation effects‚ÄîSanta lives in your widget; tap him ten times to wake him up for a surprise! üéÖüéÅ</p></div><div><h2>Web Audio Simulation Development</h2><p>M2.1 developed a 16-step drum machine simulator based on the Web Audio API. It integrates synthesized drum sounds, non-linear rhythm algorithms, and real-time glitch sound effects, providing an avant-garde electronic music experience! (Turn on the sound in the video below to listen!) <br>Try it out: <a href=\"https://21okxwno2u.space.minimax.io/\" target=\"_blank\">https://21okxwno2u.space.minimax.io</a></p></div><div><h2>Rust TUI</h2><p>M2.1 built a powerful Linux security audit tool with dual CLI + TUI modes using Rust, supporting one-click low-level scanning and intelligent risk rating for critical items such as processes, networks, and SSH.</p></div><div><h2>Python Data Dashboard</h2><p>M2.1 created a Web3 cryptocurrency price dashboard in the style of The Matrix. Use Python (backend for real-time price API fetching), HTML (structure), and CSS (Matrix aesthetic: green digital rain on black background, monospaced font, glowing neon green text, terminal-like UI).</p></div><div><h2>C++ Image Rendering</h2><p>M2.1 utilized C++ and GLSL to implement complex light transport algorithms, accurately rendering the physical refraction of a crystal ball, detailed SDF modeling of a snowman, and shimmering snow effects in a real-time environment.</p></div><div><h2>Java Real-time Danmaku</h2><p>M2.1 implemented a high-performance real-time Danmaku (bullet chat) system based on Java, a clean and intuitive user interface, and millisecond-level response capabilities.</p></div><div><h2>SVG Generation</h2><p>M2.1 generated an interactive isometric SVG island map, constructing a detailed miniature world that supports one-click zooming to freely explore four major themed areas. <br>Try it out: <a href=\"https://08tmc3aada59.space.minimax.io/\" target=\"_blank\">https://08tmc3aada59.space.minimax.io/</a></p></div></div></div><div><div><h2>Agentic Tool Use</h2></div><div><h2>Tool Use Capability: Excel Market Research</h2><p>M2.1 demonstrated its tool-use capabilities by autonomously invoking Excel and Yahoo Finance to complete an end-to-end task, ranging from market research data cleaning and analysis to chart generation.</p></div></div><h3>Digital Employee</h3><p>The \"Digital Employee\" is a key feature of the MiniMax M2.1 model. M2.1 accepts web content presented in text form and controls mouse clicks and keyboard inputs via text-based commands. It can complete end-to-end tasks in daily office scenarios across administration, data science, finance, human resources, and software development. The following demo video is a screen recording of M2.1's behavioral trajectory in the <a href=\"https://the-agent-company.com/\" target=\"_blank\">Agent Company Benchmark</a></p><div><div><h2>End-to-End Office Automation</h2></div><div><div><h2>Demo 1: Administrative tasks</h2><p>Task Requirements: Proactively collect employees' equipment requests on communication software, then search for relevant documents on the enterprise's internal server to obtain equipment prices, calculate the total cost and determine whether the department budget is sufficient, and then record equipment changes.</p></div><div><h2>Demo 2: Project management tasks</h2><p>Task Requirements: Search for blocked or backlogged issues on the project management software, then find relevant employees on the communication software and consult them for solutions, and update the status of the issues based on the employees' feedback.</p></div><div><h2>Demo 3: Software development tasks</h2><p>Task Requirements: A colleague wants to know which is the most recent Merge Request that modified a certain file. Search for the relevant Merge Request, find its number, and inform the colleague.</p></div></div></div><h3>How to Use</h3><div><ul> <li>The MiniMax-M2.1 API is now live on the <a href=\"https://www.minimax.io/\" target=\"_blank\">MiniMax Open Platform</a>: <a href=\"https://platform.minimax.io/docs/guides/text-generation\" target=\"_blank\">https://platform.minimax.io/docs/guides/text-generation</a></li> <li>Our product MiniMax Agent, built on MiniMax-M2.1, is now publicly available: <a href=\"https://agent.minimax.io/\" target=\"_blank\">https://agent.minimax.io/</a></li> </ul></div><h3>Contact Us</h3></div></div>"},{"id":"https://geneguessr.brinedew.bio/","title":"Show HN: GeneGuessr ‚Äì a daily biology web puzzle","link":"https://geneguessr.brinedew.bio/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46363870","content":"<a href=\"https://news.ycombinator.com/item?id=46363870\">Comments</a>","date":1766482837000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false},{"id":"https://github.com/thu-ml/TurboDiffusion","title":"TurboDiffusion: 100‚Äì200√ó Acceleration for Video Diffusion Models","link":"https://github.com/thu-ml/TurboDiffusion","hnCommentsUrl":"https://news.ycombinator.com/item?id=46388907","content":"<a href=\"https://news.ycombinator.com/item?id=46388907\">Comments</a>","date":1766719189000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<h2 tabindex=\"-1\" dir=\"auto\">TurboDiffusion</h2> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/TurboDiffusion_Logo.png\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/TurboDiffusion_Logo.png\" width=\"30%\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a> </p> <p dir=\"auto\">This repository provides the official implementation of <strong>TurboDiffusion</strong>, a video generation acceleration framework that can speed up end-to-end diffusion generation by <math-renderer data-run-id=\"b6a8a1318f74547cd9421c8baa616925\">$100 \\sim 200\\times$</math-renderer> on a single RTX 5090, while maintaining video quality.<br> TurboDiffusion primarily uses <a href=\"https://github.com/thu-ml/SageAttention\">SageAttention</a>, <a href=\"https://github.com/thu-ml/SLA\">SLA (Sparse-Linear Attention)</a> for attention acceleration, and <a href=\"https://github.com/NVlabs/rcm\">rCM</a> for timestep distillation.</p> <p dir=\"auto\">Paper: <a href=\"https://arxiv.org/pdf/2512.16093\" rel=\"nofollow\">TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times</a></p> <p dir=\"auto\"><strong>Note</strong>: the checkpoints and paper are not finalized, and will be updated later to improve quality.</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/TurboDiffusion_speedup.png\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/TurboDiffusion_speedup.png\" width=\"99%\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a> </p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/acceleration_decomposition.png\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/acceleration_decomposition.png\" width=\"93%\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a> </p> <div dir=\"auto\"> <markdown-accessiblity-table><table> <tbody><tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/11.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/11.gif\" width=\"387\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/11.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/11.gif\" width=\"387\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> </tbody></table></markdown-accessiblity-table><p> An example of a <b>5-second video</b> generated by Wan-2.1-T2V-1.3B-480P on a single <b>RTX 5090</b>. </p></div> <h2 tabindex=\"-1\" dir=\"auto\">Available Models</h2> <markdown-accessiblity-table><table> <thead> <tr> <th>Model Name</th> <th>Checkpoint Link</th> <th>Best Resolution</th> </tr> </thead> <tbody> <tr> <td><code>TurboWan2.2-I2V-A14B-720P</code></td> <td><a href=\"https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P\" rel=\"nofollow\">Huggingface Model</a></td> <td>720p</td> </tr> <tr> <td><code>TurboWan2.1-T2V-1.3B-480P</code></td> <td><a href=\"https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P\" rel=\"nofollow\">Huggingface Model</a></td> <td>480p</td> </tr> <tr> <td><code>TurboWan2.1-T2V-14B-480P</code></td> <td><a href=\"https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-480P\" rel=\"nofollow\">Huggingface Model</a></td> <td>480p</td> </tr> <tr> <td><code>TurboWan2.1-T2V-14B-720P</code></td> <td><a href=\"https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-14B-720P\" rel=\"nofollow\">Huggingface Model</a></td> <td>720p</td> </tr> </tbody> </table></markdown-accessiblity-table> <p dir=\"auto\">Note: All checkpoints support generating videos at 480p or 720p. The \"Best Resolution\" column indicates the resolution at which the model provides the best video quality.</p> <h2 tabindex=\"-1\" dir=\"auto\">Installation</h2> <p dir=\"auto\"><strong>Base environment</strong>: <code>python&gt;=3.9</code>, <code>torch&gt;=2.7.0</code>. <code>torch==2.8.0</code> is recommended, as higher versions may cause OOM.</p> <p dir=\"auto\">Install TurboDiffusion by pip:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"conda create -n turbodiffusion python=3.12 conda activate turbodiffusion pip install turbodiffusion --no-build-isolation\"><pre>conda create -n turbodiffusion python=3.12 conda activate turbodiffusion pip install turbodiffusion --no-build-isolation</pre></div> <p dir=\"auto\">Or compile from source:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"git clone https://github.com/thu-ml/TurboDiffusion.git cd TurboDiffusion git submodule update --init --recursive pip install -e . --no-build-isolation\"><pre>git clone https://github.com/thu-ml/TurboDiffusion.git <span>cd</span> TurboDiffusion git submodule update --init --recursive pip install -e <span>.</span> --no-build-isolation</pre></div> <p dir=\"auto\">To enable SageSLA, a fast SLA forward pass based on SageAttention, install <a href=\"https://github.com/thu-ml/SpargeAttn\">SpargeAttn</a> first:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation\"><pre>pip install git+https://github.com/thu-ml/SpargeAttn.git --no-build-isolation</pre></div> <h2 tabindex=\"-1\" dir=\"auto\">Inference</h2> <p dir=\"auto\">For GPUs with more than 40GB of GPU memory, <strong>e.g., H100, please use the unquantized checkpoints (without <code>-quant</code>) and remove <code>--quant_linear</code> from the command. For RTX 5090, RTX 4090, or similar GPUs, please use the quantized checkpoints (with <code>-quant</code>) and add <code>--quant_linear</code> in the command.)</strong></p> <ol dir=\"auto\"> <li> <p dir=\"auto\">Download the VAE (<strong>applicable for both Wan2.1 and Wan2.2</strong>) and umT5 text encoder checkpoints:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"mkdir checkpoints cd checkpoints wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth\"><pre>mkdir checkpoints <span>cd</span> checkpoints wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/Wan2.1_VAE.pth wget https://huggingface.co/Wan-AI/Wan2.1-T2V-1.3B/resolve/main/models_t5_umt5-xxl-enc-bf16.pth</pre></div> </li> <li> <p dir=\"auto\">Download our quantized model checkpoints (For RTX 5090 or similar GPUs):</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth\"><pre><span><span>#</span> For Wan2.1-T2V-1.3B</span> wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P-quant.pth <span><span>#</span> For Wan2.2-I2V-14B</span> wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P-quant.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P-quant.pth</pre></div> <p dir=\"auto\"><strong>Or</strong> download our unquantized model checkpoints (For H100 or similar GPUs):</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# For Wan2.1-T2V-1.3B wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth # For Wan2.2-I2V-14B wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth\"><pre><span><span>#</span> For Wan2.1-T2V-1.3B</span> wget https://huggingface.co/TurboDiffusion/TurboWan2.1-T2V-1.3B-480P/resolve/main/TurboWan2.1-T2V-1.3B-480P.pth <span><span>#</span> For Wan2.2-I2V-14B</span> wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-high-720P.pth wget https://huggingface.co/TurboDiffusion/TurboWan2.2-I2V-A14B-720P/resolve/main/TurboWan2.2-I2V-A14B-low-720P.pth</pre></div> </li> <li> <p dir=\"auto\">Use the inference script for the <strong>T2V</strong> models:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"export PYTHONPATH=turbodiffusion # Arguments: # --dit_path Path to the finetuned TurboDiffusion checkpoint # --model Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1‚Äì4 (default: 4) # --sigma_max Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: &quot;480p&quot; or &quot;720p&quot; (default: 480p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.1_t2v_infer.py \\ --model Wan2.1-1.3B \\ --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \\ --resolution 480p \\ --prompt &quot;A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.&quot; \\ --num_samples 1 \\ --num_steps 4 \\ --quant_linear \\ --attention_type sagesla \\ --sla_topk 0.1\"><pre><span>export</span> PYTHONPATH=turbodiffusion <span><span>#</span> Arguments:</span> <span><span>#</span> --dit_path Path to the finetuned TurboDiffusion checkpoint</span> <span><span>#</span> --model Model to use: Wan2.1-1.3B or Wan2.1-14B (default: Wan2.1-1.3B)</span> <span><span>#</span> --num_samples Number of videos to generate (default: 1)</span> <span><span>#</span> --num_steps Sampling steps, 1‚Äì4 (default: 4)</span> <span><span>#</span> --sigma_max Initial sigma for rCM (default: 80); larger choices (e.g., 1600) reduce diversity but may enhance quality</span> <span><span>#</span> --vae_path Path to Wan2.1 VAE (default: checkpoints/Wan2.1_VAE.pth)</span> <span><span>#</span> --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)</span> <span><span>#</span> --num_frames Number of frames to generate (default: 81)</span> <span><span>#</span> --prompt Text prompt for video generation</span> <span><span>#</span> --resolution Output resolution: \"480p\" or \"720p\" (default: 480p)</span> <span><span>#</span> --aspect_ratio Aspect ratio in W:H format (default: 16:9)</span> <span><span>#</span> --seed Random seed for reproducibility (default: 0)</span> <span><span>#</span> --save_path Output file path including extension (default: output/generated_video.mp4)</span> <span><span>#</span> --attention_type Attention module to use: original, sla or sagesla (default: sagesla)</span> <span><span>#</span> --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality</span> <span><span>#</span> --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint</span> <span><span>#</span> --default_norm Use the original LayerNorm and RMSNorm of Wan models</span> python turbodiffusion/inference/wan2.1_t2v_infer.py \\ --model Wan2.1-1.3B \\ --dit_path checkpoints/TurboWan2.1-T2V-1.3B-480P-quant.pth \\ --resolution 480p \\ --prompt <span><span>\"</span>A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.<span>\"</span></span> \\ --num_samples 1 \\ --num_steps 4 \\ --quant_linear \\ --attention_type sagesla \\ --sla_topk 0.1</pre></div> <p dir=\"auto\">Or the script for the <strong>I2V</strong> model:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"export PYTHONPATH=turbodiffusion # --image_path Path to the input image # --high_noise_model_path Path to the high noise TurboDiffusion checkpoint # --low_noise_model_path Path to the high noise TurboDiffusion checkpoint # --boundary Timestep boundary for switching from high to low noise model (default: 0.9) # --model Model to use: Wan2.2-A14B (default: Wan2.2-A14B) # --num_samples Number of videos to generate (default: 1) # --num_steps Sampling steps, 1‚Äì4 (default: 4) # --sigma_max Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality # --vae_path Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth) # --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth) # --num_frames Number of frames to generate (default: 81) # --prompt Text prompt for video generation # --resolution Output resolution: &quot;480p&quot; or &quot;720p&quot; (default: 720p) # --aspect_ratio Aspect ratio in W:H format (default: 16:9) # --adaptive_resolution Enable adaptive resolution based on input image size # --ode Use ODE for sampling (sharper but less robust than SDE) # --seed Random seed for reproducibility (default: 0) # --save_path Output file path including extension (default: output/generated_video.mp4) # --attention_type Attention module to use: original, sla or sagesla (default: sagesla) # --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality # --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint # --default_norm Use the original LayerNorm and RMSNorm of Wan models python turbodiffusion/inference/wan2.2_i2v_infer.py \\ --model Wan2.2-A14B \\ --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \\ --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \\ --resolution 720p \\ --adaptive_resolution \\ --image_path assets/i2v_inputs/i2v_input_0.jpg \\ --prompt &quot;POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces.&quot; \\ --num_samples 1 \\ --num_steps 4 \\ --quant_linear \\ --attention_type sagesla \\ --sla_topk 0.1 \\ --ode\"><pre><span>export</span> PYTHONPATH=turbodiffusion <span><span>#</span> --image_path Path to the input image</span> <span><span>#</span> --high_noise_model_path Path to the high noise TurboDiffusion checkpoint</span> <span><span>#</span> --low_noise_model_path Path to the high noise TurboDiffusion checkpoint</span> <span><span>#</span> --boundary Timestep boundary for switching from high to low noise model (default: 0.9)</span> <span><span>#</span> --model Model to use: Wan2.2-A14B (default: Wan2.2-A14B)</span> <span><span>#</span> --num_samples Number of videos to generate (default: 1)</span> <span><span>#</span> --num_steps Sampling steps, 1‚Äì4 (default: 4)</span> <span><span>#</span> --sigma_max Initial sigma for rCM (default: 200); larger choices (e.g., 1600) reduce diversity but may enhance quality</span> <span><span>#</span> --vae_path Path to Wan2.2 VAE (default: checkpoints/Wan2.2_VAE.pth)</span> <span><span>#</span> --text_encoder_path Path to umT5 text encoder (default: checkpoints/models_t5_umt5-xxl-enc-bf16.pth)</span> <span><span>#</span> --num_frames Number of frames to generate (default: 81)</span> <span><span>#</span> --prompt Text prompt for video generation</span> <span><span>#</span> --resolution Output resolution: \"480p\" or \"720p\" (default: 720p)</span> <span><span>#</span> --aspect_ratio Aspect ratio in W:H format (default: 16:9)</span> <span><span>#</span> --adaptive_resolution Enable adaptive resolution based on input image size</span> <span><span>#</span> --ode Use ODE for sampling (sharper but less robust than SDE)</span> <span><span>#</span> --seed Random seed for reproducibility (default: 0)</span> <span><span>#</span> --save_path Output file path including extension (default: output/generated_video.mp4)</span> <span><span>#</span> --attention_type Attention module to use: original, sla or sagesla (default: sagesla)</span> <span><span>#</span> --sla_topk Top-k ratio for SLA/SageSLA attention (default: 0.1), we recommend using 0.15 for better video quality</span> <span><span>#</span> --quant_linear Enable quantization for linear layers, pass this if using a quantized checkpoint</span> <span><span>#</span> --default_norm Use the original LayerNorm and RMSNorm of Wan models</span> python turbodiffusion/inference/wan2.2_i2v_infer.py \\ --model Wan2.2-A14B \\ --low_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-low-720P-quant.pth \\ --high_noise_model_path checkpoints/TurboWan2.2-I2V-A14B-high-720P-quant.pth \\ --resolution 720p \\ --adaptive_resolution \\ --image_path assets/i2v_inputs/i2v_input_0.jpg \\ --prompt <span><span>\"</span>POV selfie video, ultra-messy and extremely fast. A white cat in sunglasses stands on a surfboard with a neutral look when the board suddenly whips sideways, throwing cat and camera into the water; the frame dives sharply downward, swallowed by violent bursts of bubbles, spinning turbulence, and smeared water streaks as the camera sinks. Shadows thicken, pressure ripples distort the edges, and loose bubbles rush upward past the lens, showing the camera is still sinking. Then the cat kicks upward with explosive speed, dragging the view through churning bubbles and rapidly brightening water as sunlight floods back in; the camera races upward, water streaming off the lens, and finally breaks the surface in a sudden blast of light and spray, snapping back into a crooked, frantic selfie as the cat resurfaces.<span>\"</span></span> \\ --num_samples 1 \\ --num_steps 4 \\ --quant_linear \\ --attention_type sagesla \\ --sla_topk 0.1 \\ --ode</pre></div> </li> </ol> <p dir=\"auto\">Interactive inference via the terminal is available at <code>turbodiffusion/serve/</code>. This allows multi-turn video generation without reloading the model.</p> <h2 tabindex=\"-1\" dir=\"auto\">Evaluation</h2> <p dir=\"auto\">We evaluate video generation on <strong>a single RTX 5090 GPU</strong>. The E2E Time refers to the end-to-end diffusion generation latency, excluding text encoding and VAE decoding.</p> <h3 tabindex=\"-1\" dir=\"auto\">Wan-2.2-I2V-A14B-720P</h3> <markdown-accessiblity-table><table> <tbody><tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/0.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/0.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/1.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/1.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/2.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/2.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/2.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/2.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/3.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/3.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/3.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/3.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/4.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/4.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/4.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/4.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/5.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/5.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4549s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/original/A14B_720p/gif/6.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/original/A14B_720p/gif/6.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>38s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/6.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/i2v/turbodiffusion/A14B_720p/gif/6.gif\" width=\"360\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> </tbody></table></markdown-accessiblity-table> <h3 tabindex=\"-1\" dir=\"auto\">Wan-2.1-T2V-1.3B-480P</h3> <markdown-accessiblity-table><table> <tbody><tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/2.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/2.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/2.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/2.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/2.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/2.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/7.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/7.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/7.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/7.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/7.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/7.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/11.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/11.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/11.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/11.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/11.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/11.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/13.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/13.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/13.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/13.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/13.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/13.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 184s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/1.3B/14.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/1.3B/14.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 5.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_1.3B/14.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_1.3B/14.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>1.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/1.3B/14.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/1.3B/14.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> </tbody></table></markdown-accessiblity-table> <h3 tabindex=\"-1\" dir=\"auto\">Wan-2.1-T2V-14B-720P</h3> <markdown-accessiblity-table><table> <tbody><tr> <td> <p>Original, E2E Time: 4767s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 72.6s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>24s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4767s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/3.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/3.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 72.6s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/3.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/3.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>24s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/3.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/3.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 4767s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_720p/6.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_720p/6.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 72.6s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_720p/6.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_720p/6.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>24s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_720p/6.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_720p/6.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> </tbody></table></markdown-accessiblity-table> <h3 tabindex=\"-1\" dir=\"auto\">Wan-2.1-T2V-14B-480P</h3> <markdown-accessiblity-table><table> <tbody><tr> <td> <p>Original, E2E Time: 1676s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 26.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>9.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/0.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/0.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 1676s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 26.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>9.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/1.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/1.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 1676s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/4.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/4.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 26.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/4.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/4.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>9.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/4.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/4.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> <tr> <td> <p>Original, E2E Time: 1676s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/original/14B_480p/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/original/14B_480p/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>FastVideo, E2E Time: 26.3s</p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/fastvideo/video_14B_480p/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/fastvideo/video_14B_480p/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> <td> <p>TurboDiffusion, E2E Time: <b>9.9s</b></p> <p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/thu-ml/TurboDiffusion/blob/main/assets/videos/turbodiffusion/14B_480p/5.gif\"><img src=\"https://github.com/thu-ml/TurboDiffusion/raw/main/assets/videos/turbodiffusion/14B_480p/5.gif\" width=\"249\" data-animated-image=\"\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p> </td> </tr> </tbody></table></markdown-accessiblity-table> <h2 tabindex=\"-1\" dir=\"auto\">Training</h2> <p dir=\"auto\">In this repo, we provide training code based on Wan2.1 and its synthetic data. The training builds on the rCM codebase (<a href=\"https://github.com/NVlabs/rcm\">https://github.com/NVlabs/rcm</a>), with infrastructure support including FSDP2, Ulysses CP, and selective activation checkpointing (SAC). For rCM training instructions, please refer to the original rCM repository; <a href=\"https://github.com/thu-ml/SLA\">SLA (Sparse-Linear Attention)</a> training guidance is provided here.</p> <h4 tabindex=\"-1\" dir=\"auto\">Additional Installation</h4> <p dir=\"auto\">For rCM/SLA training, additionally run:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"pip install megatron-core hydra-core wandb webdataset pip install --no-build-isolation transformer_engine[pytorch]\"><pre>pip install megatron-core hydra-core wandb webdataset pip install --no-build-isolation transformer_engine[pytorch]</pre></div> <h4 tabindex=\"-1\" dir=\"auto\">Checkpoints Downloading</h4> <p dir=\"auto\">Download the Wan2.1 pretrained checkpoints in <code>.pth</code> format and VAE/text encoder to <code>assets/checkpoints</code>:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# make sure git lfs is installed git clone https://huggingface.co/worstcoder/Wan assets/checkpoints\"><pre><span><span>#</span> make sure git lfs is installed</span> git clone https://huggingface.co/worstcoder/Wan assets/checkpoints</pre></div> <p dir=\"auto\">FSDP2 relies on <a href=\"https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html\" rel=\"nofollow\">Distributed Checkpoint (DCP)</a> for loading and saving checkpoints. Before training, convert <code>.pth</code> teacher checkpoints to <code>.dcp</code> first:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp\"><pre>python -m torch.distributed.checkpoint.format_utils torch_to_dcp assets/checkpoints/Wan2.1-T2V-1.3B.pth assets/checkpoints/Wan2.1-T2V-1.3B.dcp</pre></div> <p dir=\"auto\">After training, the saved <code>.dcp</code> checkpoints can be converted to <code>.pth</code> using the script <code>scripts/dcp_to_pth.py</code>.</p> <h4 tabindex=\"-1\" dir=\"auto\">Dataset Downloading</h4> <p dir=\"auto\">We provide Wan2.1-14B-synthesized datasets. Download to <code>assets/datasets</code> using:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# make sure git lfs is installed git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets\"><pre><span><span>#</span> make sure git lfs is installed</span> git clone https://huggingface.co/datasets/worstcoder/Wan_datasets assets/datasets</pre></div> <h4 tabindex=\"-1\" dir=\"auto\">Start Training</h4> <p dir=\"auto\">We implement white-box SLA training by aligning the predictions of the SLA-enabled model with those of the full-attention pretrained model. Unlike black-box training in the original paper, which tunes the pretrained model using diffusion loss, white-box training mitigates distribution shift and is less sensitive to the training data.</p> <p dir=\"auto\">Single-node training example:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"WORKDIR=&quot;/your/path/to/turbodiffusion&quot; cd $WORKDIR export PYTHONPATH=turbodiffusion # the &quot;IMAGINAIRE_OUTPUT_ROOT&quot; environment variable is the path to save experiment output files export IMAGINAIRE_OUTPUT_ROOT=${WORKDIR}/outputs CHECKPOINT_ROOT=${WORKDIR}/assets/checkpoints DATASET_ROOT=${WORKDIR}/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K # your Wandb information export WANDB_API_KEY=xxx export WANDB_ENTITY=xxx registry=registry_sla experiment=wan2pt1_1pt3B_res480p_t2v_SLA torchrun --nproc_per_node=8 \\ -m scripts.train --config=rcm/configs/${registry}.py -- experiment=${experiment} \\ model.config.teacher_ckpt=${CHECKPOINT_ROOT}/Wan2.1-T2V-1.3B.dcp \\ model.config.tokenizer.vae_pth=${CHECKPOINT_ROOT}/Wan2.1_VAE.pth \\ model.config.text_encoder_path=${CHECKPOINT_ROOT}/models_t5_umt5-xxl-enc-bf16.pth \\ model.config.neg_embed_path=${CHECKPOINT_ROOT}/umT5_wan_negative_emb.pt \\ dataloader_train.tar_path_pattern=${DATASET_ROOT}/shard*.tar\"><pre>WORKDIR=<span><span>\"</span>/your/path/to/turbodiffusion<span>\"</span></span> <span>cd</span> <span>$WORKDIR</span> <span>export</span> PYTHONPATH=turbodiffusion <span><span>#</span> the \"IMAGINAIRE_OUTPUT_ROOT\" environment variable is the path to save experiment output files</span> <span>export</span> IMAGINAIRE_OUTPUT_ROOT=<span>${WORKDIR}</span>/outputs CHECKPOINT_ROOT=<span>${WORKDIR}</span>/assets/checkpoints DATASET_ROOT=<span>${WORKDIR}</span>/assets/datasets/Wan2.1_14B_480p_16:9_Euler-step100_shift-3.0_cfg-5.0_seed-0_250K <span><span>#</span> your Wandb information</span> <span>export</span> WANDB_API_KEY=xxx <span>export</span> WANDB_ENTITY=xxx registry=registry_sla experiment=wan2pt1_1pt3B_res480p_t2v_SLA torchrun --nproc_per_node=8 \\ -m scripts.train --config=rcm/configs/<span>${registry}</span>.py -- experiment=<span>${experiment}</span> \\ model.config.teacher_ckpt=<span>${CHECKPOINT_ROOT}</span>/Wan2.1-T2V-1.3B.dcp \\ model.config.tokenizer.vae_pth=<span>${CHECKPOINT_ROOT}</span>/Wan2.1_VAE.pth \\ model.config.text_encoder_path=<span>${CHECKPOINT_ROOT}</span>/models_t5_umt5-xxl-enc-bf16.pth \\ model.config.neg_embed_path=<span>${CHECKPOINT_ROOT}</span>/umT5_wan_negative_emb.pt \\ dataloader_train.tar_path_pattern=<span>${DATASET_ROOT}</span>/shard<span>*</span>.tar</pre></div> <p dir=\"auto\">Please refer to <code>turbodiffusion/rcm/configs/experiments/sla/wan2pt1_t2v.py</code> for the 14B config or perform modifications as needed.</p> <h4 tabindex=\"-1\" dir=\"auto\">Model Merging</h4> <p dir=\"auto\">The parameter updates from SLA training can be merged into rCM checkpoints using <code>turbodiffusion/scripts/merge_models.py</code>, enabling rCM to perform sparse attention inference. Specify <code>--base</code> as the rCM model, <code>--diff_base</code> as the pretrained model, and <code>--diff_target</code> as the SLA-tuned model.</p> <h2 tabindex=\"-1\" dir=\"auto\">ComfyUI Integration</h2> <p dir=\"auto\">We thank the community effort <a href=\"https://github.com/anveshane/Comfyui_turbodiffusion\">Comfyui_turbodiffusion</a> for integrating TurboDiffusion into ComfyUI.</p> <h2 tabindex=\"-1\" dir=\"auto\">Roadmap</h2> <p dir=\"auto\">We're actively working on the following features and improvements:</p> <ul> <li> Organize and release training code</li> <li> Optimize infrastructure for better parallel</li> <li> vLLM-Omni integration</li> <li> Support for more video generation models</li> <li> Support for autoregressive video generation models</li> <li> More hardware-level operator optimizations</li> </ul> <p dir=\"auto\">We welcome community members to help maintain and extend TurboDiffusion. Welcome to join the TurboDiffusion Team and contribute together!</p> <h2 tabindex=\"-1\" dir=\"auto\">Citation</h2> <p dir=\"auto\"><strong>If you use this code or find our work valuable, please cite:</strong></p> <div data-snippet-clipboard-copy-content=\"@article{zhang2025turbodiffusion, title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times}, author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun}, journal={arXiv preprint arXiv:2512.16093}, year={2025} } @software{turbodiffusion2025, title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times}, author={The TurboDiffusion Team}, url={https://github.com/thu-ml/TurboDiffusion}, year={2025} } @inproceedings{zhang2025sageattention, title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei}, booktitle={International Conference on Learning Representations (ICLR)}, year={2025} } @article{zhang2025sla, title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention}, author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others}, journal={arXiv preprint arXiv:2509.24006}, year={2025} } @article{zheng2025rcm, title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency}, author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng}, journal={arXiv preprint arXiv:2510.08431}, year={2025} } @inproceedings{zhang2024sageattention2, title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization}, author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei}, booktitle={International Conference on Machine Learning (ICML)}, year={2025} }\"><pre><code>@article{zhang2025turbodiffusion, title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times}, author={Zhang, Jintao and Zheng, Kaiwen and Jiang, Kai and Wang, Haoxu and Stoica, Ion and Gonzalez, Joseph E and Chen, Jianfei and Zhu, Jun}, journal={arXiv preprint arXiv:2512.16093}, year={2025} } @software{turbodiffusion2025, title={TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times}, author={The TurboDiffusion Team}, url={https://github.com/thu-ml/TurboDiffusion}, year={2025} } @inproceedings{zhang2025sageattention, title={SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration}, author={Zhang, Jintao and Wei, Jia and Zhang, Pengle and Zhu, Jun and Chen, Jianfei}, booktitle={International Conference on Learning Representations (ICLR)}, year={2025} } @article{zhang2025sla, title={SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention}, author={Zhang, Jintao and Wang, Haoxu and Jiang, Kai and Yang, Shuo and Zheng, Kaiwen and Xi, Haocheng and Wang, Ziteng and Zhu, Hongzhou and Zhao, Min and Stoica, Ion and others}, journal={arXiv preprint arXiv:2509.24006}, year={2025} } @article{zheng2025rcm, title={Large Scale Diffusion Distillation via Score-Regularized Continuous-Time Consistency}, author={Zheng, Kaiwen and Wang, Yuji and Ma, Qianli and Chen, Huayu and Zhang, Jintao and Balaji, Yogesh and Chen, Jianfei and Liu, Ming-Yu and Zhu, Jun and Zhang, Qinsheng}, journal={arXiv preprint arXiv:2510.08431}, year={2025} } @inproceedings{zhang2024sageattention2, title={Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization}, author={Zhang, Jintao and Huang, Haofeng and Zhang, Pengle and Wei, Jia and Zhu, Jun and Chen, Jianfei}, booktitle={International Conference on Machine Learning (ICML)}, year={2025} } </code></pre></div>"},{"id":"https://gamingcouch.com","title":"Show HN: Gaming Couch ‚Äì a local multiplayer party game platform for 8 players","link":"https://gamingcouch.com","hnCommentsUrl":"https://news.ycombinator.com/item?id=46344573","content":"<a href=\"https://news.ycombinator.com/item?id=46344573\">Comments</a>","date":1766322516000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div><p><label dir=\"ltr\"><span>Select website language</span></label></p><section><p><img alt=\"Illustration of the red player character\" loading=\"lazy\" width=\"572\" height=\"372\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fplayer-red.7fe808f6.png&amp;w=1200&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"><img alt=\"Illustration of the blue player character\" loading=\"lazy\" width=\"572\" height=\"372\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fplayer-blue.7de72022.png&amp;w=1200&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></section><section><a target=\"_blank\" aria-label=\"Play now\" title=\"Play now\" href=\"https://play.gamingcouch.com/?from=home\"><p><span>Play</span><span>Now!</span></p></a><h2>Couch Co-Op Party Games Made Easy!</h2><p>Play fun, fast, and rivalry-inducing party games with up to 8 players.</p><p>Your smartphone is your controller - no consoles or downloads required!</p></section><section><h2>Awards &amp; Honors</h2><p><img alt=\"2nd place at the Very Big Indie Pitch competition (mobile category), 2025\" loading=\"lazy\" width=\"2218\" height=\"1072\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fwreath-bip.444efa7f.png&amp;w=3840&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"><img alt=\"Gamescom Invest Circle pitch competition finalist, 2025\" loading=\"lazy\" width=\"2218\" height=\"1072\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fwreath-gamescom.5072b750.png&amp;w=3840&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"><img alt=\"International Game Developer Association Finland Grant finalist, 2025\" loading=\"lazy\" width=\"2218\" height=\"1072\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fwreath-igda.f8927b5c.png&amp;w=3840&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p></section><section><h2>In Numbers</h2><p>as of October 2025</p><div><div><div><p><span>92</span></p><p>Countries playing on Gaming Couch</p></div></div><div><div><p><span>101 min</span></p><p>Avg. game session duration</p></div></div><div><div><p><span>4.4 / 5</span></p><p>Rated by first-time players</p></div></div></div></section><section><h2>Mentions in Media</h2><p><a target=\"_blank\" href=\"https://suomigamehub.com/uutiset/gaming-couch-mahdollistaa-matalan-kynnyksen-moninpelaamisen-puhelimien-yli/\"><img alt=\"SuomiGamesHub media logo\" loading=\"lazy\" width=\"300\" height=\"300\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmedia-sgh.a805b71b.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://www.pocketgamer.com/features/gamingcouchcom-hands-on/\"><img alt=\"Pocketgamer.com logo\" loading=\"lazy\" width=\"400\" height=\"400\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmedia-pocketgamer.fb9b91da.jpeg&amp;w=828&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://errori.net/kotinmainen-startup-gaming-couch-tuo-uuden-otteen-sosiaaliseen-pelaamiseen/\"><img alt=\"Errori.net media logo\" loading=\"lazy\" width=\"274\" height=\"300\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmedia-errori.e5fd7921.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://tekimobile.com/noticia/12-jogos-indie-celular-apresentados-evento-helsinki/\"><img alt=\"tekimobile.com logo\" loading=\"lazy\" width=\"342\" height=\"60\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmedia-tekimobile.eb934b5d.png&amp;w=750&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://www.v2.fi/uutiset/pelit/40138/Kotimainen-pelialan-startup-Gaming-Couch-tuo-pelaajat-yhteisten-pelikokemusten-aarelle/\"><img alt=\"v2.fi media logo\" loading=\"lazy\" width=\"513\" height=\"300\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fmedia-v2.53e89ba2.png&amp;w=1080&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p><h3>Want to cover Gaming Couch? <a target=\"_blank\" href=\"https://impress.games/press-kit/gaming-couch/gamingcouchcom\">Check out our press kit!</a></h3></section><section><h2>FAQ</h2></section><a target=\"_blank\" aria-label=\"Play now\" title=\"Play now\" href=\"https://play.gamingcouch.com/?from=home\"><p><span>Play</span><span>Now!</span></p></a><section><h2>Follow us on:</h2><p><a target=\"_blank\" href=\"https://discord.gg/UqSX9ZGz5u\"><img alt=\"discord\" loading=\"lazy\" width=\"1694\" height=\"256\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fdiscord.fb367e8d.png&amp;w=3840&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://linkedin.com/company/gaming-couch/\"><img alt=\"linkedin\" loading=\"lazy\" width=\"256\" height=\"218\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flinkedin.1b77ff90.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://www.youtube.com/@GamingCouchCommunity\"><img alt=\"youtube\" loading=\"lazy\" width=\"256\" height=\"181\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fyoutube.3cd11051.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://www.tiktok.com/@gamingcouch.com\"><img alt=\"tiktok\" loading=\"lazy\" width=\"256\" height=\"256\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Ftiktok.b51fe968.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a><a target=\"_blank\" href=\"https://www.instagram.com/gamingcouchcom/\"><img alt=\"instagram\" loading=\"lazy\" width=\"256\" height=\"256\" decoding=\"async\" data-nimg=\"1\" src=\"https://gamingcouch.com/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Finstagram.a2d1477a.png&amp;w=640&amp;q=75\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a></p></section><section><label dir=\"ltr\"><span>Select website language</span></label></section></div></div>"},{"id":"https://github.com/popovicu/ultimate-linux","title":"Ultimate-Linux: Userspace for Linux in Pure JavaScript","link":"https://github.com/popovicu/ultimate-linux","hnCommentsUrl":"https://news.ycombinator.com/item?id=46388700","content":"<a href=\"https://news.ycombinator.com/item?id=46388700\">Comments</a>","date":1766716349000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<h2 tabindex=\"-1\" dir=\"auto\">Ultimate Linux!!!</h2> <p dir=\"auto\">This is a fun tiny project for building a tiny Linux distribution in just JavaScript (and a tiny bit of C to enable mounting to get some fun results).</p> <div data-snippet-clipboard-copy-content=\"--- ULTIMATE LINUX SHELL --- Commands: ls, cd, cat, mkdir, mount, exit\"><pre><code>--- ULTIMATE LINUX SHELL --- Commands: ls, cd, cat, mkdir, mount, exit </code></pre></div> <h2 tabindex=\"-1\" dir=\"auto\">Background context</h2> <p dir=\"auto\">I post a lot on X (Twitter) and if you don't follow me already, <a href=\"https://x.com/popovicu94\" rel=\"nofollow\">please follow now</a>!</p> <p dir=\"auto\">Lately I've been posting a lot about Unix, Linux, ideas of kernel syscall stability, etc.</p> <p dir=\"auto\">In particular, I explored lately how Linux is more or less unique in the kernel/OS world for multiple reasons. First, it's a rare kernel that is shipped independently from the rest of the OS. BSDs, for example, ship the kernel in a coherent unit with the foundational userspace. Linux thus has a unique problem of defining its contract with software built on top of it. And Linux chose stable syscall ABI as this contract. This is in contrast with something like macOS, which is a Unix-certified OS, but which exposes only its system library as the public contract. Apple doesn't guarantee binary backwards compatibility.</p> <p dir=\"auto\">Then I explored how pure Go binaries can interestingly target the kernel itself directly via syscalls for its static binaries, and not depend on the system libraries, at least on Linux. There were some explorations around <code>u-root</code> project, etc.</p> <p dir=\"auto\">Every once in a while I get comments about how wrong I am when talking about C, Go, Rust, you name it. Comments like Go sucks because it does what it does, I'm wrong when I say \"Linux is a kernel, not a complete OS\", I don't understand Unix, POSIX, whatever you can think of.</p> <p dir=\"auto\">So this time I'm doing something to get all their love. I'm creating a libc-less micro Linux distribution in... JavaScript! A standalone JavaScript binary no less! Of course, there's a transpilation step through C, but who cares -- this is the Ultimate Linux! üí™üêß</p> <p dir=\"auto\">Anyway, putting the jokes aside, if you want to really understand what is going on here and you want to understand the fundamentals of how the Linux kernel interfaces with user software, please check out <a href=\"https://popovicu.com/posts/making-a-micro-linux-distro/\" rel=\"nofollow\">this article</a> that I have previously written. It's about making these \"micro Linux distros\" and it should give you fundamental understanding of what Linux distros really are.</p> <h2 tabindex=\"-1\" dir=\"auto\">Build instructions</h2> <p dir=\"auto\">Download <code>quickjs</code> source code:</p> <div data-snippet-clipboard-copy-content=\"wget https://bellard.org/quickjs/quickjs-2025-09-13-2.tar.xz\"><pre><code>wget https://bellard.org/quickjs/quickjs-2025-09-13-2.tar.xz </code></pre></div> <p dir=\"auto\">Unpack it:</p> <div data-snippet-clipboard-copy-content=\"tar -xf quickjs-2025-09-13-2.tar.xz\"><pre><code>tar -xf quickjs-2025-09-13-2.tar.xz </code></pre></div> <p dir=\"auto\">Go inside the source directory, run <code>make</code> and go back up.</p> <p dir=\"auto\">Now go ahead and install <code>musl</code> libc on your system: <a href=\"https://musl.libc.org/\" rel=\"nofollow\">https://musl.libc.org/</a></p> <p dir=\"auto\"><strong>Do not worry</strong>, <code>musl</code> installation is <em>polite</em> by default, meaning it will install itself into <code>/usr/lib/local</code>, it will not clash with your host's libc. The reason why we install <code>musl</code> is because it provides <code>gcc</code> and <code>clang</code> wrapper scripts for linking against <code>musl</code> instead of your system library. You can then use</p> <div data-snippet-clipboard-copy-content=\"/usr/local/musl/bin/musl-gcc\"><pre><code>/usr/local/musl/bin/musl-gcc </code></pre></div> <p dir=\"auto\">instead of your system's GCC to link against the freshly built <code>musl</code> instead of your host system. That's what we do here and we link statically against <code>musl</code> to make a standalone ELF file which doesn't depend on the running system's libc.</p> <p dir=\"auto\">We're now ready to transpile the JavaScript code to C, link it together with some system operations and produce the final ULTIMATE SHELL!</p> <div data-snippet-clipboard-copy-content=\"./quickjs-2025-09-13/qjsc -M sys_ops,js_init_module_sys_ops -e -o ultimate_shell.c ultimate_shell.js &amp;&amp; /usr/local/musl/bin/musl-gcc -static -o ultimate_shell ultimate_shell.c sys_ops.c -I ./quickjs-2025-09-13 ./quickjs-2025-09-13/libquickjs.a -lm -ldl -lpthread\"><pre><code>./quickjs-2025-09-13/qjsc -M sys_ops,js_init_module_sys_ops -e -o ultimate_shell.c ultimate_shell.js &amp;&amp; /usr/local/musl/bin/musl-gcc -static -o ultimate_shell ultimate_shell.c sys_ops.c -I ./quickjs-2025-09-13 ./quickjs-2025-09-13/libquickjs.a -lm -ldl -lpthread </code></pre></div> <p dir=\"auto\">You can run <code>./ultimate_shell</code> on your build machine as well, it should be fully portable.</p> <p dir=\"auto\">However, let's run it on a VM! First, let's build <code>initramfs</code>.</p> <div data-snippet-clipboard-copy-content=\"echo &quot;ultimate_shell&quot; | cpio -o -H newc > image.cpio\"><pre><code>echo \"ultimate_shell\" | cpio -o -H newc &gt; image.cpio </code></pre></div> <p dir=\"auto\">Now let's run the VM with the Ultimate Shell as the PID 1!</p> <div data-snippet-clipboard-copy-content=\"qemu-system-x86_64 -m 4G -kernel /tmp/linux/linux-6.17.12/arch/x86/boot/bzImage -initrd ./image.cpio -nographic --enable-kvm -smp 8 -append &quot;console=ttyS0 rdinit=/ultimate_shell&quot;\"><pre><code>qemu-system-x86_64 -m 4G -kernel /tmp/linux/linux-6.17.12/arch/x86/boot/bzImage -initrd ./image.cpio -nographic --enable-kvm -smp 8 -append \"console=ttyS0 rdinit=/ultimate_shell\" </code></pre></div> <p dir=\"auto\">After a long blob of text from QEMU, you should get the shell prompt and you can play around a bit:</p> <div data-snippet-clipboard-copy-content=\"... [ 0.805878] x86/mm: Checked W+X mappings: passed, no W+X pages found. [ 0.807049] x86/mm: Checking user space page tables [ 0.839182] x86/mm: Checked W+X mappings: passed, no W+X pages found. [ 0.840185] Run /ultimate_shell as init process --- ULTIMATE LINUX SHELL --- Commands: ls, cd, cat, mkdir, mount, exit [/] # ls . .. ultimate_shell root dev [/] # ls /dev . .. console [/] # mkdir proc [/] # ls . .. proc ultimate_shell root dev [/] # mount proc /proc proc Mount proc -> /proc: Success [/] # cat /proc/cmdline console=ttyS0 rdinit=/ultimate_shell [/] # cat /proc/1/environ HOME=/ [/] # cat /proc/1/cmdline /ultimate_shell [/] #\"><pre><code>... [ 0.805878] x86/mm: Checked W+X mappings: passed, no W+X pages found. [ 0.807049] x86/mm: Checking user space page tables [ 0.839182] x86/mm: Checked W+X mappings: passed, no W+X pages found. [ 0.840185] Run /ultimate_shell as init process --- ULTIMATE LINUX SHELL --- Commands: ls, cd, cat, mkdir, mount, exit [/] # ls . .. ultimate_shell root dev [/] # ls /dev . .. console [/] # mkdir proc [/] # ls . .. proc ultimate_shell root dev [/] # mount proc /proc proc Mount proc -&gt; /proc: Success [/] # cat /proc/cmdline console=ttyS0 rdinit=/ultimate_shell [/] # cat /proc/1/environ HOME=/ [/] # cat /proc/1/cmdline /ultimate_shell [/] # </code></pre></div>"},{"id":"https://destroytoday.com/blog/animating-quines-for-larva-labs","title":"Animating Quines for Larva Labs","link":"https://destroytoday.com/blog/animating-quines-for-larva-labs","hnCommentsUrl":"https://news.ycombinator.com/item?id=46354187","content":"<a href=\"https://news.ycombinator.com/item?id=46354187\">Comments</a>","date":1766412242000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<section><figure> <picture> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?fm=webp&amp;w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?fm=webp&amp;w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?fm=webp&amp;w=1440&amp;q=80\"> <img alt=\"Quine collection\" src=\"https://images.ctfassets.net/zi79s2th73f3/7eefFO0pxE9KocqUIufVAM/707f3b204cd6b70e4e7865a6470b4d74/quine-collection.png?w=1440&amp;q=80\" width=\"720\" height=\"376\" loading=\"lazy\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </picture> <figcaption> <p>‚ÄúQuine‚Äù by Larva Labs</p> </figcaption> </figure><p>A couple months ago, when I was first planning to <a href=\"https://destroytoday.com/blog/going-independent-again\">go independent again</a>, I ran into my friend, John Watkinson, on the walk home from my studio. John is half of <a href=\"https://larvalabs.com/\">Larva Labs</a> along with my good friend, Matt Hall, and they‚Äôre responsible for industry-defining projects, like <a href=\"https://www.larvalabs.com/cryptopunks\">CryptoPunks</a> and <a href=\"https://www.larvalabs.com/autoglyphs\">Autoglyphs</a>. I actually sat next to them when they were first working on CryptoPunks, and they asked if I wanted to have any. I remember telling them that I was too busy working on <a href=\"https://cushionapp.com/\">Cushion</a> to be bothered. So that was cool. In any case, we caught up for a bit and I mentioned my plans to return to freelance life. John had an immediate lightbulb reaction, as if this was perfect timing‚Äîand it was. It turns out Matt and John were weeks away from announcing a new project, and they were looking for someone to animate it.</p><figure> <picture> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?fm=webp&amp;w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?fm=webp&amp;w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?fm=webp&amp;w=1440&amp;q=80\"> <img alt=\"Quine code\" src=\"https://images.ctfassets.net/zi79s2th73f3/1lIF4dmV28XAglerHJ6mAV/03a80c8b4e1d89c553a38d8b1f6fa4d1/quine-code-zoom.png?w=1440&amp;q=80\" width=\"720\" height=\"605.343396226415\" loading=\"lazy\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </picture> <figcaption> <p>A quine at 100% scale</p> </figcaption> </figure><p>Matt invited me over to their studio, where they told me about <a href=\"https://larvalabs.com/quine\">Quine</a>. In their words, ‚ÄúQuine is a generative art project that blurs the line between its code and the art it produces.‚Äù At first, these quines just look like cool procedurally-generated pixel art. And, while that‚Äôs true, if you look closer, you‚Äôll see that there‚Äôs real code embedded within each quine. Matt explained to me that if you extract the code and run it, it‚Äôll generate the next variation of that quine‚Äôs sequence. I was immediately intrigued from that concept alone, but it goes even deeper than that.</p><figure> <picture> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?fm=webp&amp;w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?w=960&amp;w=1440&amp;q=80\" media=\"(max-width: 480px)\"><source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?fm=webp&amp;w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"><source srcset=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?w=1280&amp;w=1440&amp;q=80\" media=\"(max-width: 640px)\"> <source type=\"image/webp\" srcset=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?fm=webp&amp;w=1440&amp;q=80\"> <img alt=\"Quine 3-gen example\" src=\"https://images.ctfassets.net/zi79s2th73f3/5QnmPhSQ3pAr58ej0Azcem/6b4a9e5b806669324d8a53c32546427a/quine-3-gen.png?w=1440&amp;q=80\" width=\"720\" height=\"437.78620166793024\" loading=\"lazy\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </picture> </figure><p>Each quine has <a href=\"https://larvalabs.com/writing/2025-11-4-18-0/a-field-guide-to-quine\">a number of characteristics</a>, including one that Matt and John refer to as its ‚Äúquinity‚Äù. This can be a 3-Quine, a 5-Quine, a 7-Quine, etc. Basically, the quinity relates to the quine‚Äôs generation loop. For a 3-Quine, if you would repeatedly extract and run the code within each quine generation, after the third generation, it‚Äôll loop back to the beginning. A 5-Quine will loop after the fifth generation, etc. As an engineer, I caught myself chuckling and shaking my head at how cool and clever this is. And, if you‚Äôre a sucker for collectibles, it gets even more interesting. Apparently, there are two very rare types of quines, known as a ‚ÄúPerfect-Quines‚Äù and ‚ÄúPseudo-Quines‚Äù. Perfect-Quines, when executed, will only ever recreate themselves. And, Pseudo-Quines will generate an effectively infinite number of generations without ever looping. At this point, I went from intrigued to <em>hooked</em>.</p><p>So, where does animation come into play? If the written explanation of Quine had you seeing the math meme, you‚Äôre not alone. Sometimes the easiest way to explain something complex is through visuals. Matt and John thought that an animation simulating the generation process would go a long way to demonstrate the overall concept. That‚Äôs where I come in.</p><p>At its core, the code that generates and is embedded within each quine is JavaScript, and when executed, this code generates an SVG. Coincidentally, I happen to have a <em>ton</em> of experience animating SVGs from my time at <a href=\"https://stripe.com/\">Stripe</a>, so I‚Äôm right at home with this project. The SVGs that make up these quines, however, are <em>very</em> dense to say the least. Each quine is 1440x2560 pixels in dimension, and the squares that construct each quine are 14x14 pixels with a 2-pixel gap between them. This means that a single quine as an SVG could potentially have up to 14,400 <code>&lt;rect&gt;</code> elements. On top of this, each quine could also include between 3,500 and 4,500 <code>&lt;tspan&gt;</code> elements from the code printed within it. For anyone familiar with SVG performance, this is <em>a lot</em> of elements to display, let alone animate. I didn‚Äôt waste any time even considering we stick with SVGs before immediately switching my attention to <code>&lt;canvas&gt;</code>‚Äîthe hardware-accelerated HTML element that can animate thousands of shapes without breaking a sweat. Even though Canvas can run circles around SVGs, I still felt compelled to do some performance testing.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/1BO5ZoVZXYyjIwFKmHBSZI/7a3ca77303651e6c2cfc27f7516f279c/quine-perf-test-crop.jpg\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/L00DFtszBQqtbK8lYg4w8/d57c785e4680aaef74cca49904a8b26b/quine-perf-test-crop.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/L00DFtszBQqtbK8lYg4w8/d57c785e4680aaef74cca49904a8b26b/quine-perf-test-crop.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Performance testing</p> </figcaption> </figure><p>I started by testing the extreme‚Äîanimating all the shapes individually. I had no intention of actually running with this approach, and I knew it would perform the slowest, but I still wanted to establish a baseline to gauge <em>how</em> slow this would be‚Äîsurprisingly, not bad! I then tried animating the text that would represent the code. Instead of animating the positions again, I focused on transforming scale. Again, not bad! Next, I tested the more realistic approach‚Äîanimating elements as groups. This would certainly improve performance because I‚Äôm not only animating a single layer, but I‚Äôm also avoiding the iteration loop on every element. Lastly, I tested animating groups of elements that are layered behind text. Because of the flat nature of <code>&lt;canvas&gt;</code>, I didn‚Äôt expect this to factor into performance at all, but I was still curious to see it as a disorienting visual!</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/6Peys8I2Lzna5414BKybpY/6a6c8e8b9cafb86e05072c5b36a661e4/image.png\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/5hqaX4IhILg8WQ6WkuG0dk/6382f5bdbfe46e3559922f49b4c4f196/quine-printing-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/5hqaX4IhILg8WQ6WkuG0dk/6382f5bdbfe46e3559922f49b4c4f196/quine-printing-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine printing variations</p> </figcaption> </figure><p>Once I had a better grasp on performance, I moved onto the actual animation. When thinking about the quine generation process, I immediately imagined a quine being ‚Äúprinted,‚Äù like an inkjet printer‚Äîline by line in a single direction. From the performance tests involving layered groups, I also imagined printing individual colors as separate passes, like with screen printing. Combining the two metaphors, I iterated through several variations:</p><ul><li><p>single direction linear passes where the ‚Äúink‚Äù carries with the squeegee until it reaches its spot</p></li><li><p>single direction passes with easing</p></li><li><p>bi-directional passes with easing</p></li><li><p>single direction linear passes with a printer line that reveals each row</p></li></ul><p>I found myself most drawn to the mechanical approach‚Äîslow and linear. I wanted the quine to emerge from each layer with a sense of anticipation. </p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/4D3IQsvwgIVtVRcoHOv3zh/207f04174879141210fe96217268e692/image.png\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/3Tidl5OI6u0MHjxbHDBhGd/bad99afd3cf03b87e327d7389beb0d3f/quine-text-60fps-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/3Tidl5OI6u0MHjxbHDBhGd/bad99afd3cf03b87e327d7389beb0d3f/quine-text-60fps-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine code inversion</p> </figcaption> </figure><p>Now that I had the movement dialed in, I zoomed in on the detailed part of the quine‚Äîthe code within it. Originally, I ‚Äúprinted‚Äù the code together with its squares, but this buried the lede too much. By printing them together, they held equal weight despite the code being the reason the quine even existed. And to the untrained eye, you might even miss the code altogether if it‚Äôs printed along with the squares. Thinking back to the concept of layers, I decided to try printing the code first‚Äîon its own layer‚Äîto emphasize that it can stand alone and that the viewer should notice it. Only then do I start printing the layers of squares to combine the two. In doing so, I introduced another transition opportunity‚Äîthe code inverting itself as the squares are printed. With this visual, we really drive home the message that there‚Äôs code within the quine.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/38HGIflioTZPUDsFbyUbWk/fe2e1252eb97226a3c4c3e9206bdf847/image.jpeg\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/6BqKFeGeVIdtSGJiXDlbsc/e5b24672684d50e34575db77160818f0/quine-scan-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/6BqKFeGeVIdtSGJiXDlbsc/e5b24672684d50e34575db77160818f0/quine-scan-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine scanning animation</p> </figcaption> </figure><p>With the printing animation behind me, it was time to move onto the <em>scanning</em> animation. I already highlighted that there‚Äôs code within the quine, but I also needed to communicate that the code is both meaningful and legible‚Äînot just a random snippet. People typically interact with code using an IDE, so why not include one in the demonstration? Taking a page out of the text streaming effect often seen in AI chat interfaces, I decided to combine this effect with a faux ‚Äúscanner‚Äù that passes over the original quine. As simple and straightforward as this is, it gets the point across that the code within the quine is actually real <em>and</em> recognizable when displayed in a familiar format and environment.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/4kjuQQgNYanKeL2F1uBLlr/a0c9104fa7bba32a2862de976cddfdf8/quine-second-generation-poster.jpeg\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/6C13auvV7WGQLsR9nPALrz/2dd10c98bfae852009557e9c1e46e95f/quine-2nd-generation-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/6C13auvV7WGQLsR9nPALrz/2dd10c98bfae852009557e9c1e46e95f/quine-2nd-generation-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine generation animation</p> </figcaption> </figure><p>Now that I had printing and scanning animations in a good place, it was time to combine the two. I would start with the printing animation for the original quine generation, but then I‚Äôd reuse it for the next generation, in parallel with the scanning animation. Even though I built these animations separately, they locked into place so well that I had no notes. The overall animation continued to write itself as long as I continued to lean on the analog metaphors. Print the quine. Scan the quine. Scanning the quine prints the quine.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/gcYV0DqZJFOS6fY9dpsB8/f0389dc5d4c6f07a6ecbd39f7187ded1/quine-interview-poster.jpeg\" controls=\"true\" false=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/24aOLpy2m8vTGVuhImWvAM/32e9f1fec3ed46ec4ebb9e9480160977/quine-interview-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/24aOLpy2m8vTGVuhImWvAM/32e9f1fec3ed46ec4ebb9e9480160977/quine-interview-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>The final Quine announcement video (animation at 0:52)</p> </figcaption> </figure><p>Upon screen recording the animation, I had my deliverable‚Äîan animation that would be used as b-roll for Matt and John‚Äôs announcement video. And, thanks to the animation being in video format instead of live and interactive, I didn‚Äôt need to worry about all of the typical considerations that I‚Äôm used to, like responsiveness, performance on slower machines, compatibility with different browsers, or what even happens next. I could focus on a single animation that visualizes the core concept while Matt voices over the it with the verbal explanation. Larva Labs announced Quine on <a href=\"https://www.artblocks.io/exhibitions/quine-by-larva-labs\">Art Blocks</a>, where they also held their auction for 477 of the 497 quines. The auction was another success with the sale closing at 7.56 ETH ($31,000) per Quine NFT.</p><hr><p>Once the dust settled from the auction, Larva Labs reached out again regarding an extension of the project. In a few weeks, they would be displaying Quine in gallery format at <a href=\"https://www.artbasel.com/miami-beach\">Art Basel</a> in Miami. In addition to framed prints of several quines on the walls and a massively long table with a grid of every quine, they would also have a 4K TV to, again, aid in visually demonstrating the concept, but as a looping video this time. The video format deliverable still saved me from the considerations that go into a live animation, but now I needed to ‚Äúupgrade‚Äù the animation. Instead of finishing the animation after scanning and printing once, it would need to generate an entire quine‚Äôs sequence with transitions, automatically proceed to the next quine, then loop back to the beginning when finished. John provided me with a list of 10 quines, with quinities ranging from 3 to 11. This would result in an animation that is 8 minutes and 28 seconds in length.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/1GVU0DQZ1VxP0TwzbUHWip/3547c1693023f192a34f7843c9bb80e8/quine-sequence-poster.jpeg\" controls=\"true\" false=\"\" loop=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/3jYnkJUyEFJ7qlhW1l2RB3/5e23f195808ec2fe5815aac3793e7ace/quine-sequence-o.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/3jYnkJUyEFJ7qlhW1l2RB3/5e23f195808ec2fe5815aac3793e7ace/quine-sequence-o.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine sequence animation</p> </figcaption> </figure><p>Luckily, the layout of the quine generation flow, along with my intuition for how it should move, led me to nail it on the first try. Upon scanning and printing the next generation in a quine‚Äôs sequence, the two generations and its ‚Äúcode editor‚Äù would slide and fade, so the generated image would become the source image, which would get scanned and print the following generation. In theory, I could let the animation run uninterrupted and it would loop forever because my actual animation code literally extracts the quine‚Äôs code and uses it to generate the next animation. A generator of generators, itself.</p><figure> <div> <p> <video poster=\"https://images.ctfassets.net/zi79s2th73f3/3FjPob6h0nTQ2BmB7aP32h/6e536b3105972740b44c11517ab672a7/quine-art-basel-poster.jpeg\" controls=\"true\" false=\"\" tabindex=\"0\" onload=\"this.dataset.jsLoaded=&quot;&quot;\" style=\"max-width: 100%; height: auto; border-radius: 6px; margin: 10px 0;\"> <source src=\"https://videos.ctfassets.net/zi79s2th73f3/680euiqB1C8xoupSjAwBkT/f5a5ee3f43730fe1211f47160f1f4e76/quine-art-basel.mp4\" type=\"video/mp4\"> Video tag not supported. Download the video <a href=\"https://videos.ctfassets.net/zi79s2th73f3/680euiqB1C8xoupSjAwBkT/f5a5ee3f43730fe1211f47160f1f4e76/quine-art-basel.mp4\">here</a>. </video> </p> </div> <figcaption> <p>Quine at Art Basel (animation at 0:05, 0:19, and 0:40)</p> </figcaption> </figure><p>It might go without saying, but I absolutely loved working on this project. After years of solely working on product and web, life presented me with an opportunity to return to the creative code world‚Äîin a practical sense. I also got to collaborate with close friends I haven‚Äôt jammed with since the last time I freelanced in 2018. This truly felt like the perfect reintroduction to independent life, and it was all because I bumped into John on the walk home from the studio that night. </p></section> <!---->"},{"id":"https://catalinionescu.dev/ai-agent/building-ai-agent-part-1/","title":"Building an AI agent inside a 7-year-old Rails monolith","link":"https://catalinionescu.dev/ai-agent/building-ai-agent-part-1/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46390055","content":"<a href=\"https://news.ycombinator.com/item?id=46390055\">Comments</a>","date":1766734515000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div data-toc-content=\"\"><p>I‚Äôm a Director of Engineering at <a href=\"https://www.monami.io/\">Mon Ami</a>, a US-based start-up building a SaaS solution for Aging and Disability Case Workers. We built a large Ruby on Rails monolith over the last 7 years.</p><p>It‚Äôs a multi-tenant solution where data sensitivity is crucial. We have multiple layers of access checks, but to simplify the story, we‚Äôll assume it‚Äôs all abstracted away into a <a href=\"https://github.com/varvet/pundit\">Pundit</a> policy.</p><p>While I would not describe us as a group dealing with Big Data problems, we do have <em>a lot</em> of data. Looking up clients‚Äô records is, in particular, an action that is just not performant enough with raw database operations, so we built an Algolia index to make it work.</p><p>Given all that: the big monolith, complicated data access rules, and the nature of the business we are in, building an AI agent has not yet been a primary concern for us.</p><h2 id=\"sf-ruby-and-the-disconnect\">SF Ruby, and the disconnect</h2><p>I was at <a href=\"https://sfruby.com/\">SF Ruby</a>, in San Francisco, a few weeks ago. Most of the tracks were, of course, heavily focused on AI. Lots of stories from people building AIs into all sorts of products using Ruby and Rails,</p><p>They were good talks. But most of them assumed a kind of software I don‚Äôt work on ‚Äî systems without strong boundaries, without multi-tenant concerns, without deeply embedded authorization rules.</p><p>I kept thinking: this is interesting, but it doesn‚Äôt map cleanly to my world. At Mon Ami, we can‚Äôt just release a pilot unless it passes strict data access checks.</p><p>Then I saw a talk about using the RubyLLM gem to build a RAG-like system. The conversation (LLM calls) context was augmented using function calls (tools). This is when it clicked. I could encode my complicated access logic into a specific function call and ensure the LLM gets access to some of our data without having to give it unrestricted access.</p><h2 id=\"rubyllm\">RubyLLM</h2><p><a href=\"https://github.com/crmne/ruby_llm\">RubyLLM</a> is a neat gem that abstracts away the interaction with many LLM providers with a clean API.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>gem </span><span>\"ruby_llm\"</span></span></code></pre><p>It is configured in an initializer with the API keys for the providers you want to use.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>RubyLLM</span><span>.</span><span>configure</span><span> do</span><span> |config|</span></span> <span><span> config.</span><span>openai_api_key</span><span> =</span><span> Rails</span><span>.</span><span>application</span><span>.</span><span>credentials</span><span>.</span><span>dig</span><span>(</span><span>:openai_api_key</span><span>)</span></span> <span><span> config.</span><span>anthropic_api_key</span><span> =</span><span> Rails</span><span>.</span><span>application</span><span>.</span><span>credentials</span><span>.</span><span>dig</span><span>(</span><span>:anthropic_api_key</span><span>)</span></span> <span><span> # config.default_model = \"gpt-4.1-nano\"</span></span> <span><span> # Use the new association-based acts_as API (recommended)</span></span> <span><span> config.</span><span>use_new_acts_as</span><span> =</span><span> true</span></span> <span><span> # Increase timeout for slow API responses</span></span> <span><span> config.</span><span>request_timeout</span><span> =</span><span> 600</span><span> # 10 minutes (default is 300)</span></span> <span><span> config.</span><span>max_retries</span><span> =</span><span> 3</span><span> # Retry failed requests</span></span> <span><span>end</span></span> <span><span># Load LLM tools from main app</span></span> <span><span>Dir</span><span>[</span><span>Rails</span><span>.</span><span>root</span><span>.</span><span>join</span><span>(</span><span>'app/tools/**/*.rb'</span><span>)].</span><span>each</span><span> { |f| </span><span>require</span><span> f }</span></span></code></pre><p>It provides a Conversation model as an abstraction for an LLM thread. The Conversation contains a set of Messages. It also provides a way of defining structured responses and function calls available.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>AVAILABLE_TOOLS</span><span> =</span><span> [</span></span> <span><span> Tools</span><span>::</span><span>Client</span><span>::</span><span>SearchTool</span></span> <span><span>].</span><span>freeze</span></span> <span><span>conversation</span><span> =</span><span> Conversation</span><span>.</span><span>find</span><span>(conversation_id)</span></span> <span><span>chat</span><span> =</span><span> conversation.</span><span>with_tools</span><span>(</span><span>*</span><span>AVAILABLE_TOOLS</span><span>)</span></span> <span><span>chat.</span><span>ask</span><span> 'What is the phone number for John Snow?'</span></span></code></pre><p>A Conversation is initialized by passing a model (gpt-5, claude-sonnet-4.5, etc) and has a method for chatting to it.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>conversation</span><span> =</span><span> Conversation</span><span>.</span><span>new</span><span>(</span><span>model:</span><span> RubyLLM</span><span>::</span><span>Model</span><span>.</span><span>find_by</span><span>(</span><span>model_id:</span><span> 'gpt-4o-mini'</span><span>))</span></span></code></pre><p>RubyLLM comes with a neat DSL for defining accepted parameters (the descriptions are passed to the LLM as context since it needs to decide if the tool should be used based on the conversation). The tool implements an execute method returning a hash. The hash is then presented to the LLM. This is all the magic needed.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>class</span><span> SearchTool</span><span> &lt; </span><span>BaseTool</span></span> <span><span> description </span><span>'Search for clients by name, ID, or email address. Returns matching clients.'</span></span> <span><span> param </span><span>:query</span><span>,</span></span> <span><span> desc:</span><span> 'Search query - can be client name, ID, or email address'</span><span>,</span></span> <span><span> type:</span><span> :string</span></span> <span><span> def</span><span> execute</span><span>(</span><span>query:</span><span>)</span></span> <span><span> end</span></span> <span><span>end</span></span></code></pre><p>We‚Äôll now build a modest function call and a messaging interface. The function call allows searching a client using Algolia and ensuring the resulting set is visible to the user (by merging in the pundit policy).</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>def</span><span> execute</span><span>(</span><span>query:</span><span>)</span></span> <span><span> response</span><span> =</span><span> Algolia</span><span>::</span><span>SearchClient</span></span> <span><span> .</span><span>create</span><span>(app_id, search_key)</span></span> <span><span> .</span><span>search_single_index</span><span>(</span><span>Client</span><span>.</span><span>index_name</span><span>, {</span></span> <span><span> query:</span><span> query.</span><span>truncate</span><span>(</span><span>250</span><span>)</span></span> <span><span> })</span></span> <span><span> ids</span><span> =</span><span> response.</span><span>hits</span><span>.</span><span>map</span><span> { |hit| hit[</span><span>:id</span><span>] }.</span><span>compact</span></span> <span><span> base_scope</span><span> =</span><span> Client</span><span>.</span><span>where</span><span>(</span><span>id:</span><span> ids)</span></span> <span><span> client</span><span> =</span><span> Admin</span><span>::</span><span>Org</span><span>::</span><span>ClientPolicy</span><span>::</span><span>Scope</span><span>.</span><span>new</span><span>(base_scope).</span><span>resolve</span><span>.</span><span>first</span><span> or</span><span> return</span><span> {}</span></span> <span><span> {</span></span> <span><span> id:</span><span> client.</span><span>id</span><span>,</span></span> <span><span> ami_id:</span><span> client.</span><span>slug</span><span>,</span></span> <span><span> slug:</span><span> client.</span><span>slug</span><span>,</span></span> <span><span> name:</span><span> client.</span><span>full_name</span><span>,</span></span> <span><span> email:</span><span> client.</span><span>email</span></span> <span><span> }</span></span> <span><span>end</span></span></code></pre><p>The LLM acts as the magic glue between the natural language input submitted by the user, decides which (if any) tool to use to augment the context, and then responds to the user. No model should ever know Jon Snow‚Äôs phone number from a SaaS service, but this approach allows this sort of retrieval.</p><p>The UI is built with a remote form that enqueues an Active Job.</p><pre data-language=\"haml\" tabindex=\"0\"><code><span><span>= turbo_stream_from @conversation, </span><span>:messages</span></span> <span><span>.container-fluid.h-100.d-flex.flex-column</span></span> <span><span> .sticky-top</span></span> <span><span> %</span><span>h2</span><span>.mb-0</span></span> <span><span> Conversation ##{@conversation.id}</span></span> <span><span> .flex-grow-1</span></span> <span><span> = render @messages</span></span> <span><span> .p-3.border-top.bg-white.sticky-bottom#message-form</span></span> <span><span> = form_with </span><span>url:</span><span> path, </span><span>method:</span><span> :post</span><span>, </span><span>local:</span><span> false</span><span>, </span><span>data:</span><span> { </span><span>turbo_stream:</span><span> true</span><span> } </span><span>do</span><span> |f|</span></span> <span><span> = f.</span><span>text_area</span><span> :content</span></span> <span><span> = f.</span><span>submit</span><span> 'Send'</span></span></code></pre><p>The job will process the Message.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>class</span><span> ProcessMessageJob</span><span> &lt; </span><span>ApplicationJob</span></span> <span><span> queue_as </span><span>:default</span></span> <span><span> def</span><span> perform</span><span>(conversation_id, message)</span></span> <span><span> conversation</span><span> =</span><span> Conversation</span><span>.</span><span>find</span><span>(conversation_id)</span></span> <span><span> conversation.</span><span>ask</span><span> message</span></span> <span><span> end</span></span> <span><span>end</span></span></code></pre><p>The conversation has broadcast refresh enabled to update the UI when the response is received.</p><pre data-language=\"ruby\" tabindex=\"0\"><code><span><span>class</span><span> Conversation</span><span> &lt; </span><span>RubyLLM::Conversation</span></span> <span><span> broadcasts_refreshes</span></span> <span><span>end</span></span></code></pre><p>The form has a stimulus controller that checks for new messages being appended in order to scroll to the end of the conversation.</p><h2 id=\"a-note-on-selecting-the-model\">A note on selecting the model</h2><p>I checked a few OpenAI models for this implementation: gpt-5, gpt-4o, gpt4. GPT-5 has a big context, meaning we could have long-running conversations, but because there are a number of round-trips, the delay to queries requiring 3+ consecutive tools made the Agent feel sluggish.</p><p>GPT-4, on the other hand, is interestingly very prone to hallucinations - rushing to respond to queries with made-up data instead of calling the necessary tools. GPT-4o strikes, so far, the best balance between speed and correctness.</p><h2 id=\"closing-thoughts\">Closing thoughts</h2><p>Building this tool took probably about 2-3 days of Claude-powered development (AIs building AIs). The difficulty and the complexity of building such a tool were the things that surprised me the most. The tool service object is essentially an API controller action - pass inputs and get a JSON back. Interestingly.</p><p>Before building this Agent, I looked at the other gems in this space. ActiveAgent (a somewhat similar gem for interacting with LLMs) is a decent contender that moves the prompts to a view file. It didn‚Äôt fit my needs since it had no built-in support for defining tools or having long-running conversations.</p></div></div>"},{"id":"https://fidget-spinner.github.io/posts/no-longer-sorry.html","title":"Python 3.15‚Äôs interpreter for Windows x86-64 should hopefully be 15% faster","link":"https://fidget-spinner.github.io/posts/no-longer-sorry.html","hnCommentsUrl":"https://news.ycombinator.com/item?id=46384167","content":"<a href=\"https://news.ycombinator.com/item?id=46384167\">Comments</a>","date":1766667766000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div> <h2><a href=\"https://fidget-spinner.github.io/\">Ken Jin</a></h2> <p>24 December 2025</p> <p>Some time ago I posted an <a href=\"https://fidget-spinner.github.io/posts/apology-tail-call.html\">apology piece</a> for Python‚Äôs tail calling results. I apologized for communicating performance results without noticing a compiler bug had occured.</p> <p>I can proudly say today that I am partially retracting that apology, but only for two platforms‚ÄîmacOS AArch64 (XCode Clang) and Windows x86-64 (MSVC).</p> <p>In our own experiments, the tail calling interpreter for CPython was found to beat the computed goto interpreter by 5% on pyperformance on AArch64 macOS using XCode Clang, and roughly 15% on pyperformance on Windows on an experimental internal version of MSVC. The Windows build is against a switch-case interpreter, but this in theory shouldn‚Äôt matter too much, more on that in the next section.</p> <p>This is of course, a <strong>hopefully accurate</strong> result. I tried to be more diligent here, but I am of course not infallible. However, I have found that sharing early and making a fool of myself often works well, as it has led to people catching bugs in my code, so I shall continue doing so :).</p> <p>Also this assumes the change doesn‚Äôt get reverted later in Python 3.15‚Äôs development cycle.</p> <h2 id=\"brief-background-on-interpreters\">Brief background on interpreters</h2> <p>Just a recap. There are two popular current ways of writing C-based interpreters.</p> <p>Switch-cases:</p> <pre><code>switch (opcode) { case INST_1: ... case INST_2: ... } </code></pre> <p>Where we just switch-case to the correct instruction handler.</p> <p>And the other popular way is a GCC/Clang extension called labels-as-values/computed gotos.</p> <pre><code>goto *dispatch_table[opcode]; INST_1: ... INST_2: ... </code></pre> <p>Which is basically the same idea, but to instead jump to the address of the next label. Traditionally, the key optimization here is that it needs only one jump to go to the next instruction, while in the switch-case interpreter, a naiive compiler would need two jumps.</p> <p>With modern compilers however, the benefits of the computed gotos is a lot less, mainly because modern compilers have gotten better and modern hardware has also gotten better. In Nelson Elhage‚Äôs <a href=\"https://blog.nelhage.com/post/cpython-tail-call/\">excellent investigation</a> on the next kind of interpreter, the speedup of computed gotos over switch case on modern Clang was only in the low single digits on pyperformance.</p> <p>A 3rd way that was suggested decades ago, but not really entirely feasible is call/tail-call threaded interpreters. In this scheme, each bytecode handler is its own function, and we tail-call from one handler to the next in the instruction stream:</p> <div><pre><code>return dispatch_table[opcode]; PyObject *INST_1(...) { } PyObject *INST_2(...) { } </code></pre></div> <p>This wasn‚Äôt too feasible in C for one main reason‚Äîtail call optimization was merely an <em>optimization</em>. It‚Äôs something the C compiler might do, or might not do. This means if you‚Äôre unlucky and the C compiler chooses not to perform the tail call, your interpreter might stack overflow!</p> <p>Some time ago, Clang introduced <code>__attribute__((musttail))</code>, which allowed for mandating that a call <em>must</em> be tail-called. Otherwise, the compilation will fail. To my knowledge, the first time this was popularized for use in a mainstream interpreter was in <a href=\"https://blog.reverberate.org/2021/04/21/musttail-efficient-interpreters.html\">Josh Haberman‚Äôs Protobuf blog post</a>.</p> <p>Later on, Haoran Xu noticed that the GHC calling convention combined with tail calls produced efficient code. They used this for their baseline JIT in a paper and termed the technique <a href=\"https://dl.acm.org/doi/abs/10.1145/3485513\">Copy-and-Patch</a>.</p> <h2 id=\"so-where-are-we-now\">So where are we now?</h2> <p>After using a fixed XCode Clang, our performance numbers on CPython 3.14/3.15 suggest that the tail calling interpreter does provide a modest speedup over computed gotos. Around the 5% geomean range on pyperformance.</p> <p>To my understanding, <code>uv</code> already ships Python 3.14 on macOS with tail calling, which might be responsible for some of the speedups you see on there. We‚Äôre planning to ship the official 3.15 macOS binaries on <code>python.org</code> with tail calling as well.</p> <p>However, you‚Äôre not here for that. The title of this blog post is clearly about MSVC Windows x86-64. So what about that?</p> <h2 id=\"tail-calling-for-windows\">Tail-calling for Windows</h2> <blockquote> <p>[!CAUTION] The features for MSVC discussed below are to my knowledge, experimental. They are not guaranteed to always be around unless the MSVC team decide to keep them. Use at your own risk!</p> </blockquote> <p>These are the preliminary pyperformance results for CPython on MSVC with tail-calling vs switch-case. Any number above 1.00x is a speedup (e.g. <code>1.01x == 1% speedup</code>), anything below 1.00x is a slowdown. The speedup is a geomtric mean of around 15-16%, with a range of ~60% slowdown (one or two outliers) to 78% speedup. However, the key thing is that the vast majority of benchmaarks sped up!</p> <p><img src=\"https://fidget-spinner.github.io/posts/media/TC-PGO-Ex3-vs-PGO.svg\" alt=\"Tailcall results\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></p> <p><em>Chart credits to Michael Droettboom</em></p> <blockquote> <p>[!WARNING] These results are on an experimental internal MSVC compiler, public results below.</p> </blockquote> <p>To verify this and make sure I wasn‚Äôt wrong yet again, I checked the results on my machine with Visual Studio 2026. These are the results from <a href=\"https://github.com/python/cpython/issues/139922\">this issue</a>.</p> <div><pre><code>Mean +- std dev: [spectralnorm_tc_no] 146 ms +- 1 ms -&gt; [spectralnorm_tc] 98.3 ms +- 1.1 ms: 1.48x faster Mean +- std dev: [nbody_tc_no] 145 ms +- 2 ms -&gt; [nbody_tc] 107 ms +- 2 ms: 1.35x faster Mean +- std dev: [bm_django_template_tc_no] 26.9 ms +- 0.5 ms -&gt; [bm_django_template_tc] 22.8 ms +- 0.4 ms: 1.18x faster Mean +- std dev: [xdsl_tc_no] 64.2 ms +- 1.6 ms -&gt; [xdsl_tc] 56.1 ms +- 1.5 ms: 1.14x faster </code></pre></div> <p>So yeah, the speedups are real! For a large-ish library like xDSL, we see a 14% speedup, while for smaller microbenchmarks like nbody and spectralnorm, the speedups are greater.</p> <p>Thanks to Chris Eibl and Brandt Bucher, we managed to get the <a href=\"https://github.com/python/cpython/pull/143068\">PR for this</a> on MSVC over the finish line. I also want to sincerely thank the MSVC team. I can‚Äôt say this enough: they have been a joy to work with and I‚Äôm very impressed by what they‚Äôve done, and I want to congratulate them on releasing Visual Studio 2026.</p> <p>This is now listed in the What‚Äôs New for 3.15 notes:</p> <blockquote> <p>Builds using Visual Studio 2026 (MSVC 18) may now use the new tail-calling interpreter. Results on an early experimental MSVC compiler reported roughly 15% speedup on the geometric mean of pyperformance on Windows x86-64 over the switch-case interpreter. We have observed speedups ranging from 15% for large pure-Python libraries to 40% for long-running small pure-Python scripts on Windows. (Contributed by Chris Eibl, Ken Jin, and Brandt Bucher in gh-143068. Special thanks to the MSVC team including Hulon Jenkins.)</p> </blockquote> <p>This is the <a href=\"https://learn.microsoft.com/en-us/cpp/cpp/attributes?view=msvc-170#msvcmusttail\">documentation for [[msvc::musttail]]</a>.</p> <h3 id=\"where-exactly-do-the-speedups-come-from\">Where exactly do the speedups come from?</h3> <p>I used to believe the the tail calling interpreters get their speedup from better register use. While I still believe that now, I suspect that is not the main reason for speedups in CPython.</p> <p>My main guess now is that <strong>tail calling resets compiler heuristics to sane levels, so that compilers can do their jobs</strong>.</p> <p>Let me show an example, at the time of writing, CPython 3.15‚Äôs interpreter loop is around <a href=\"https://github.com/python/cpython/blob/main/Python/generated_cases.c.h\">12k</a> lines of C code. That‚Äôs 12k lines in a <strong>single</strong> function for the switch-case and computed goto interpreter.</p> <p>This has caused many issues for compilers in the past, too many to list in fact. I have a EuroPython 2025 talk about this. In short, this overly large function breaks a lot of compiler heuristics.</p> <p>One of the most beneficial optimisations is inlining. In the past, we‚Äôve found that compilers sometimes straight up <a href=\"https://github.com/python/cpython/issues/121263\">refuse</a> to inline even the simplest of functions in that 12k loc eval loop. I want to stress that this is not the fault of the compiler. It‚Äôs actually doing the correct thing‚Äîyou usually don‚Äôt want to increase the code size of something already super large. Unfortunately, this does‚Äôt bode well for our interpreter.</p> <p>You might say just write the interpreter in assembly! However, the whole point of this exercise is to not do that.</p> <p>Ok enough talk, let‚Äôs take a look at the code now. Taking a real example, we examine <code>BINARY_OP_ADD_INT</code> which adds two Python integers. Cleaning up the code so it‚Äôs readable, things look like this:</p> <pre><code>TARGET(BINARY_OP_ADD_INT) { // Increment the instruction pointer. _Py_CODEUNIT* const this_instr = next_instr; frame-&gt;instr_ptr = next_instr; next_instr += 6; _PyStackRef right = stack_pointer[-1]; // Check that LHS is an int. PyObject *value_o = PyStackRef_AsPyObjectBorrow(left); if (!_PyLong_CheckExactAndCompact(value_o)) { JUMP_TO_PREDICTED(BINARY_OP); } // Check that RHS is an int. // ... (same code as above for LHS) // Add them together. PyObject *left_o = PyStackRef_AsPyObjectBorrow(left); PyObject *right_o = PyStackRef_AsPyObjectBorrow(right); res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o); // If the addition fails, fall back to the generic instruction. if (PyStackRef_IsNull(res)) { JUMP_TO_PREDICTED(BINARY_OP); } // Close the references. PyStackRef_CLOSE_SPECIALIZED(left, _PyLong_ExactDealloc); PyStackRef_CLOSE_SPECIALIZED(right, _PyLong_ExactDealloc); // Write to the stack, and dispatch. stack_pointer[-2] = res; stack_pointer += -1; DISPATCH(); } </code></pre> <p>Seems simple enough, let‚Äôs take a look at the assembly for switch-case on VS 2026. Note again, this is a non-PGO build for easy source information, PGO generally makes some of these problems go away, but not all of them:</p> <pre><code> if (!_PyLong_CheckExactAndCompact(value_o)) { 00007FFC4DE24DCE mov rcx,rbx 00007FFC4DE24DD1 mov qword ptr [rsp+58h],rax 00007FFC4DE24DD6 call _PyLong_CheckExactAndCompact (07FFC4DE227F0h) 00007FFC4DE24DDB test eax,eax 00007FFC4DE24DDD je _PyEval_EvalFrameDefault+10EFh (07FFC4DE258FFh) ... res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o); 00007FFC4DE24DFF mov rdx,rbx 00007FFC4DE24E02 mov rcx,r15 00007FFC4DE24E05 call _PyCompactLong_Add (07FFC4DD34150h) 00007FFC4DE24E0A mov rbx,rax ... PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc); 00007FFC4DE24E17 lea rdx,[_PyLong_ExactDealloc (07FFC4DD33BD0h)] 00007FFC4DE24E1E mov rcx,rsi 00007FFC4DE24E21 call PyStackRef_CLOSE_SPECIALIZED (07FFC4DE222A0h) </code></pre> <p>Huh‚Ä¶ all our functions were not inlined. Surely that must‚Äôve mean they were too big or something right? Let‚Äôs look at <code>PyStackReF_CLOSE_SPECIALIZED</code>:</p> <pre><code>static inline void PyStackRef_CLOSE_SPECIALIZED(_PyStackRef ref, destructor destruct) { assert(!PyStackRef_IsNull(ref)); if (PyStackRef_RefcountOnObject(ref)) { Py_DECREF_MORTAL_SPECIALIZED(BITS_TO_PTR(ref), destruct); } } </code></pre> <p>That looks ‚Ä¶ inlineable?</p> <p>Here‚Äôs how <code>BINARY_OP_ADD_INT</code> looks with tail calling on VS 2026 (again, no PGO):</p> <pre><code> if (!_PyLong_CheckExactAndCompact(left_o)) { 00007FFC67164785 cmp qword ptr [rax+8],rdx 00007FFC67164789 jne _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h) 00007FFC6716478F mov r9,qword ptr [rax+10h] 00007FFC67164793 cmp r9,10h 00007FFC67164797 jae _TAIL_CALL_BINARY_OP_ADD_INT@@_A+149h (07FFC67164879h) ... res = _PyCompactLong_Add((PyLongObject *)left_o, (PyLongObject *)right_o); 00007FFC6716479D mov eax,dword ptr [rax+18h] 00007FFC671647A0 and r9d,3 00007FFC671647A4 and r8d,3 00007FFC671647A8 mov edx,1 00007FFC671647AD sub rdx,r9 00007FFC671647B0 mov ecx,1 00007FFC671647B5 imul rdx,rax 00007FFC671647B9 mov eax,dword ptr [rbx+18h] 00007FFC671647BC sub rcx,r8 00007FFC671647BF imul rcx,rax 00007FFC671647C3 add rcx,rdx 00007FFC671647C6 call medium_from_stwodigits (07FFC6706E9E0h) 00007FFC671647CB mov rbx,rax ... PyStackRef_CLOSE_SPECIALIZED(value, _PyLong_ExactDealloc); 00007FFC671647EB test bpl,1 00007FFC671647EF jne _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch) 00007FFC671647F1 add dword ptr [rbp],0FFFFFFFFh 00007FFC671647F5 jne _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0ECh (07FFC6716481Ch) 00007FFC671647F7 mov rax,qword ptr [_PyRuntime+25F8h (07FFC675C45F8h)] 00007FFC671647FE test rax,rax 00007FFC67164801 je _TAIL_CALL_BINARY_OP_ADD_INT@@_A+0E4h (07FFC67164814h) 00007FFC67164803 mov r8,qword ptr [_PyRuntime+2600h (07FFC675C4600h)] 00007FFC6716480A mov edx,1 00007FFC6716480F mov rcx,rbp 00007FFC67164812 call rax 00007FFC67164814 mov rcx,rbp 00007FFC67164817 call _PyLong_ExactDealloc (07FFC67073DA0h) </code></pre> <p>Would you look at that, suddenly our trivial functions get inlined :).</p> <p>You might also say, surely this does not happen on PGO builds? Well the issue I linked above actually says it does! So yeah happy days.</p> <p>Once again I want to stress, this is not the compiler‚Äôs fault! It‚Äôs just that the CPython interpreter loop is not the best thing to optimize.</p> <h3 id=\"how-do-i-try-this-out\">How do I try this out?</h3> <p>Unfortunately, for now, you will have to build from source.</p> <p>With VS 2026, after cloning CPython, for a release build with PGO:</p> <div><pre><code><span>$</span><span>env</span><span>:</span><span>PlatformToolset</span><span>=</span><span>\"v145\"</span><span>.</span><span>/PCbuild/build.bat</span><span>--tail-call-interp</span><span>-c</span><span>Release</span><span>-p</span><span>x64</span><span>--pgo</span></code></pre></div> <p>Hopefully, we can distribute this in an easier binary form in the future once Python 3.15‚Äôs development matures!</p> <h2 id=\"addendum--edits\">Addendum &amp; Edits</h2> <p>I was asked for a cross-compiler test. So here‚Äôs a quick and dirty toy benchmark of pystones. The last row is the tail call enabled build. All configurations have PGO. On this toy benchmark, we get roughly a 30% uplift. Note that this is unscientific as it was only a sample size of 1 and I cannot disable Turbo Boost on my laptop on Windows for some reason.</p> <table> <thead> <tr> <th>Compiler</th> <th>PlatformToolSet</th> <th>Pystones/second (higher is better)</th> </tr> </thead> <tbody> <tr> <td>VS2019</td> <td>142</td> <td>677544</td> </tr> <tr> <td>VS2022</td> <td>143</td> <td>710773</td> </tr> <tr> <td>VS2026</td> <td>145</td> <td>682089</td> </tr> <tr> <td>VS2026+TC</td> <td>145</td> <td>970306</td> </tr> </tbody> </table> </div></div>"},{"id":"https://docs.python.org/3.15/library/profiling.sampling.html","title":"Tachyon: High frequency statistical sampling profiler","link":"https://docs.python.org/3.15/library/profiling.sampling.html","hnCommentsUrl":"https://news.ycombinator.com/item?id=46353257","content":"<a href=\"https://news.ycombinator.com/item?id=46353257\">Comments</a>","date":1766402502000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><section id=\"module-profiling.sampling\" role=\"main\"> <p><span>Added in version 3.15.</span></p> <p><strong>Source code:</strong> <a href=\"https://github.com/python/cpython/tree/main/Lib/profiling/sampling/\">Lib/profiling/sampling/</a></p> <hr> <p>The <a href=\"#module-profiling.sampling\" title=\"profiling.sampling: Statistical sampling profiler for Python processes.\"><code><span>profiling.sampling</span></code></a> module, named <strong>Tachyon</strong>, provides statistical profiling of Python programs through periodic stack sampling. Tachyon can run scripts directly or attach to any running Python process without requiring code changes or restarts. Because sampling occurs externally to the target process, overhead is virtually zero, making Tachyon suitable for both development and production environments.</p> <section id=\"what-is-statistical-profiling\"> <h2>What is statistical profiling?<a href=\"#what-is-statistical-profiling\" title=\"Link to this heading\">¬∂</a></h2> <p>Statistical profiling builds a picture of program behavior by periodically capturing snapshots of the call stack. Rather than instrumenting every function call and return as deterministic profilers do, Tachyon reads the call stack at regular intervals to record what code is currently running.</p> <p>This approach rests on a simple principle: functions that consume significant CPU time will appear frequently in the collected samples. By gathering thousands of samples over a profiling session, Tachyon constructs an accurate statistical estimate of where time is spent. The more samples collected, the more precise this estimate becomes.</p> <section id=\"how-time-is-estimated\"> <h3>How time is estimated<a href=\"#how-time-is-estimated\" title=\"Link to this heading\">¬∂</a></h3> <p>The time values shown in Tachyon‚Äôs output are <strong>estimates derived from sample counts</strong>, not direct measurements. Tachyon counts how many times each function appears in the collected samples, then multiplies by the sampling interval to estimate time.</p> <p>For example, with a 10 kHz sampling rate over a 10-second profile, Tachyon collects approximately 100,000 samples. If a function appears in 5,000 samples (5% of total), Tachyon estimates it consumed 5% of the 10-second duration, or about 500 milliseconds. This is a statistical estimate, not a precise measurement.</p> <p>The accuracy of these estimates depends on sample count. With 100,000 samples, a function showing 5% has a margin of error of roughly ¬±0.5%. With only 1,000 samples, the same 5% measurement could actually represent anywhere from 3% to 7% of real time.</p> <p>This is why longer profiling durations and shorter sampling intervals produce more reliable results‚Äîthey collect more samples. For most performance analysis, the default settings provide sufficient accuracy to identify bottlenecks and guide optimization efforts.</p> <p>Because sampling is statistical, results will vary slightly between runs. A function showing 12% in one run might show 11% or 13% in the next. This is normal and expected. Focus on the overall pattern rather than exact percentages, and don‚Äôt worry about small variations between runs.</p> </section> <section id=\"when-to-use-a-different-approach\"> <h3>When to use a different approach<a href=\"#when-to-use-a-different-approach\" title=\"Link to this heading\">¬∂</a></h3> <p>Statistical sampling is not ideal for every situation.</p> <p>For very short scripts that complete in under one second, the profiler may not collect enough samples for reliable results. Use <a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a> instead, or run the script in a loop to extend profiling time.</p> <p>When you need exact call counts, sampling cannot provide them. Sampling estimates frequency from snapshots, so if you need to know precisely how many times a function was called, use <a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a>.</p> <p>When comparing two implementations where the difference might be only 1-2%, sampling noise can obscure real differences. Use <a href=\"https://docs.python.org/3.15/library/timeit.html#module-timeit\" title=\"timeit: Measure the execution time of small code snippets.\"><code><span>timeit</span></code></a> for micro-benchmarks or <a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a> for precise measurements.</p> <p>The key difference from <a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a> is how measurement happens. A tracing profiler instruments your code, recording every function call and return. This provides exact call counts and precise timing but adds overhead to every function call. A sampling profiler, by contrast, observes the program from outside at fixed intervals without modifying its execution. Think of the difference like this: tracing is like having someone follow you and write down every step you take, while sampling is like taking photographs every second and inferring your path from those snapshots.</p> <p>This external observation model is what makes sampling profiling practical for production use. The profiled program runs at full speed because there is no instrumentation code running inside it, and the target process is never stopped or paused during sampling‚ÄîTachyon reads the call stack directly from the process‚Äôs memory while it continues to run. You can attach to a live server, collect data, and detach without the application ever knowing it was observed. The trade-off is that very short-lived functions may be missed if they happen to complete between samples.</p> <p>Statistical profiling excels at answering the question, ‚ÄúWhere is my program spending time?‚Äù It reveals hotspots and bottlenecks in production code where deterministic profiling overhead would be unacceptable. For exact call counts and complete call graphs, use <a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a> instead.</p> </section> </section> <section id=\"quick-examples\"> <h2>Quick examples<a href=\"#quick-examples\" title=\"Link to this heading\">¬∂</a></h2> <p>Profile a script and see the results immediately:</p> <div><pre><span>python -m profiling.sampling run script.py</span> </pre></div> <p>Profile a module with arguments:</p> <div><pre><span>python -m profiling.sampling run -m mypackage.module arg1 arg2</span> </pre></div> <p>Generate an interactive flame graph:</p> <div><pre><span>python -m profiling.sampling run --flamegraph -o profile.html script.py</span> </pre></div> <p>Attach to a running process by PID:</p> <div><pre><span>python -m profiling.sampling attach 12345</span> </pre></div> <p>Use live mode for real-time monitoring (press <code><span>q</span></code> to quit):</p> <div><pre><span>python -m profiling.sampling run --live script.py</span> </pre></div> <p>Profile for 60 seconds with a faster sampling rate:</p> <div><pre><span>python -m profiling.sampling run -d 60 -r 20khz script.py</span> </pre></div> <p>Generate a line-by-line heatmap:</p> <div><pre><span>python -m profiling.sampling run --heatmap script.py</span> </pre></div> <p>Enable opcode-level profiling to see which bytecode instructions are executing:</p> <div><pre><span>python -m profiling.sampling run --opcodes --flamegraph script.py</span> </pre></div> </section> <section id=\"commands\"> <h2>Commands<a href=\"#commands\" title=\"Link to this heading\">¬∂</a></h2> <p>Tachyon operates through two subcommands that determine how to obtain the target process.</p> <section id=\"the-run-command\"> <h3>The <code><span>run</span></code> command<a href=\"#the-run-command\" title=\"Link to this heading\">¬∂</a></h3> <p>The <code><span>run</span></code> command launches a Python script or module and profiles it from startup:</p> <div><pre><span>python -m profiling.sampling run script.py</span> <span>python -m profiling.sampling run -m mypackage.module</span> </pre></div> <p>When profiling a script, the profiler starts the target in a subprocess, waits for it to initialize, then begins collecting samples. The <code><span>-m</span></code> flag indicates that the target should be run as a module (equivalent to <code><span>python</span> <span>-m</span></code>). Arguments after the target are passed through to the profiled program:</p> <div><pre><span>python -m profiling.sampling run script.py --config settings.yaml</span> </pre></div> </section> <section id=\"the-attach-command\"> <h3>The <code><span>attach</span></code> command<a href=\"#the-attach-command\" title=\"Link to this heading\">¬∂</a></h3> <p>The <code><span>attach</span></code> command connects to an already-running Python process by its process ID:</p> <div><pre><span>python -m profiling.sampling attach 12345</span> </pre></div> <p>This command is particularly valuable for investigating performance issues in production systems. The target process requires no modification and need not be restarted. The profiler attaches, collects samples for the specified duration, then detaches and produces output.</p> <div><pre><span>python -m profiling.sampling attach --live 12345</span> <span>python -m profiling.sampling attach --flamegraph -d 30 -o profile.html 12345</span> </pre></div> <p>On most systems, attaching to another process requires appropriate permissions. See <a href=\"#profiling-permissions\"><span>Platform requirements</span></a> for platform-specific requirements.</p> </section> <section id=\"the-replay-command\"> <h3>The <code><span>replay</span></code> command<a href=\"#the-replay-command\" title=\"Link to this heading\">¬∂</a></h3> <p>The <code><span>replay</span></code> command converts binary profile files to other output formats:</p> <div><pre><span>python -m profiling.sampling replay profile.bin</span> <span>python -m profiling.sampling replay --flamegraph -o profile.html profile.bin</span> </pre></div> <p>This command is useful when you have captured profiling data in binary format and want to analyze it later or convert it to a visualization format. Binary profiles can be replayed multiple times to different formats without re-profiling.</p> <div><pre><span># </span>Convertbinarytopstats<span>(</span>default,printstostdout<span>)</span> <span>python -m profiling.sampling replay profile.bin</span> <span># </span>Convertbinarytoflamegraph <span>python -m profiling.sampling replay --flamegraph -o output.html profile.bin</span> <span># </span>Convertbinarytogeckoformat<span>for</span>FirefoxProfiler <span>python -m profiling.sampling replay --gecko -o profile.json profile.bin</span> <span># </span>Convertbinarytoheatmap <span>python -m profiling.sampling replay --heatmap -o my_heatmap profile.bin</span> </pre></div> </section> <section id=\"profiling-in-production\"> <h3>Profiling in production<a href=\"#profiling-in-production\" title=\"Link to this heading\">¬∂</a></h3> <p>The sampling profiler is designed for production use. It imposes no measurable overhead on the target process because it reads memory externally rather than instrumenting code. The target application continues running at full speed and is unaware it is being profiled.</p> <p>When profiling production systems, keep these guidelines in mind:</p> <p>Start with shorter durations (10-30 seconds) to get quick results, then extend if you need more statistical accuracy. By default, profiling runs until the target process completes, which is usually sufficient to identify major hotspots.</p> <p>If possible, profile during representative load rather than peak traffic. Profiles collected during normal operation are easier to interpret than those collected during unusual spikes.</p> <p>The profiler itself consumes some CPU on the machine where it runs (not on the target process). On the same machine, this is typically negligible. When profiling remote processes, network latency does not affect the target.</p> <p>Results from production may differ from development due to different data sizes, concurrent load, or caching effects. This is expected and is often exactly what you want to capture.</p> </section> <section id=\"platform-requirements\"> <h3>Platform requirements<a href=\"#platform-requirements\" title=\"Link to this heading\">¬∂</a></h3> <p>The profiler reads the target process‚Äôs memory to capture stack traces. This requires elevated permissions on most operating systems.</p> <p><strong>Linux</strong></p> <p>On Linux, the profiler uses <code><span>ptrace</span></code> or <code><span>process_vm_readv</span></code> to read the target process‚Äôs memory. This typically requires one of:</p> <ul> <li><p>Running as root</p></li> <li><p>Having the <code><span>CAP_SYS_PTRACE</span></code> capability</p></li> <li><p>Adjusting the Yama ptrace scope: <code><span>/proc/sys/kernel/yama/ptrace_scope</span></code></p></li> </ul> <p>The default ptrace_scope of 1 restricts ptrace to parent processes only. To allow attaching to any process owned by the same user, set it to 0:</p> <div><pre><span>echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope</span> </pre></div> <p><strong>macOS</strong></p> <p>On macOS, the profiler uses <code><span>task_for_pid()</span></code> to access the target process. This requires one of:</p> <ul> <li><p>Running as root</p></li> <li><p>The profiler binary having the <code><span>com.apple.security.cs.debugger</span></code> entitlement</p></li> <li><p>System Integrity Protection (SIP) being disabled (not recommended)</p></li> </ul> <p><strong>Windows</strong></p> <p>On Windows, the profiler requires administrative privileges or the <code><span>SeDebugPrivilege</span></code> privilege to read another process‚Äôs memory.</p> </section> <section id=\"version-compatibility\"> <h3>Version compatibility<a href=\"#version-compatibility\" title=\"Link to this heading\">¬∂</a></h3> <p>The profiler and target process must run the same Python minor version (for example, both Python 3.15). Attaching from Python 3.14 to a Python 3.15 process is not supported.</p> <p>Additional restrictions apply to pre-release Python versions: if either the profiler or target is running a pre-release (alpha, beta, or release candidate), both must run the exact same version.</p> <p>On free-threaded Python builds, the profiler cannot attach from a free-threaded build to a standard build, or vice versa.</p> </section> </section> <section id=\"sampling-configuration\"> <h2>Sampling configuration<a href=\"#sampling-configuration\" title=\"Link to this heading\">¬∂</a></h2> <p>Before exploring the various output formats and visualization options, it is important to understand how to configure the sampling process itself. The profiler offers several options that control how frequently samples are collected, how long profiling runs, which threads are observed, and what additional context is captured in each sample.</p> <p>The default configuration works well for most use cases:</p> <table> <colgroup> <col> <col> </colgroup> <thead> <tr><th><p>Option</p></th> <th><p>Default</p></th> </tr> </thead> <tbody> <tr><td><p>Default for <code><span>--sampling-rate</span></code> / <code><span>-r</span></code></p></td> <td><p>1 kHz</p></td> </tr> <tr><td><p>Default for <code><span>--duration</span></code> / <code><span>-d</span></code></p></td> <td><p>Run to completion</p></td> </tr> <tr><td><p>Default for <code><span>--all-threads</span></code> / <code><span>-a</span></code></p></td> <td><p>Main thread only</p></td> </tr> <tr><td><p>Default for <code><span>--native</span></code></p></td> <td><p>No <code><span>&lt;native&gt;</span></code> frames (C code time attributed to caller)</p></td> </tr> <tr><td><p>Default for <code><span>--no-gc</span></code></p></td> <td><p><code><span>&lt;GC&gt;</span></code> frames included when garbage collection is active</p></td> </tr> <tr><td><p>Default for <code><span>--mode</span></code></p></td> <td><p>Wall-clock mode (all samples recorded)</p></td> </tr> <tr><td><p>Default for <code><span>--realtime-stats</span></code></p></td> <td><p>Disabled</p></td> </tr> <tr><td><p>Default for <code><span>--subprocesses</span></code></p></td> <td><p>Disabled</p></td> </tr> <tr><td><p>Default for <code><span>--blocking</span></code></p></td> <td><p>Disabled (non-blocking sampling)</p></td> </tr> </tbody> </table> <section id=\"sampling-rate-and-duration\"> <h3>Sampling rate and duration<a href=\"#sampling-rate-and-duration\" title=\"Link to this heading\">¬∂</a></h3> <p>The two most fundamental parameters are the sampling rate and duration. Together, these determine how many samples will be collected during a profiling session.</p> <p>The <a href=\"#cmdoption-profiling.sampling-r\"><code><span>--sampling-rate</span></code></a> option (<a href=\"#cmdoption-profiling.sampling-r\"><code><span>-r</span></code></a>) sets how frequently samples are collected. The default is 1 kHz (10,000 samples per second):</p> <div><pre><span>python -m profiling.sampling run -r 20khz script.py</span> </pre></div> <p>Higher rates capture more samples and provide finer-grained data at the cost of slightly higher profiler CPU usage. Lower rates reduce profiler overhead but may miss short-lived functions. For most applications, the default rate provides a good balance between accuracy and overhead.</p> <p>The <a href=\"#cmdoption-profiling.sampling-d\"><code><span>--duration</span></code></a> option (<a href=\"#cmdoption-profiling.sampling-d\"><code><span>-d</span></code></a>) sets how long to profile in seconds. By default, profiling continues until the target process exits or is interrupted:</p> <div><pre><span>python -m profiling.sampling run -d 60 script.py</span> </pre></div> <p>Specifying a duration is useful when attaching to long-running processes or when you want to limit profiling to a specific time window. When profiling a script, the default behavior of running to completion is usually what you want.</p> </section> <section id=\"thread-selection\"> <h3>Thread selection<a href=\"#thread-selection\" title=\"Link to this heading\">¬∂</a></h3> <p>Python programs often use multiple threads, whether explicitly through the <a href=\"https://docs.python.org/3.15/library/threading.html#module-threading\" title=\"threading: Thread-based parallelism.\"><code><span>threading</span></code></a> module or implicitly through libraries that manage thread pools.</p> <p>By default, the profiler samples only the main thread. The <a href=\"#cmdoption-profiling.sampling-a\"><code><span>--all-threads</span></code></a> option (<a href=\"#cmdoption-profiling.sampling-a\"><code><span>-a</span></code></a>) enables sampling of all threads in the process:</p> <div><pre><span>python -m profiling.sampling run -a script.py</span> </pre></div> <p>Multi-thread profiling reveals how work is distributed across threads and can identify threads that are blocked or starved. Each thread‚Äôs samples are combined in the output, with the ability to filter by thread in some formats. This option is particularly useful when investigating concurrency issues or when work is distributed across a thread pool.</p> </section> <section id=\"blocking-mode\"> <h3>Blocking mode<a href=\"#blocking-mode\" title=\"Link to this heading\">¬∂</a></h3> <p>By default, Tachyon reads the target process‚Äôs memory without stopping it. This non-blocking approach is ideal for most profiling scenarios because it imposes virtually zero overhead on the target application: the profiled program runs at full speed and is unaware it is being observed.</p> <p>However, non-blocking sampling can occasionally produce incomplete or inconsistent stack traces in applications with many generators or coroutines that rapidly switch between yield points, or in programs with very fast-changing call stacks where functions enter and exit between the start and end of a single stack read, resulting in reconstructed stacks that mix frames from different execution states or that never actually existed.</p> <p>For these cases, the <a href=\"#cmdoption-profiling.sampling-blocking\"><code><span>--blocking</span></code></a> option stops the target process during each sample:</p> <div><pre><span>python -m profiling.sampling run --blocking script.py</span> <span>python -m profiling.sampling attach --blocking 12345</span> </pre></div> <p>When blocking mode is enabled, the profiler suspends the target process, reads its stack, then resumes it. This guarantees that each captured stack represents a real, consistent snapshot of what the process was doing at that instant. The trade-off is that the target process runs slower because it is repeatedly paused.</p> <div> <p>Warning</p> <p>Do not use very high sample rates (low <code><span>--interval</span></code> values) with blocking mode. Suspending and resuming a process takes time, and if the sampling interval is too short, the target will spend more time stopped than running. For blocking mode, intervals of 1000 microseconds (1 millisecond) or higher are recommended. The default 100 microsecond interval may cause noticeable slowdown in the target application.</p> </div> <p>Use blocking mode only when you observe inconsistent stacks in your profiles, particularly with generator-heavy or coroutine-heavy code. For most applications, the default non-blocking mode provides accurate results with zero impact on the target process.</p> </section> <section id=\"special-frames\"> <h3>Special frames<a href=\"#special-frames\" title=\"Link to this heading\">¬∂</a></h3> <p>The profiler can inject artificial frames into the captured stacks to provide additional context about what the interpreter is doing at the moment each sample is taken. These synthetic frames help distinguish different types of execution that would otherwise be invisible.</p> <p>The <a href=\"#cmdoption-profiling.sampling-native\"><code><span>--native</span></code></a> option adds <code><span>&lt;native&gt;</span></code> frames to indicate when Python has called into C code (extension modules, built-in functions, or the interpreter itself):</p> <div><pre><span>python -m profiling.sampling run --native script.py</span> </pre></div> <p>These frames help distinguish time spent in Python code versus time spent in native libraries. Without this option, native code execution appears as time in the Python function that made the call. This is useful when optimizing code that makes heavy use of C extensions like NumPy or database drivers.</p> <p>By default, the profiler includes <code><span>&lt;GC&gt;</span></code> frames when garbage collection is active. The <a href=\"#cmdoption-profiling.sampling-no-gc\"><code><span>--no-gc</span></code></a> option suppresses these frames:</p> <div><pre><span>python -m profiling.sampling run --no-gc script.py</span> </pre></div> <p>GC frames help identify programs where garbage collection consumes significant time, which may indicate memory allocation patterns worth optimizing. If you see substantial time in <code><span>&lt;GC&gt;</span></code> frames, consider investigating object allocation rates or using object pooling.</p> </section> <section id=\"opcode-aware-profiling\"> <h3>Opcode-aware profiling<a href=\"#opcode-aware-profiling\" title=\"Link to this heading\">¬∂</a></h3> <p>The <a href=\"#cmdoption-profiling.sampling-opcodes\"><code><span>--opcodes</span></code></a> option enables instruction-level profiling that captures which Python bytecode instructions are executing at each sample:</p> <div><pre><span>python -m profiling.sampling run --opcodes --flamegraph script.py</span> </pre></div> <p>This feature provides visibility into Python‚Äôs bytecode execution, including adaptive specialization optimizations. When a generic instruction like <code><span>LOAD_ATTR</span></code> is specialized at runtime into a more efficient variant like <code><span>LOAD_ATTR_INSTANCE_VALUE</span></code>, the profiler shows both the specialized name and the base instruction.</p> <p>Opcode information appears in several output formats:</p> <ul> <li><p><strong>Flame graphs</strong>: Hovering over a frame displays a tooltip with a bytecode instruction breakdown, showing which opcodes consumed time in that function</p></li> <li><p><strong>Heatmap</strong>: Expandable bytecode panels per source line show instruction breakdown with specialization percentages</p></li> <li><p><strong>Live mode</strong>: An opcode panel shows instruction-level statistics for the selected function, accessible via keyboard navigation</p></li> <li><p><strong>Gecko format</strong>: Opcode transitions are emitted as interval markers in the Firefox Profiler timeline</p></li> </ul> <p>This level of detail is particularly useful for:</p> <ul> <li><p>Understanding the performance impact of Python‚Äôs adaptive specialization</p></li> <li><p>Identifying hot bytecode instructions that might benefit from optimization</p></li> <li><p>Analyzing the effectiveness of different code patterns at the instruction level</p></li> <li><p>Debugging performance issues that occur at the bytecode level</p></li> </ul> <p>The <a href=\"#cmdoption-profiling.sampling-opcodes\"><code><span>--opcodes</span></code></a> option is compatible with <a href=\"#cmdoption-profiling.sampling-live\"><code><span>--live</span></code></a>, <a href=\"#cmdoption-profiling.sampling-flamegraph\"><code><span>--flamegraph</span></code></a>, <a href=\"#cmdoption-profiling.sampling-heatmap\"><code><span>--heatmap</span></code></a>, and <a href=\"#cmdoption-profiling.sampling-gecko\"><code><span>--gecko</span></code></a> formats. It requires additional memory to store opcode information and may slightly reduce sampling performance, but provides unprecedented visibility into Python‚Äôs execution model.</p> </section> <section id=\"real-time-statistics\"> <h3>Real-time statistics<a href=\"#real-time-statistics\" title=\"Link to this heading\">¬∂</a></h3> <p>The <a href=\"#cmdoption-profiling.sampling-realtime-stats\"><code><span>--realtime-stats</span></code></a> option displays sampling rate statistics during profiling:</p> <div><pre><span>python -m profiling.sampling run --realtime-stats script.py</span> </pre></div> <p>This shows the actual achieved sampling rate, which may be lower than requested if the profiler cannot keep up. The statistics help verify that profiling is working correctly and that sufficient samples are being collected. See <a href=\"#sampling-efficiency\"><span>Sampling efficiency</span></a> for details on interpreting these metrics.</p> </section> <section id=\"subprocess-profiling\"> <h3>Subprocess profiling<a href=\"#subprocess-profiling\" title=\"Link to this heading\">¬∂</a></h3> <p>The <a href=\"#cmdoption-profiling.sampling-subprocesses\"><code><span>--subprocesses</span></code></a> option enables automatic profiling of subprocesses spawned by the target:</p> <div><pre><span>python -m profiling.sampling run --subprocesses script.py</span> <span>python -m profiling.sampling attach --subprocesses 12345</span> </pre></div> <p>When enabled, the profiler monitors the target process for child process creation. When a new Python child process is detected, a separate profiler instance is automatically spawned to profile it. This is useful for applications that use <a href=\"https://docs.python.org/3.15/library/multiprocessing.html#module-multiprocessing\" title=\"multiprocessing: Process-based parallelism.\"><code><span>multiprocessing</span></code></a>, <a href=\"https://docs.python.org/3.15/library/subprocess.html#module-subprocess\" title=\"subprocess: Subprocess management.\"><code><span>subprocess</span></code></a>, <a href=\"https://docs.python.org/3.15/library/concurrent.futures.html#module-concurrent.futures\" title=\"concurrent.futures: Execute computations concurrently using threads or processes.\"><code><span>concurrent.futures</span></code></a> with <a href=\"https://docs.python.org/3.15/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor\" title=\"concurrent.futures.ProcessPoolExecutor\"><code><span>ProcessPoolExecutor</span></code></a>, or other process spawning mechanisms.</p> <div id=\"id3\"> <p><span>worker_pool.py</span><a href=\"#id3\" title=\"Link to this code\">¬∂</a></p> <div><pre><span>from</span><span>concurrent.futures</span><span>import</span> <span>ProcessPoolExecutor</span> <span>import</span><span>math</span> <span>def</span><span>compute_factorial</span><span>(</span><span>n</span><span>):</span> <span>total</span> <span>=</span> <span>0</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>50</span><span>):</span> <span>total</span> <span>+=</span> <span>math</span><span>.</span><span>factorial</span><span>(</span><span>n</span><span>)</span> <span>return</span> <span>total</span> <span>if</span> <span>__name__</span> <span>==</span> <span>\"__main__\"</span><span>:</span> <span>numbers</span> <span>=</span> <span>[</span><span>5000</span> <span>+</span> <span>i</span> <span>*</span> <span>100</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>50</span><span>)]</span> <span>with</span> <span>ProcessPoolExecutor</span><span>(</span><span>max_workers</span><span>=</span><span>4</span><span>)</span> <span>as</span> <span>executor</span><span>:</span> <span>results</span> <span>=</span> <span>list</span><span>(</span><span>executor</span><span>.</span><span>map</span><span>(</span><span>compute_factorial</span><span>,</span> <span>numbers</span><span>))</span> <span>print</span><span>(</span><span>f</span><span>\"Computed </span><span>{</span><span>len</span><span>(</span><span>results</span><span>)</span><span>}</span><span> factorials\"</span><span>)</span> </pre></div> </div> <div><pre><span>python -m profiling.sampling run --subprocesses --flamegraph worker_pool.py</span> </pre></div> <p>This produces separate flame graphs for the main process and each worker process: <code><span>flamegraph_&lt;main_pid&gt;.html</span></code>, <code><span>flamegraph_&lt;worker1_pid&gt;.html</span></code>, and so on.</p> <p>Each subprocess receives its own output file. The filename is derived from the specified output path (or the default) with the subprocess‚Äôs process ID appended:</p> <ul> <li><p>If you specify <code><span>-o</span> <span>profile.html</span></code>, subprocesses produce <code><span>profile_12345.html</span></code>, <code><span>profile_12346.html</span></code>, and so on</p></li> <li><p>With default output, subprocesses produce files like <code><span>flamegraph_12345.html</span></code> or directories like <code><span>heatmap_12345</span></code></p></li> <li><p>For pstats format (which defaults to stdout), subprocesses produce files like <code><span>profile_12345.pstats</span></code></p></li> </ul> <p>The subprocess profilers inherit most sampling options from the parent (sampling rate, duration, thread selection, native frames, GC frames, async-aware mode, and output format). All Python descendant processes are profiled recursively, including grandchildren and further descendants.</p> <p>Subprocess detection works by periodically scanning for new descendants of the target process and checking whether each new process is a Python process by probing the process memory for Python runtime structures. Non-Python subprocesses (such as shell commands or external tools) are ignored.</p> <p>There is a limit of 100 concurrent subprocess profilers to prevent resource exhaustion in programs that spawn many processes. If this limit is reached, additional subprocesses are not profiled and a warning is printed.</p> <p>The <a href=\"#cmdoption-profiling.sampling-subprocesses\"><code><span>--subprocesses</span></code></a> option is incompatible with <a href=\"#cmdoption-profiling.sampling-live\"><code><span>--live</span></code></a> mode because live mode uses an interactive terminal interface that cannot accommodate multiple concurrent profiler displays.</p> </section> <section id=\"sampling-efficiency\"> <h3>Sampling efficiency<a href=\"#sampling-efficiency\" title=\"Link to this heading\">¬∂</a></h3> <p>Sampling efficiency metrics help assess the quality of the collected data. These metrics appear in the profiler‚Äôs terminal output and in the flame graph sidebar.</p> <p><strong>Sampling efficiency</strong> is the percentage of sample attempts that succeeded. Each sample attempt reads the target process‚Äôs call stack from memory. An attempt can fail if the process is in an inconsistent state at the moment of reading, such as during a context switch or while the interpreter is updating its internal structures. A low efficiency may indicate that the profiler could not keep up with the requested sampling rate, often due to system load or an overly aggressive interval setting.</p> <p><strong>Missed samples</strong> is the percentage of expected samples that were not collected. Based on the configured interval and duration, the profiler expects to collect a certain number of samples. Some samples may be missed if the profiler falls behind schedule, for example when the system is under heavy load. A small percentage of missed samples is normal and does not significantly affect the statistical accuracy of the profile.</p> <p>Both metrics are informational. Even with some failed attempts or missed samples, the profile remains statistically valid as long as enough samples were collected. The profiler reports the actual number of samples captured, which you can use to judge whether the data is sufficient for your analysis.</p> </section> </section> <section id=\"profiling-modes\"> <h2>Profiling modes<a href=\"#profiling-modes\" title=\"Link to this heading\">¬∂</a></h2> <p>The sampling profiler supports four modes that control which samples are recorded. The mode determines what the profile measures: total elapsed time, CPU execution time, time spent holding the global interpreter lock, or exception handling.</p> <section id=\"wall-clock-mode\"> <h3>Wall-clock mode<a href=\"#wall-clock-mode\" title=\"Link to this heading\">¬∂</a></h3> <p>Wall-clock mode (<a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a><code><span>=wall</span></code>) captures all samples regardless of what the thread is doing. This is the default mode and provides a complete picture of where time passes during program execution:</p> <div><pre><span>python -m profiling.sampling run --mode=wall script.py</span> </pre></div> <p>In wall-clock mode, samples are recorded whether the thread is actively executing Python code, waiting for I/O, blocked on a lock, or sleeping. This makes wall-clock profiling ideal for understanding the overall time distribution in your program, including time spent waiting.</p> <p>If your program spends significant time in I/O operations, network calls, or sleep, wall-clock mode will show these waits as time attributed to the calling function. This is often exactly what you want when optimizing end-to-end latency.</p> </section> <section id=\"cpu-mode\"> <h3>CPU mode<a href=\"#cpu-mode\" title=\"Link to this heading\">¬∂</a></h3> <p>CPU mode (<a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a><code><span>=cpu</span></code>) records samples only when the thread is actually executing on a CPU core:</p> <div><pre><span>python -m profiling.sampling run --mode=cpu script.py</span> </pre></div> <p>Samples taken while the thread is sleeping, blocked on I/O, or waiting for a lock are discarded. The resulting profile shows where CPU cycles are consumed, filtering out idle time.</p> <p>CPU mode is useful when you want to focus on computational hotspots without being distracted by I/O waits. If your program alternates between computation and network calls, CPU mode reveals which computational sections are most expensive.</p> </section> <section id=\"comparing-wall-clock-and-cpu-profiles\"> <h3>Comparing wall-clock and CPU profiles<a href=\"#comparing-wall-clock-and-cpu-profiles\" title=\"Link to this heading\">¬∂</a></h3> <p>Running both wall-clock and CPU mode profiles can reveal whether a function‚Äôs time is spent computing or waiting.</p> <p>If a function appears prominently in both profiles, it is a true computational hotspot‚Äîactively using the CPU. Optimization should focus on algorithmic improvements or more efficient code.</p> <p>If a function is high in wall-clock mode but low or absent in CPU mode, it is I/O-bound or waiting. The function spends most of its time waiting for network, disk, locks, or sleep. CPU optimization won‚Äôt help here; consider async I/O, connection pooling, or reducing wait time instead.</p> <div><pre><span>import</span><span>time</span> <span>def</span><span>do_sleep</span><span>():</span> <span>time</span><span>.</span><span>sleep</span><span>(</span><span>2</span><span>)</span> <span>def</span><span>do_compute</span><span>():</span> <span>sum</span><span>(</span><span>i</span><span>**</span><span>2</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>1000000</span><span>))</span> <span>if</span> <span>__name__</span> <span>==</span> <span>\"__main__\"</span><span>:</span> <span>do_sleep</span><span>()</span> <span>do_compute</span><span>()</span> </pre></div> <div><pre><span>python -m profiling.sampling run --mode=wall script.py # do_sleep ~98%, do_compute ~1%</span> <span>python -m profiling.sampling run --mode=cpu script.py # do_sleep absent, do_compute dominates</span> </pre></div> </section> <section id=\"gil-mode\"> <h3>GIL mode<a href=\"#gil-mode\" title=\"Link to this heading\">¬∂</a></h3> <p>GIL mode (<a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a><code><span>=gil</span></code>) records samples only when the thread holds Python‚Äôs global interpreter lock:</p> <div><pre><span>python -m profiling.sampling run --mode=gil script.py</span> </pre></div> <p>The GIL is held only while executing Python bytecode. When Python calls into C extensions, performs I/O operations, or executes native code, the GIL is typically released. This means GIL mode effectively measures time spent running Python code specifically, filtering out time in native libraries.</p> <p>In multi-threaded programs, GIL mode reveals which code is preventing other threads from running Python bytecode. Since only one thread can hold the GIL at a time, functions that appear frequently in GIL mode profiles are monopolizing the interpreter.</p> <p>GIL mode helps answer questions like ‚Äúwhich functions are monopolizing the GIL?‚Äù and ‚Äúwhy are my other threads starving?‚Äù It can also be useful in single-threaded programs to distinguish Python execution time from time spent in C extensions or I/O.</p> <div><pre><span>import</span><span>hashlib</span> <span>def</span><span>hash_work</span><span>():</span> <span># C extension - releases GIL during computation</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>200</span><span>):</span> <span>hashlib</span><span>.</span><span>sha256</span><span>(</span><span>b</span><span>\"data\"</span> <span>*</span> <span>250000</span><span>)</span><span>.</span><span>hexdigest</span><span>()</span> <span>def</span><span>python_work</span><span>():</span> <span># Pure Python - holds GIL during computation</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>3</span><span>):</span> <span>sum</span><span>(</span><span>i</span><span>**</span><span>2</span> <span>for</span> <span>i</span> <span>in</span> <span>range</span><span>(</span><span>1000000</span><span>))</span> <span>if</span> <span>__name__</span> <span>==</span> <span>\"__main__\"</span><span>:</span> <span>hash_work</span><span>()</span> <span>python_work</span><span>()</span> </pre></div> <div><pre><span>python -m profiling.sampling run --mode=cpu script.py # hash_work ~42%, python_work ~38%</span> <span>python -m profiling.sampling run --mode=gil script.py # hash_work ~5%, python_work ~60%</span> </pre></div> </section> <section id=\"exception-mode\"> <h3>Exception mode<a href=\"#exception-mode\" title=\"Link to this heading\">¬∂</a></h3> <p>Exception mode (<code><span>--mode=exception</span></code>) records samples only when a thread has an active exception:</p> <div><pre><span>python -m profiling.sampling run --mode=exception script.py</span> </pre></div> <p>Samples are recorded in two situations: when an exception is being propagated up the call stack (after <code><span>raise</span></code> but before being caught), or when code is executing inside an <code><span>except</span></code> block where exception information is still present in the thread state.</p> <p>The following example illustrates which code regions are captured:</p> <div><pre><span>def</span><span>example</span><span>():</span> <span>try</span><span>:</span> <span>raise</span> <span>ValueError</span><span>(</span><span>\"error\"</span><span>)</span> <span># Captured: exception being raised</span> <span>except</span> <span>ValueError</span><span>:</span> <span>process_error</span><span>()</span> <span># Captured: inside except block</span> <span>finally</span><span>:</span> <span>cleanup</span><span>()</span> <span># NOT captured: exception already handled</span> <span>def</span><span>example_propagating</span><span>():</span> <span>try</span><span>:</span> <span>try</span><span>:</span> <span>raise</span> <span>ValueError</span><span>(</span><span>\"error\"</span><span>)</span> <span>finally</span><span>:</span> <span>cleanup</span><span>()</span> <span># Captured: exception propagating through</span> <span>except</span> <span>ValueError</span><span>:</span> <span>pass</span> <span>def</span><span>example_no_exception</span><span>():</span> <span>try</span><span>:</span> <span>do_work</span><span>()</span> <span>finally</span><span>:</span> <span>cleanup</span><span>()</span> <span># NOT captured: no exception involved</span> </pre></div> <p>Note that <code><span>finally</span></code> blocks are only captured when an exception is actively propagating through them. Once an <code><span>except</span></code> block finishes executing, Python clears the exception information before running any subsequent <code><span>finally</span></code> block. Similarly, <code><span>finally</span></code> blocks that run during normal execution (when no exception was raised) are not captured because no exception state is present.</p> <p>This mode is useful for understanding where your program spends time handling errors. Exception handling can be a significant source of overhead in code that uses exceptions for flow control (such as <code><span>StopIteration</span></code> in iterators) or in applications that process many error conditions (such as network servers handling connection failures).</p> <p>Exception mode helps answer questions like ‚Äúhow much time is spent handling exceptions?‚Äù and ‚Äúwhich exception handlers are the most expensive?‚Äù It can reveal hidden performance costs in code that catches and processes many exceptions, even when those exceptions are handled gracefully. For example, if a parsing library uses exceptions internally to signal format errors, this mode will capture time spent in those handlers even if the calling code never sees the exceptions.</p> </section> </section> <section id=\"output-formats\"> <h2>Output formats<a href=\"#output-formats\" title=\"Link to this heading\">¬∂</a></h2> <p>The profiler produces output in several formats, each suited to different analysis workflows. The format is selected with a command-line flag, and output goes to stdout, a file, or a directory depending on the format.</p> <section id=\"pstats-format\"> <h3>pstats format<a href=\"#pstats-format\" title=\"Link to this heading\">¬∂</a></h3> <p>The pstats format (<a href=\"#cmdoption-profiling.sampling-pstats\"><code><span>--pstats</span></code></a>) produces a text table similar to what deterministic profilers generate. This is the default output format:</p> <div><pre><span>python -m profiling.sampling run script.py</span> <span>python -m profiling.sampling run --pstats script.py</span> </pre></div> <figure id=\"id4\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-pstats.png\"><img alt=\"Tachyon pstats terminal output\" src=\"https://docs.python.org/3.15/_images/tachyon-pstats.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The pstats format displays profiling results in a color-coded table showing function hotspots, sample counts, and timing estimates.</span><a href=\"#id4\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <p>Output appears on stdout by default:</p> <div><pre><span>Profile Stats (Mode: wall):</span> <span> nsamples sample% tottime (ms) cumul% cumtime (ms) filename:lineno(function)</span> <span> 234/892 11.7% 234.00 44.6% 892.00 server.py:145(handle_request)</span> <span> 156/156 7.8% 156.00 7.8% 156.00 &lt;built-in&gt;:0(socket.recv)</span> <span> 98/421 4.9% 98.00 21.1% 421.00 parser.py:67(parse_message)</span> </pre></div> <p>The columns show sampling counts and estimated times:</p> <ul> <li><p><strong>nsamples</strong>: Displayed as <code><span>direct/cumulative</span></code> (for example, <code><span>10/50</span></code>). Direct samples are when the function was at the top of the stack, actively executing. Cumulative samples are when the function appeared anywhere on the stack, including when it was waiting for functions it called. If a function shows <code><span>10/50</span></code>, it was directly executing in 10 samples and was on the call stack in 50 samples total.</p></li> <li><p><strong>sample%</strong> and <strong>cumul%</strong>: Percentages of total samples for direct and cumulative counts respectively.</p></li> <li><p><strong>tottime</strong> and <strong>cumtime</strong>: Estimated wall-clock time based on sample counts and the profiling duration. Time units are selected automatically based on the magnitude: seconds for large values, milliseconds for moderate values, or microseconds for small values.</p></li> </ul> <p>The output includes a legend explaining each column and a summary of interesting functions that highlights:</p> <ul> <li><p><strong>Hot spots</strong>: Functions with high direct/cumulative sample ratio (ratio close to 1.0). These functions spend most of their time executing their own code rather than waiting for callees. High ratios indicate where CPU time is actually consumed.</p></li> <li><p><strong>Indirect calls</strong>: Functions with large differences between cumulative and direct samples. These are orchestration functions that delegate work to other functions. They appear frequently on the stack but rarely at the top.</p></li> <li><p><strong>Call magnification</strong>: Functions where cumulative samples far exceed direct samples (high cumulative/direct multiplier). These are frequently-nested functions that appear deep in many call chains.</p></li> </ul> <p>Use <a href=\"#cmdoption-profiling.sampling-no-summary\"><code><span>--no-summary</span></code></a> to suppress both the legend and summary sections.</p> <p>To save pstats output to a file instead of stdout:</p> <div><pre><span>python -m profiling.sampling run -o profile.txt script.py</span> </pre></div> <p>The pstats format supports several options for controlling the display. The <a href=\"#cmdoption-profiling.sampling-sort\"><code><span>--sort</span></code></a> option determines the column used for ordering results:</p> <div><pre><span>python -m profiling.sampling run --sort=tottime script.py</span> <span>python -m profiling.sampling run --sort=cumtime script.py</span> <span>python -m profiling.sampling run --sort=nsamples script.py</span> </pre></div> <p>The <a href=\"#cmdoption-profiling.sampling-l\"><code><span>--limit</span></code></a> option restricts output to the top N entries:</p> <div><pre><span>python -m profiling.sampling run --limit=30 script.py</span> </pre></div> <p>The <a href=\"#cmdoption-profiling.sampling-no-summary\"><code><span>--no-summary</span></code></a> option suppresses the header summary that precedes the statistics table.</p> </section> <section id=\"collapsed-stacks-format\"> <h3>Collapsed stacks format<a href=\"#collapsed-stacks-format\" title=\"Link to this heading\">¬∂</a></h3> <p>Collapsed stacks format (<a href=\"#cmdoption-profiling.sampling-collapsed\"><code><span>--collapsed</span></code></a>) produces one line per unique call stack, with a count of how many times that stack was sampled:</p> <div><pre><span>python -m profiling.sampling run --collapsed script.py</span> </pre></div> <p>The output looks like:</p> <div><pre>main;process_data;parse_json;decode_utf8 42 main;process_data;parse_json 156 main;handle_request;send_response 89 </pre></div> <p>Each line contains semicolon-separated function names representing the call stack from bottom to top, followed by a space and the sample count. This format is designed for compatibility with external flame graph tools, particularly Brendan Gregg‚Äôs <code><span>flamegraph.pl</span></code> script.</p> <p>To generate a flame graph from collapsed stacks:</p> <div><pre><span>python -m profiling.sampling run --collapsed script.py &gt; stacks.txt</span> <span>flamegraph.pl stacks.txt &gt; profile.svg</span> </pre></div> <p>The resulting SVG can be viewed in any web browser and provides an interactive visualization where you can click to zoom into specific call paths.</p> </section> <section id=\"flame-graph-format\"> <h3>Flame graph format<a href=\"#flame-graph-format\" title=\"Link to this heading\">¬∂</a></h3> <p>Flame graph format (<a href=\"#cmdoption-profiling.sampling-flamegraph\"><code><span>--flamegraph</span></code></a>) produces a self-contained HTML file with an interactive flame graph visualization:</p> <div><pre><span>python -m profiling.sampling run --flamegraph script.py</span> <span>python -m profiling.sampling run --flamegraph -o profile.html script.py</span> </pre></div> <figure id=\"id5\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-flamegraph.png\"><img alt=\"Tachyon interactive flame graph\" src=\"https://docs.python.org/3.15/_images/tachyon-flamegraph.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The flame graph visualization shows call stacks as nested rectangles, with width proportional to time spent. The sidebar displays runtime statistics, GIL metrics, and hotspot functions.</span><a href=\"#id5\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <p><a href=\"https://docs.python.org/3.15/_static/tachyon-example-flamegraph.html\">Try the interactive example</a>!</p> <p>If no output file is specified, the profiler generates a filename based on the process ID (for example, <code><span>flamegraph.12345.html</span></code>).</p> <p>The generated HTML file requires no external dependencies and can be opened directly in a web browser. The visualization displays call stacks as nested rectangles, with width proportional to time spent. Hovering over a rectangle shows details about that function including source code context, and clicking zooms into that portion of the call tree.</p> <p>The flame graph interface includes:</p> <ul> <li><p>A sidebar showing profile summary, thread statistics, sampling efficiency metrics (see <a href=\"#sampling-efficiency\"><span>Sampling efficiency</span></a>), and top hotspot functions</p></li> <li><p>Search functionality supporting both function name matching and <code><span>file.py:42</span></code> line patterns</p></li> <li><p>Per-thread filtering via dropdown</p></li> <li><p>Dark/light theme toggle (preference saved across sessions)</p></li> <li><p>SVG export for saving the current view</p></li> </ul> <p>The thread statistics section shows runtime behavior metrics:</p> <ul> <li><p><strong>GIL Held</strong>: percentage of samples where a thread held the global interpreter lock (actively running Python code)</p></li> <li><p><strong>GIL Released</strong>: percentage of samples where no thread held the GIL</p></li> <li><p><strong>Waiting GIL</strong>: percentage of samples where a thread was waiting to acquire the GIL</p></li> <li><p><strong>GC</strong>: percentage of samples during garbage collection</p></li> </ul> <p>These statistics help identify GIL contention and understand how time is distributed between Python execution, native code, and waiting.</p> <p>Flame graphs are particularly effective for identifying deep call stacks and understanding the hierarchical structure of time consumption. Wide rectangles at the top indicate functions that consume significant time either directly or through their callees.</p> </section> <section id=\"gecko-format\"> <h3>Gecko format<a href=\"#gecko-format\" title=\"Link to this heading\">¬∂</a></h3> <p>Gecko format (<a href=\"#cmdoption-profiling.sampling-gecko\"><code><span>--gecko</span></code></a>) produces JSON output compatible with the Firefox Profiler:</p> <div><pre><span>python -m profiling.sampling run --gecko script.py</span> <span>python -m profiling.sampling run --gecko -o profile.json script.py</span> </pre></div> <p>The <a href=\"https://profiler.firefox.com/\">Firefox Profiler</a> is a sophisticated web-based tool originally built for profiling Firefox itself. It provides features beyond basic flame graphs, including a timeline view, call tree exploration, and marker visualization. See the <a href=\"https://profiler.firefox.com/docs/#/\">Firefox Profiler documentation</a> for detailed usage instructions.</p> <p>To use the output, open the Firefox Profiler in your browser and load the JSON file. The profiler runs entirely client-side, so your profiling data never leaves your machine.</p> <p>Gecko format automatically collects additional metadata about GIL state and CPU activity, enabling analysis features specific to Python‚Äôs threading model. The profiler emits interval markers that appear as colored bands in the Firefox Profiler timeline:</p> <ul> <li><p><strong>GIL markers</strong>: show when threads hold or release the global interpreter lock</p></li> <li><p><strong>CPU markers</strong>: show when threads are executing on CPU versus idle</p></li> <li><p><strong>Code type markers</strong>: distinguish Python code from native (C extension) code</p></li> <li><p><strong>GC markers</strong>: indicate garbage collection activity</p></li> </ul> <p>For this reason, the <a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a> option is not available with Gecko format; all relevant data is captured automatically.</p> <figure id=\"id6\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-gecko-calltree.png\"><img alt=\"Firefox Profiler Call Tree view\" src=\"https://docs.python.org/3.15/_images/tachyon-gecko-calltree.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The Call Tree view shows the complete call hierarchy with sample counts and percentages. The sidebar displays detailed statistics for the selected function including running time and sample distribution.</span><a href=\"#id6\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <figure id=\"id7\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-gecko-flamegraph.png\"><img alt=\"Firefox Profiler Flame Graph view\" src=\"https://docs.python.org/3.15/_images/tachyon-gecko-flamegraph.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The Flame Graph visualization shows call stacks as nested rectangles. Functions names are visible in the call hierarchy.</span><a href=\"#id7\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <figure id=\"id8\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-gecko-opcodes.png\"><img alt=\"Firefox Profiler Marker Chart with opcodes\" src=\"https://docs.python.org/3.15/_images/tachyon-gecko-opcodes.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The Marker Chart displays interval markers including CPU state, GIL status, and opcodes. With <code><span>--opcodes</span></code> enabled, bytecode instructions like <code><span>BINARY_OP_ADD_FLOAT</span></code>, <code><span>CALL_PY_EXACT_ARGS</span></code>, and <code><span>CALL_LIST_APPEND</span></code> appear as markers showing execution over time.</span><a href=\"#id8\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> </section> <section id=\"heatmap-format\"> <h3>Heatmap format<a href=\"#heatmap-format\" title=\"Link to this heading\">¬∂</a></h3> <p>Heatmap format (<a href=\"#cmdoption-profiling.sampling-heatmap\"><code><span>--heatmap</span></code></a>) generates an interactive HTML visualization showing sample counts at the source line level:</p> <div><pre><span>python -m profiling.sampling run --heatmap script.py</span> <span>python -m profiling.sampling run --heatmap -o my_heatmap script.py</span> </pre></div> <figure id=\"id9\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-heatmap.png\"><img alt=\"Tachyon heatmap visualization\" src=\"https://docs.python.org/3.15/_images/tachyon-heatmap.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>The heatmap overlays sample counts directly on your source code. Lines are color-coded from cool (few samples) to hot (many samples). Navigation buttons (‚ñ≤‚ñº) let you jump between callers and callees.</span><a href=\"#id9\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <p>Unlike other formats that produce a single file, heatmap output creates a directory containing HTML files for each profiled source file. If no output path is specified, the directory is named <code><span>heatmap_PID</span></code>.</p> <p>The heatmap visualization displays your source code with a color gradient indicating how many samples were collected at each line. Hot lines (many samples) appear in warm colors, while cold lines (few or no samples) appear in cool colors. This view helps pinpoint exactly which lines of code are responsible for time consumption.</p> <p>The heatmap interface provides several interactive features:</p> <ul> <li><p><strong>Coloring modes</strong>: toggle between ‚ÄúSelf Time‚Äù (direct execution) and ‚ÄúTotal Time‚Äù (cumulative, including time in called functions)</p></li> <li><p><strong>Cold code filtering</strong>: show all lines or only lines with samples</p></li> <li><p><strong>Call graph navigation</strong>: each line shows navigation buttons (‚ñ≤ for callers, ‚ñº for callees) that let you trace execution paths through your code. When multiple functions called or were called from a line, a menu appears showing all options with their sample counts.</p></li> <li><p><strong>Scroll minimap</strong>: a vertical overview showing the heat distribution across the entire file</p></li> <li><p><strong>Hierarchical index</strong>: files organized by type (stdlib, site-packages, project) with aggregate sample counts per folder</p></li> <li><p><strong>Dark/light theme</strong>: toggle with preference saved across sessions</p></li> <li><p><strong>Line linking</strong>: click line numbers to create shareable URLs</p></li> </ul> <p>When opcode-level profiling is enabled with <a href=\"#cmdoption-profiling.sampling-opcodes\"><code><span>--opcodes</span></code></a>, each hot line can be expanded to show which bytecode instructions consumed time:</p> <figure id=\"id10\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-heatmap-with-opcodes.png\"><img alt=\"Heatmap with expanded bytecode panel\" src=\"https://docs.python.org/3.15/_images/tachyon-heatmap-with-opcodes.png\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>Expanding a hot line reveals the bytecode instructions executed, including specialized variants. The panel shows sample counts per instruction and the overall specialization percentage for the line.</span><a href=\"#id10\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <p><a href=\"https://docs.python.org/3.15/_static/tachyon-example-heatmap.html\">Try the interactive example</a>!</p> <p>Heatmaps are especially useful when you know which file contains a performance issue but need to identify the specific lines. Many developers prefer this format because it maps directly to their source code, making it easy to read and navigate. For smaller scripts and focused analysis, heatmaps provide an intuitive view that shows exactly where time is spent without requiring interpretation of hierarchical visualizations.</p> </section> <section id=\"binary-format\"> <h3>Binary format<a href=\"#binary-format\" title=\"Link to this heading\">¬∂</a></h3> <p>Binary format (<a href=\"#cmdoption-profiling.sampling-binary\"><code><span>--binary</span></code></a>) produces a compact binary file for efficient storage of profiling data:</p> <div><pre><span>python -m profiling.sampling run --binary -o profile.bin script.py</span> <span>python -m profiling.sampling attach --binary -o profile.bin 12345</span> </pre></div> <p>The <a href=\"#cmdoption-profiling.sampling-compression\"><code><span>--compression</span></code></a> option controls data compression:</p> <ul> <li><p><code><span>auto</span></code> (default): Use zstd compression if available, otherwise no compression</p></li> <li><p><code><span>zstd</span></code>: Force zstd compression (requires <a href=\"https://docs.python.org/3.15/library/compression.zstd.html#module-compression.zstd\" title=\"compression.zstd: Low-level interface to compression and decompression routines in the zstd library.\"><code><span>compression.zstd</span></code></a> support)</p></li> <li><p><code><span>none</span></code>: Disable compression</p></li> </ul> <div><pre><span>python -m profiling.sampling run --binary --compression=zstd -o profile.bin script.py</span> </pre></div> <p>To analyze binary profiles, use the <a href=\"#replay-command\"><span>The replay command</span></a> to convert them to other formats like flame graphs or pstats output.</p> </section> </section> <section id=\"record-and-replay-workflow\"> <h2>Record and replay workflow<a href=\"#record-and-replay-workflow\" title=\"Link to this heading\">¬∂</a></h2> <p>The binary format combined with the replay command enables a record-and-replay workflow that separates data capture from analysis. Rather than generating visualizations during profiling, you capture raw data to a compact binary file and convert it to different formats later.</p> <p>This approach has three main benefits:</p> <ul> <li><p>Sampling runs faster because the work of building data structures for visualization is deferred until replay.</p></li> <li><p>A single binary capture can be converted to multiple output formats without re-profiling: pstats for a quick overview, flame graph for visual exploration, heatmap for line-level detail.</p></li> <li><p>Binary files are compact and easy to share with colleagues who can convert them to their preferred format.</p></li> </ul> <p>A typical workflow:</p> <div><pre><span># </span>Captureprofile<span>in</span>productionorduringtests <span>python -m profiling.sampling attach --binary -o profile.bin 12345</span> <span># </span>Later,analyzewithdifferentformats <span>python -m profiling.sampling replay profile.bin</span> <span>python -m profiling.sampling replay --flamegraph -o profile.html profile.bin</span> <span>python -m profiling.sampling replay --heatmap -o heatmap profile.bin</span> </pre></div> </section> <section id=\"live-mode\"> <h2>Live mode<a href=\"#live-mode\" title=\"Link to this heading\">¬∂</a></h2> <p>Live mode (<a href=\"#cmdoption-profiling.sampling-live\"><code><span>--live</span></code></a>) provides a terminal-based real-time view of profiling data, similar to the <code><span>top</span></code> command for system processes:</p> <div><pre><span>python -m profiling.sampling run --live script.py</span> <span>python -m profiling.sampling attach --live 12345</span> </pre></div> <figure id=\"id11\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-live-mode-2.gif\"><img alt=\"Tachyon live mode showing all threads\" src=\"https://docs.python.org/3.15/_images/tachyon-live-mode-2.gif\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>Live mode displays real-time profiling statistics, showing combined data from multiple threads in a multi-threaded application.</span><a href=\"#id11\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <p>The display updates continuously as new samples arrive, showing the current hottest functions. This mode requires the <a href=\"https://docs.python.org/3.15/library/curses.html#module-curses\" title=\"curses: An interface to the curses library, providing portable terminal handling. (Unix)\"><code><span>curses</span></code></a> module, which is available on Unix-like systems but not on Windows. The terminal must be at least 60 columns wide and 12 lines tall; larger terminals display more columns.</p> <p>The header displays the top 3 hottest functions, sampling efficiency metrics, and thread status statistics (GIL held percentage, CPU usage, GC time). The main table shows function statistics with the currently sorted column indicated by an arrow (‚ñº).</p> <p>When <a href=\"#cmdoption-profiling.sampling-opcodes\"><code><span>--opcodes</span></code></a> is enabled, an additional opcode panel appears below the main table, showing instruction-level statistics for the currently selected function. This panel displays which bytecode instructions are executing most frequently, including specialized variants and their base opcodes.</p> <figure id=\"id12\"> <a href=\"https://docs.python.org/3.15/_images/tachyon-live-mode-1.gif\"><img alt=\"Tachyon live mode with opcode panel\" src=\"https://docs.python.org/3.15/_images/tachyon-live-mode-1.gif\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </a> <figcaption> <p><span>Live mode with <code><span>--opcodes</span></code> enabled shows an opcode panel with a bytecode instruction breakdown for the selected function.</span><a href=\"#id12\" title=\"Link to this image\">¬∂</a></p> </figcaption> </figure> <section id=\"keyboard-commands\"> <h3>Keyboard commands<a href=\"#keyboard-commands\" title=\"Link to this heading\">¬∂</a></h3> <p>Within live mode, keyboard commands control the display:</p> <dl> <dt><kbd>q</kbd></dt><dd><p>Quit the profiler and return to the shell.</p> </dd> <dt><kbd>s</kbd> / <kbd>S</kbd></dt><dd><p>Cycle through sort orders forward/backward (sample count, percentage, total time, cumulative percentage, cumulative time).</p> </dd> <dt><kbd>p</kbd></dt><dd><p>Pause or resume display updates. Sampling continues in the background while the display is paused, so you can freeze the view to examine results without stopping data collection.</p> </dd> <dt><kbd>r</kbd></dt><dd><p>Reset all statistics and start fresh. This is disabled after profiling finishes to prevent accidental data loss.</p> </dd> <dt><kbd>/</kbd></dt><dd><p>Enter filter mode to search for functions by name. The filter uses case-insensitive substring matching against the filename and function name. Type a pattern and press Enter to apply, or Escape to cancel. Glob patterns and regular expressions are not supported.</p> </dd> <dt><kbd>c</kbd></dt><dd><p>Clear the current filter and show all functions again.</p> </dd> <dt><kbd>t</kbd></dt><dd><p>Toggle between viewing all threads combined or per-thread statistics. In per-thread mode, a thread counter (for example, <code><span>1/4</span></code>) appears showing your position among the available threads.</p> </dd> <dt><kbd>‚Üê</kbd> <kbd>‚Üí</kbd> or <kbd>‚Üë</kbd> <kbd>‚Üì</kbd></dt><dd><p>In per-thread view, navigate between threads. Navigation wraps around from the last thread to the first and vice versa.</p> </dd> <dt><kbd>+</kbd> / <kbd>-</kbd></dt><dd><p>Increase or decrease the display refresh rate. The range is 0.05 seconds (20 Hz, very responsive) to 1.0 second (1 Hz, lower overhead). Faster refresh rates use more CPU. The default is 0.1 seconds (10 Hz).</p> </dd> <dt><kbd>x</kbd></dt><dd><p>Toggle trend indicators that show whether functions are becoming hotter or cooler over time. When enabled, increasing metrics appear in green and decreasing metrics appear in red, comparing each update to the previous one.</p> </dd> <dt><kbd>h</kbd> or <kbd>?</kbd></dt><dd><p>Show the help screen with all available commands.</p> </dd> <dt><kbd>j</kbd> / <kbd>k</kbd> (or <kbd>Up</kbd> / <kbd>Down</kbd>)</dt><dd><p>Navigate through opcode entries in the opcode panel (when <code><span>--opcodes</span></code> is enabled). These keys scroll through the instruction-level statistics for the currently selected function.</p> </dd> </dl> <p>When profiling finishes (duration expires or target process exits), the display shows a ‚ÄúPROFILING COMPLETE‚Äù banner and freezes the final results. You can still navigate, sort, and filter the results before pressing <kbd>q</kbd> to exit.</p> <p>Live mode is incompatible with output format options (<a href=\"#cmdoption-profiling.sampling-collapsed\"><code><span>--collapsed</span></code></a>, <a href=\"#cmdoption-profiling.sampling-flamegraph\"><code><span>--flamegraph</span></code></a>, and so on) because it uses an interactive terminal interface rather than producing file output.</p> </section> </section> <section id=\"async-aware-profiling\"> <h2>Async-aware profiling<a href=\"#async-aware-profiling\" title=\"Link to this heading\">¬∂</a></h2> <p>For programs using <a href=\"https://docs.python.org/3.15/library/asyncio.html#module-asyncio\" title=\"asyncio: Asynchronous I/O.\"><code><span>asyncio</span></code></a>, the profiler offers async-aware mode (<a href=\"#cmdoption-profiling.sampling-async-aware\"><code><span>--async-aware</span></code></a>) that reconstructs call stacks based on the task structure rather than the raw Python frames:</p> <div><pre><span>python -m profiling.sampling run --async-aware async_script.py</span> </pre></div> <p>Standard profiling of async code can be confusing because the physical call stack often shows event loop internals rather than the logical flow of your coroutines. Async-aware mode addresses this by tracking which task is running and presenting stacks that reflect the <code><span>await</span></code> chain.</p> <div><pre><span>import</span><span>asyncio</span> <span>async</span> <span>def</span><span>fetch</span><span>(</span><span>url</span><span>):</span> <span>await</span> <span>asyncio</span><span>.</span><span>sleep</span><span>(</span><span>0.1</span><span>)</span> <span>return</span> <span>url</span> <span>async</span> <span>def</span><span>main</span><span>():</span> <span>for</span> <span>_</span> <span>in</span> <span>range</span><span>(</span><span>50</span><span>):</span> <span>await</span> <span>asyncio</span><span>.</span><span>gather</span><span>(</span><span>fetch</span><span>(</span><span>\"a\"</span><span>),</span> <span>fetch</span><span>(</span><span>\"b\"</span><span>),</span> <span>fetch</span><span>(</span><span>\"c\"</span><span>))</span> <span>if</span> <span>__name__</span> <span>==</span> <span>\"__main__\"</span><span>:</span> <span>asyncio</span><span>.</span><span>run</span><span>(</span><span>main</span><span>())</span> </pre></div> <div><pre><span>python -m profiling.sampling run --async-aware --flamegraph -o out.html script.py</span> </pre></div> <div> <p>Note</p> <p>Async-aware profiling requires the target process to have the <a href=\"https://docs.python.org/3.15/library/asyncio.html#module-asyncio\" title=\"asyncio: Asynchronous I/O.\"><code><span>asyncio</span></code></a> module loaded. If you profile a script before it imports asyncio, async-aware mode will not be able to capture task information.</p> </div> <section id=\"async-modes\"> <h3>Async modes<a href=\"#async-modes\" title=\"Link to this heading\">¬∂</a></h3> <p>The <a href=\"#cmdoption-profiling.sampling-async-mode\"><code><span>--async-mode</span></code></a> option controls which tasks appear in the profile:</p> <div><pre><span>python -m profiling.sampling run --async-aware --async-mode=running async_script.py</span> <span>python -m profiling.sampling run --async-aware --async-mode=all async_script.py</span> </pre></div> <p>With <a href=\"#cmdoption-profiling.sampling-async-mode\"><code><span>--async-mode</span></code></a><code><span>=running</span></code> (the default), only the task currently executing on the CPU is profiled. This shows where your program is actively spending time and is the typical choice for performance analysis.</p> <p>With <a href=\"#cmdoption-profiling.sampling-async-mode\"><code><span>--async-mode</span></code></a><code><span>=all</span></code>, tasks that are suspended (awaiting I/O, locks, or other tasks) are also included. This mode is useful for understanding what your program is waiting on, but produces larger profiles since every suspended task appears in each sample.</p> </section> <section id=\"task-markers-and-stack-reconstruction\"> <h3>Task markers and stack reconstruction<a href=\"#task-markers-and-stack-reconstruction\" title=\"Link to this heading\">¬∂</a></h3> <p>In async-aware profiles, you will see <code><span>&lt;task&gt;</span></code> frames that mark boundaries between asyncio tasks. These are synthetic frames inserted by the profiler to show the task structure. The task name appears as the function name in these frames.</p> <p>When a task awaits another task, the profiler reconstructs the logical call chain by following the <code><span>await</span></code> relationships. Only ‚Äúleaf‚Äù tasks (tasks that no other task is currently awaiting) generate their own stack entries. Tasks being awaited by other tasks appear as part of their awaiter‚Äôs stack instead.</p> <p>If a task has multiple awaiters (a diamond pattern in the task graph), the profiler deterministically selects one parent and annotates the task marker with the number of parents, for example <code><span>MyTask</span> <span>(2</span> <span>parents)</span></code>. This indicates that alternate execution paths exist but are not shown in this particular stack.</p> </section> <section id=\"option-restrictions\"> <h3>Option restrictions<a href=\"#option-restrictions\" title=\"Link to this heading\">¬∂</a></h3> <p>Async-aware mode uses a different stack reconstruction mechanism and is incompatible with: <a href=\"#cmdoption-profiling.sampling-native\"><code><span>--native</span></code></a>, <a href=\"#cmdoption-profiling.sampling-no-gc\"><code><span>--no-gc</span></code></a>, <a href=\"#cmdoption-profiling.sampling-a\"><code><span>--all-threads</span></code></a>, and <a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a><code><span>=cpu</span></code> or <a href=\"#cmdoption-profiling.sampling-mode\"><code><span>--mode</span></code></a><code><span>=gil</span></code>.</p> </section> </section> <section id=\"command-line-interface\"> <h2>Command-line interface<a href=\"#command-line-interface\" title=\"Link to this heading\">¬∂</a></h2> <p>The complete command-line interface for reference.</p> <section id=\"global-options\"> <h3>Global options<a href=\"#global-options\" title=\"Link to this heading\">¬∂</a></h3> <dl> <dt id=\"cmdoption-profiling.sampling-arg-run\"> <span><span>run</span></span><a href=\"#cmdoption-profiling.sampling-arg-run\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Run and profile a Python script or module.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-arg-attach\"> <span><span>attach</span></span><a href=\"#cmdoption-profiling.sampling-arg-attach\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Attach to and profile a running process by PID.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-arg-replay\"> <span><span>replay</span></span><a href=\"#cmdoption-profiling.sampling-arg-replay\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Convert a binary profile file to another output format.</p> </dd></dl> </section> <section id=\"sampling-options\"> <h3>Sampling options<a href=\"#sampling-options\" title=\"Link to this heading\">¬∂</a></h3> <dl> <dt id=\"cmdoption-profiling.sampling-r\"> <span><span>-r</span></span><span> <span>&lt;rate&gt;</span></span><span><span>,</span> </span><span><span>--sampling-rate</span></span><span> <span>&lt;rate&gt;</span></span><a href=\"#cmdoption-profiling.sampling-r\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Sampling rate (for example, <code><span>10000</span></code>, <code><span>10khz</span></code>, <code><span>10k</span></code>). Default: <code><span>1khz</span></code>.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-d\"> <span><span>-d</span></span><span> <span>&lt;seconds&gt;</span></span><span><span>,</span> </span><span><span>--duration</span></span><span> <span>&lt;seconds&gt;</span></span><a href=\"#cmdoption-profiling.sampling-d\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Profiling duration in seconds. Default: run to completion.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-a\"> <span><span>-a</span></span><span><span>,</span> </span><span><span>--all-threads</span></span><a href=\"#cmdoption-profiling.sampling-a\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Sample all threads, not just the main thread.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-realtime-stats\"> <span><span>--realtime-stats</span></span><a href=\"#cmdoption-profiling.sampling-realtime-stats\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Display sampling statistics during profiling.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-native\"> <span><span>--native</span></span><a href=\"#cmdoption-profiling.sampling-native\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Include <code><span>&lt;native&gt;</span></code> frames for non-Python code.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-no-gc\"> <span><span>--no-gc</span></span><a href=\"#cmdoption-profiling.sampling-no-gc\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Exclude <code><span>&lt;GC&gt;</span></code> frames for garbage collection.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-async-aware\"> <span><span>--async-aware</span></span><a href=\"#cmdoption-profiling.sampling-async-aware\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Enable async-aware profiling for asyncio programs.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-opcodes\"> <span><span>--opcodes</span></span><a href=\"#cmdoption-profiling.sampling-opcodes\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Gather bytecode opcode information for instruction-level profiling. Shows which bytecode instructions are executing, including specializations. Compatible with <code><span>--live</span></code>, <code><span>--flamegraph</span></code>, <code><span>--heatmap</span></code>, and <code><span>--gecko</span></code> formats only.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-subprocesses\"> <span><span>--subprocesses</span></span><a href=\"#cmdoption-profiling.sampling-subprocesses\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Also profile subprocesses. Each subprocess gets its own profiler instance and output file. Incompatible with <code><span>--live</span></code>.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-blocking\"> <span><span>--blocking</span></span><a href=\"#cmdoption-profiling.sampling-blocking\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Pause the target process during each sample. This ensures consistent stack traces at the cost of slowing down the target. Use with longer intervals (1000 ¬µs or higher) to minimize impact. See <a href=\"#blocking-mode\"><span>Blocking mode</span></a> for details.</p> </dd></dl> </section> <section id=\"mode-options\"> <h3>Mode options<a href=\"#mode-options\" title=\"Link to this heading\">¬∂</a></h3> <dl> <dt id=\"cmdoption-profiling.sampling-mode\"> <span><span>--mode</span></span><span> <span>&lt;mode&gt;</span></span><a href=\"#cmdoption-profiling.sampling-mode\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Sampling mode: <code><span>wall</span></code> (default), <code><span>cpu</span></code>, <code><span>gil</span></code>, or <code><span>exception</span></code>. The <code><span>cpu</span></code>, <code><span>gil</span></code>, and <code><span>exception</span></code> modes are incompatible with <code><span>--async-aware</span></code>.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-async-mode\"> <span><span>--async-mode</span></span><span> <span>&lt;mode&gt;</span></span><a href=\"#cmdoption-profiling.sampling-async-mode\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Async profiling mode: <code><span>running</span></code> (default) or <code><span>all</span></code>. Requires <code><span>--async-aware</span></code>.</p> </dd></dl> </section> <section id=\"output-options\"> <h3>Output options<a href=\"#output-options\" title=\"Link to this heading\">¬∂</a></h3> <dl> <dt id=\"cmdoption-profiling.sampling-pstats\"> <span><span>--pstats</span></span><a href=\"#cmdoption-profiling.sampling-pstats\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate text statistics output. This is the default.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-collapsed\"> <span><span>--collapsed</span></span><a href=\"#cmdoption-profiling.sampling-collapsed\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate collapsed stack format for external flame graph tools.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-flamegraph\"> <span><span>--flamegraph</span></span><a href=\"#cmdoption-profiling.sampling-flamegraph\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate self-contained HTML flame graph.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-gecko\"> <span><span>--gecko</span></span><a href=\"#cmdoption-profiling.sampling-gecko\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate Gecko JSON format for Firefox Profiler.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-heatmap\"> <span><span>--heatmap</span></span><a href=\"#cmdoption-profiling.sampling-heatmap\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate HTML heatmap with line-level sample counts.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-binary\"> <span><span>--binary</span></span><a href=\"#cmdoption-profiling.sampling-binary\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Generate high-performance binary format for later conversion with the <code><span>replay</span></code> command.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-compression\"> <span><span>--compression</span></span><span> <span>&lt;type&gt;</span></span><a href=\"#cmdoption-profiling.sampling-compression\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Compression for binary format: <code><span>auto</span></code> (use zstd if available, default), <code><span>zstd</span></code>, or <code><span>none</span></code>.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-o\"> <span><span>-o</span></span><span> <span>&lt;path&gt;</span></span><span><span>,</span> </span><span><span>--output</span></span><span> <span>&lt;path&gt;</span></span><a href=\"#cmdoption-profiling.sampling-o\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Output file or directory path. Default behavior varies by format: <a href=\"#cmdoption-profiling.sampling-pstats\"><code><span>--pstats</span></code></a> writes to stdout, while other formats generate a file named <code><span>&lt;format&gt;_&lt;PID&gt;.&lt;ext&gt;</span></code> (for example, <code><span>flamegraph_12345.html</span></code>). <a href=\"#cmdoption-profiling.sampling-heatmap\"><code><span>--heatmap</span></code></a> creates a directory named <code><span>heatmap_&lt;PID&gt;</span></code>.</p> </dd></dl> </section> <section id=\"pstats-display-options\"> <h3>pstats display options<a href=\"#pstats-display-options\" title=\"Link to this heading\">¬∂</a></h3> <p>These options apply only to pstats format output.</p> <dl> <dt id=\"cmdoption-profiling.sampling-sort\"> <span><span>--sort</span></span><span> <span>&lt;key&gt;</span></span><a href=\"#cmdoption-profiling.sampling-sort\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Sort order: <code><span>nsamples</span></code>, <code><span>tottime</span></code>, <code><span>cumtime</span></code>, <code><span>sample-pct</span></code>, <code><span>cumul-pct</span></code>, <code><span>nsamples-cumul</span></code>, or <code><span>name</span></code>. Default: <code><span>nsamples</span></code>.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-l\"> <span><span>-l</span></span><span> <span>&lt;count&gt;</span></span><span><span>,</span> </span><span><span>--limit</span></span><span> <span>&lt;count&gt;</span></span><a href=\"#cmdoption-profiling.sampling-l\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Maximum number of entries to display. Default: 15.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-no-summary\"> <span><span>--no-summary</span></span><a href=\"#cmdoption-profiling.sampling-no-summary\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Omit the Legend and Summary of Interesting Functions sections from output.</p> </dd></dl> </section> <section id=\"run-command-options\"> <h3>Run command options<a href=\"#run-command-options\" title=\"Link to this heading\">¬∂</a></h3> <dl> <dt id=\"cmdoption-profiling.sampling-m\"> <span><span>-m</span></span><span><span>,</span> </span><span><span>--module</span></span><a href=\"#cmdoption-profiling.sampling-m\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Treat the target as a module name rather than a script path.</p> </dd></dl> <dl> <dt id=\"cmdoption-profiling.sampling-live\"> <span><span>--live</span></span><a href=\"#cmdoption-profiling.sampling-live\" title=\"Link to this definition\">¬∂</a></dt> <dd><p>Start interactive terminal interface instead of batch profiling.</p> </dd></dl> <div> <p>See also</p> <dl> <dt><a href=\"https://docs.python.org/3.15/library/profiling.html#module-profiling\" title=\"profiling: Python profiling tools for performance analysis.\"><code><span>profiling</span></code></a></dt><dd><p>Overview of Python profiling tools and guidance on choosing a profiler.</p> </dd> <dt><a href=\"https://docs.python.org/3.15/library/profiling.tracing.html#module-profiling.tracing\" title=\"profiling.tracing: Deterministic tracing profiler for Python programs.\"><code><span>profiling.tracing</span></code></a></dt><dd><p>Deterministic tracing profiler for exact call counts and timing.</p> </dd> <dt><a href=\"https://docs.python.org/3.15/library/pstats.html#module-pstats\" title=\"pstats: Statistics object for analyzing profiler output.\"><code><span>pstats</span></code></a></dt><dd><p>Statistics analysis for profile data.</p> </dd> <dt><a href=\"https://profiler.firefox.com/\">Firefox Profiler</a></dt><dd><p>Web-based profiler that accepts Gecko format output. See the <a href=\"https://profiler.firefox.com/docs/#/\">documentation</a> for usage details.</p> </dd> <dt><a href=\"https://github.com/brendangregg/FlameGraph\">FlameGraph</a></dt><dd><p>Tools for generating flame graphs from collapsed stack format.</p> </dd> </dl> </div> </section> </section> </section></div>"},{"id":"https://fahrplan.events.ccc.de/congress/2025/fahrplan/","title":"Fahrplan ‚Äì 39C3","link":"https://fahrplan.events.ccc.de/congress/2025/fahrplan/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46386211","content":"<a href=\"https://news.ycombinator.com/item?id=46386211\">Comments</a>","date":1766688014000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div> <p> 10:00 </p> <p> 10:30 </p> <p> 11:00 </p> <p> 11:30 </p> <p> 12:00 </p> <p> 12:30 </p> <p> 13:00 </p> <p> 13:30 </p> <p> 14:00 </p> <p> 14:30 </p> <p> 15:00 </p> <p> 15:30 </p> <p> 16:00 </p> <p> 16:30 </p> <p> 17:00 </p> <p> 17:30 </p> <p> 18:00 </p> <p> 18:30 </p> <p> 19:00 </p> <p> 19:30 </p> <p> 20:00 </p> <p> 20:30 </p> <p> 21:00 </p> <p> 21:30 </p> <p> 22:00 </p> <p> 22:30 </p> <p> 23:00 </p> <p> 23:30 </p> <p> 00:00 </p> <p> 00:30 </p> <p> 01:00 </p> <p> 01:30 </p> <p> 02:00 </p> <p> 02:30 </p> <p> 03:00 </p> <p> 03:30 </p> <p> 04:00 </p> <p> 04:30 </p> <p> 05:00 </p> <p> 05:30 </p> <p> 06:00 </p> <p> 06:30 </p> <p> 07:00 </p> <p> 07:30 </p> <p> 08:00 </p> <p> 08:30 </p> <p> 09:00 </p> <p> 09:30 </p> <p> 10:00 </p> <p> 10:30 </p> <p> 11:00 </p> <p> 11:30 </p> <p> 12:00 </p> <p> 12:30 </p> <p> 13:00 </p> <p> 13:30 </p> <p> 14:00 </p> <p> 14:30 </p> <p> 15:00 </p> <p> 15:30 </p> <p> 16:00 </p> <p> 16:30 </p> <p> 17:00 </p> <p> 17:30 </p> <p> 18:00 </p> <p> 18:30 </p> <p> 19:00 </p> <p> 19:30 </p> <p> 20:00 </p> <p> 20:30 </p> <p> 21:00 </p> <p> 21:30 </p> <p> 22:00 </p> <p> 22:30 </p> <p> 23:00 </p> <p> 23:30 </p> <p> 00:00 </p> <p> 00:30 </p> <p> 01:00 </p> <p> 01:30 </p> <p> 02:00 </p> <p> 02:30 </p> <p> 03:00 </p> <p> 03:30 </p> <p> 04:00 </p> <p> 04:30 </p> <p> 05:00 </p> <p> 05:30 </p> <p> 06:00 </p> <p> 06:30 </p> <p> 07:00 </p> <p> 07:30 </p> <p> 08:00 </p> <p> 08:30 </p> <p> 09:00 </p> <p> 09:30 </p> <p> 10:00 </p> <p> 10:30 </p> <p> 11:00 </p> <p> 11:30 </p> <p> 12:00 </p> <p> 12:30 </p> <p> 13:00 </p> <p> 13:30 </p> <p> 14:00 </p> <p> 14:30 </p> <p> 15:00 </p> <p> 15:30 </p> <p> 16:00 </p> <p> 16:30 </p> <p> 17:00 </p> <p> 17:30 </p> <p> 18:00 </p> <p> 18:30 </p> <p> 19:00 </p> <p> 19:30 </p> <p> 20:00 </p> <p> 20:30 </p> <p> 21:00 </p> <p> 21:30 </p> <p> 22:00 </p> <p> 22:30 </p> <p> 23:00 </p> <p> 23:30 </p> <p> 00:00 </p> <p> 00:30 </p> <p> 01:00 </p> <p> 01:30 </p> <p> 02:00 </p> <p> 02:30 </p> <p> 03:00 </p> <p> 03:30 </p> <p> 04:00 </p> <p> 04:30 </p> <p> 05:00 </p> <p> 05:30 </p> <p> 06:00 </p> <p> 06:30 </p> <p> 07:00 </p> <p> 07:30 </p> <p> 08:00 </p> <p> 08:30 </p> <p> 09:00 </p> <p> 09:30 </p> <p> 10:00 </p> <p> 10:30 </p> <p> 11:00 </p> <p> 11:30 </p> <p> 12:00 </p> <p> 12:30 </p> <p> 13:00 </p> <p> 13:30 </p> <p> 14:00 </p> <p> 14:30 </p> <p> 15:00 </p> <p> 15:30 </p> <p> 16:00 </p> <p> 16:30 </p> <p> 17:00 </p> <p> 17:30 </p> <div> <h2> Sat - Day 1 - December 27 </h2> </div> <div> <h2> Sun - Day 2 - December 28 </h2> </div> <div> <h2> Mon - Day 3 - December 29 </h2> </div> <div> <h2> Tue - Day 4 - December 30 </h2> </div> </div>"},{"id":"https://tiled.art/en/home/?id=SilverAndGold","title":"Tiled Art","link":"https://tiled.art/en/home/?id=SilverAndGold","hnCommentsUrl":"https://news.ycombinator.com/item?id=46324881","content":"<a href=\"https://news.ycombinator.com/item?id=46324881\">Comments</a>","date":1766145630000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div id=\"introduction\"> <p> Welcome to the world of tessellation art, where talented artists craft a figure that interlocks with itself perfectly to fill the page. Dutch artist <a href=\"https://tiled.art/en/artists/?id=escher\">M. C. Escher</a> mastered the form first, inspiring artists worldwide to create the captivating art shown on this site. </p> <p> <span>At right, enjoy a gallery of artworks</span> <span>Above, enjoy a gallery of artworks</span> as each emerges from an underlying grid. <span>Click an artwork‚Äòs title for a full view,</span> <span>Tap an artwork‚Äòs title for a full view,</span> or explore all the <a href=\"https://tiled.art/en/art/\">Art</a> and <a href=\"https://tiled.art/en/artists/\">Artists</a>. </p> <p> And try <a href=\"https://tiled.art/en/create/\">creating your own tessellation</a>, with the tiles staying interlocked automatically. See if you can make a recognizable figure in this wild world where you have to draw two sides of the tile at once! Then add interior detail using drawing tools or <a href=\"https://inkscape.org/\" target=\"_blank\">Inkscape</a>. </p> <p> Seeing the variety in tessellation art it‚Äôs natural to wonder how it all works. On each artwork page you can run an animation to see the structure behind the art (<a href=\"https://tiled.art/en/art/Sprint/\">example</a>). And on the <a href=\"https://tiled.art/en/symmetryIntro/\">Symmetries</a> pages you can follow a step-by-step <a href=\"https://tiled.art/en/symmetryTutorial/\">tutorial</a>, explore live <a href=\"https://tiled.art/en/symmetries/\">diagrams</a> with sample artworks, learn how tessellations are <a href=\"https://tiled.art/en/symmetryClassification/\">classified</a>, and more. </p> <p> Feel free to explore‚Äîsmartphone, tablet, or computer‚Äîand <a href=\"https://tiled.art/en/about/#community\">connect with the community.</a> Or check out the video <a href=\"https://vimeo.com/819335286\" target=\"_blank\">Tiled.art in 90 seconds</a> for a quick overview. </p> <p><i><br> Note that all artworks here are copyrighted, and shown with permission of the artist. To use an artwork for another purpose, <a href=\"https://tiled.art/en/artists/\">contact the artist</a> via their website. </i></p> </div></div>"},{"id":"https://www.newyorker.com/news/press-room/the-entire-new-yorker-archive-is-now-fully-digitized","title":"The entire New Yorker archive is now digitized","link":"https://www.newyorker.com/news/press-room/the-entire-new-yorker-archive-is-now-fully-digitized","hnCommentsUrl":"https://news.ycombinator.com/item?id=46336577","content":"<a href=\"https://news.ycombinator.com/item?id=46336577\">Comments</a>","date":1766242179000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false},{"id":"https://calnewport.com/on-paperbacks-and-tiktok/","title":"Paperbacks and TikTok","link":"https://calnewport.com/on-paperbacks-and-tiktok/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46358211","content":"<a href=\"https://news.ycombinator.com/item?id=46358211\">Comments</a>","date":1766433331000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false},{"id":"https://clickhouse.com/blog/postgres-cdc-year-in-review-2025","title":"Lessons from a year of Postgres CDC in production","link":"https://clickhouse.com/blog/postgres-cdc-year-in-review-2025","hnCommentsUrl":"https://news.ycombinator.com/item?id=46332784","content":"<a href=\"https://news.ycombinator.com/item?id=46332784\">Comments</a>","date":1766192405000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false},{"id":"https://github.com/NVIDIA/cuda-tile","title":"CUDA Tile Open Sourced","link":"https://github.com/NVIDIA/cuda-tile","hnCommentsUrl":"https://news.ycombinator.com/item?id=46330732","content":"<a href=\"https://news.ycombinator.com/item?id=46330732\">Comments</a>","date":1766177394000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<h2 tabindex=\"-1\" dir=\"auto\">CUDA Tile IR</h2> <p dir=\"auto\">CUDA Tile IR is an MLIR-based intermediate representation and compiler infrastructure for CUDA kernel optimization, focusing on tile-based computation patterns and optimizations targeting NVIDIA tensor core units. The project provides a comprehensive ecosystem for expressing and optimizing tiled computations for NVIDIA GPUs, simplifying the development of high-performance CUDA kernels through abstractions for common tiling patterns, memory hierarchy management, and GPU-specific optimizations.</p> <p dir=\"auto\">This open-source release is aligned with the <strong>CUDA Toolkit 13.1</strong> release. For more information about CUDA Tile, visit <a href=\"https://developer.nvidia.com/cuda/tile\" rel=\"nofollow\">https://developer.nvidia.com/cuda/tile</a>.</p> <h2 tabindex=\"-1\" dir=\"auto\">Core Components</h2> <p dir=\"auto\">CUDA Tile is composed of:</p> <ul dir=\"auto\"> <li><strong>CUDA Tile Dialect</strong>: A domain-specific MLIR dialect that provides first-class operations and types for tile-based computations</li> <li><strong>Python Bindings</strong>: Complete Python API for programmatic IR construction, manipulation, and transformation</li> <li><strong>Bytecode:</strong>: Efficient binary representation with support for serialization and de-serialization between the CUDA Tile dialect and binary format.</li> <li><strong>Conformance Test Suite</strong>: Comprehensive tests ensuring compliance with the CUDA Tile specification and validation of dialect semantics</li> </ul> <h2 tabindex=\"-1\" dir=\"auto\">CUDA Tile Specification</h2> <p dir=\"auto\">CUDA Tile development is driven by the CUDA Tile IR specification, which defines the formal semantics, operations, and type system for tile-based computations on NVIDIA GPUs. For detailed information about the CUDA Tile IR specification, including dialect operations, type system, and transformation passes, please refer to the <a href=\"https://docs.nvidia.com/cuda/tile-ir/13.1/index.html\" rel=\"nofollow\">CUDA Tile Specification</a>.</p> <h2 tabindex=\"-1\" dir=\"auto\">Building CUDA Tile</h2> <h3 tabindex=\"-1\" dir=\"auto\">Prerequisites</h3> <ul dir=\"auto\"> <li>CMake 3.20.0 or higher</li> <li>C++17 compatible compiler</li> <li>Python 3.6+ (for Python bindings)</li> <li>MLIR/LLVM sources or pre-built libraries at a compatible commit (see <a href=\"https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29\">cmake/IncludeLLVM.cmake</a> for the exact version)</li> <li>Ninja build system (optional)</li> </ul> <h3 tabindex=\"-1\" dir=\"auto\">Quick Start</h3> <p dir=\"auto\">For a quick start, use the following commands from the top of the repository to configure and build a release version of CUDA Tile with Python bindings enabled. MLIR/LLVM sources will be automatically downloaded from <a href=\"https://github.com/llvm/llvm-project\">https://github.com/llvm/llvm-project</a>:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# Configure cmake -G Ninja -S . -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_ASSERTIONS=OFF \\ -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON # Build cmake --build build # Run tests cmake --build build --target check-cuda-tile\"><pre><span><span>#</span> Configure</span> cmake -G Ninja -S <span>.</span> -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DLLVM_ENABLE_ASSERTIONS=OFF \\ -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON <span><span>#</span> Build</span> cmake --build build <span><span>#</span> Run tests</span> cmake --build build --target check-cuda-tile</pre></div> <h3 tabindex=\"-1\" dir=\"auto\">Build Configuration Options</h3> <h4 tabindex=\"-1\" dir=\"auto\">MLIR/LLVM Build Configuration</h4> <p dir=\"auto\">CUDA Tile requires MLIR/LLVM at a specific compatible commit. The exact commit hash is specified in <a href=\"https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29\">cmake/IncludeLLVM.cmake</a>. CUDA Tile can be built with MLIR/LLVM in three different ways:</p> <ol dir=\"auto\"> <li> <p dir=\"auto\"><strong>Automatic Download from GitHub</strong> (Default): CMake automatically downloads MLIR/LLVM sources from the official GitHub repository and builds them at the compatible commit. This is the slowest option but requires no manual LLVM setup.</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake -G Ninja -S . -B build -DCMAKE_BUILD_TYPE=Release\"><pre>cmake -G Ninja -S <span>.</span> -B build -DCMAKE_BUILD_TYPE=Release</pre></div> </li> <li> <p dir=\"auto\"><strong>Use Local LLVM Sources</strong>: CMake builds MLIR/LLVM from existing sources on your system. The commit hash of the source must be compatible with commit specified in <a href=\"https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29\">cmake/IncludeLLVM.cmake</a>.</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake -G Ninja -S . -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_USE_LLVM_SOURCE_DIR=/path/to/llvm/sources\"><pre>cmake -G Ninja -S <span>.</span> -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_USE_LLVM_SOURCE_DIR=/path/to/llvm/sources</pre></div> </li> <li> <p dir=\"auto\"><strong>Use Pre-built LLVM Libraries</strong>: CMake links against pre-compiled LLVM libraries. The commit hash of the source must be compatible with commit specified in <a href=\"https://github.com/NVIDIA/cuda-tile/blob/main/cmake/IncludeLLVM.cmake#L29\">cmake/IncludeLLVM.cmake</a>.</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake -G Ninja -S . -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_USE_LLVM_INSTALL_DIR=/path/to/llvm/install\"><pre>cmake -G Ninja -S <span>.</span> -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_USE_LLVM_INSTALL_DIR=/path/to/llvm/install</pre></div> </li> </ol> <h4 tabindex=\"-1\" dir=\"auto\">Python Bindings</h4> <p dir=\"auto\">CUDA Tile provides Python bindings for programmatic IR manipulation (disabled by default). To enable them, add the <code>-DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON</code> flag to your cmake configuration:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake -G Ninja -S . -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON\"><pre>cmake -G Ninja -S <span>.</span> -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_ENABLE_BINDINGS_PYTHON=ON</pre></div> <p dir=\"auto\">When building MLIR/LLVM from sources, MLIR Python bindings will be automatically enabled. However, when using pre-built LLVM libraries, you must ensure they were built with <code>-DMLIR_ENABLE_BINDINGS_PYTHON=ON</code>.</p> <h4 tabindex=\"-1\" dir=\"auto\">Ccache</h4> <p dir=\"auto\">To build with <code>ccache</code> enabled, add <code>-DCUDA_TILE_ENABLE_CCACHE=ON</code> to your cmake configuration:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake -G Ninja -S . -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_ENABLE_CCACHE=ON\"><pre>cmake -G Ninja -S <span>.</span> -B build \\ -DCMAKE_BUILD_TYPE=Release \\ -DCUDA_TILE_ENABLE_CCACHE=ON</pre></div> <p dir=\"auto\">When building LLVM from sources, this setting is automatically propagated to the LLVM build.</p> <h2 tabindex=\"-1\" dir=\"auto\">Testing</h2> <p dir=\"auto\">CUDA Tile uses LLVM's lit testing infrastructure for comprehensive testing. Testing is enabled by default (<code>-DCUDA_TILE_ENABLE_TESTING=ON</code>). To run the test suite:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"cmake --build build --target check-cuda-tile\"><pre>cmake --build build --target check-cuda-tile</pre></div> <h2 tabindex=\"-1\" dir=\"auto\">Integrating CUDA Tile Into Your Project</h2> <p dir=\"auto\">CUDA Tile can be integrated into your project in two ways, depending on your build system and requirements.</p> <h3 tabindex=\"-1\" dir=\"auto\">Option 1: Using Pre-built CUDA Tile Libraries</h3> <p dir=\"auto\">To use pre-built CUDA Tile libraries in your project, include the necessary headers and link against the required libraries based on your use case. For example:</p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"include_directories(${CUDA_TILE_INSTALL_DIR}/include) # CUDA Tile dialect target_link_libraries(your_target PRIVATE CudaTileDialect # CUDA Tile dialect operations and types ) # Bytecode support. target_link_libraries(your_target PRIVATE CudaTileBytecodeReader # Read bytecode format CudaTileBytecodeWriter # Write bytecode format )\"><pre><span>include_directories</span>(<span>${CUDA_TILE_INSTALL_DIR}</span>/<span>include</span>) <span># CUDA Tile dialect</span> <span>target_link_libraries</span>(your_target <span>PRIVATE</span> CudaTileDialect <span># CUDA Tile dialect operations and types</span> ) <span># Bytecode support.</span> <span>target_link_libraries</span>(your_target <span>PRIVATE</span> CudaTileBytecodeReader <span># Read bytecode format</span> CudaTileBytecodeWriter <span># Write bytecode format</span> )</pre></div> <h3 tabindex=\"-1\" dir=\"auto\">Option 2: Integrating CUDA Tile Sources</h3> <p dir=\"auto\">To build CUDA Tile from source as part of your project:</p> <ol dir=\"auto\"> <li>Integrate CUDA Tile sources into your project with CMake's FetchContent, Git submodules, or any other integration method. Example using FetchContent:</li> </ol> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"include(FetchContent) # Define CUDA Tile directories set(CUDA_TILE_SOURCE_DIR ${CMAKE_BINARY_DIR}/_deps/cuda_tile-src) set(CUDA_TILE_BINARY_DIR ${CMAKE_BINARY_DIR}/_deps/cuda_tile-build) FetchContent_Declare( cuda_tile GIT_REPOSITORY https://github.com/NVIDIA/cuda-tile.git GIT_TAG main SOURCE_DIR ${CUDA_TILE_SOURCE_DIR} BINARY_DIR ${CUDA_TILE_BINARY_DIR} )\"><pre><span>include</span>(FetchContent) <span># Define CUDA Tile directories</span> <span>set</span>(CUDA_TILE_SOURCE_DIR <span>${CMAKE_BINARY_DIR}</span>/_deps/cuda_tile-src) <span>set</span>(<span>CUDA_TILE_BINARY_DIR</span> <span>${CMAKE_BINARY_DIR}</span>/_deps/cuda_tile-<span>build</span>) FetchContent_Declare( cuda_tile GIT_REPOSITORY https://github.com/NVIDIA/cuda-tile.git GIT_TAG main SOURCE_DIR <span>${CUDA_TILE_SOURCE_DIR}</span> BINARY_DIR <span>${CUDA_TILE_BINARY_DIR}</span> )</pre></div> <ol start=\"2\" dir=\"auto\"> <li>Configure CUDA Tile build options (before calling <code>FetchContent_MakeAvailable</code>, if using FetchContent):</li> </ol> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"set(CUDA_TILE_USE_LLVM_INSTALL_DIR ${YOUR_LLVM_INSTALL_DIR} CACHE PATH &quot;&quot;) set(CUDA_TILE_ENABLE_BINDINGS_PYTHON ON CACHE BOOL &quot;&quot;) set(CUDA_TILE_ENABLE_TESTING OFF CACHE BOOL &quot;&quot;) FetchContent_MakeAvailable(cuda_tile)\"><pre><span>set</span>(CUDA_TILE_USE_LLVM_INSTALL_DIR <span>${YOUR_LLVM_INSTALL_DIR}</span> <span>CACHE</span> <span>PATH</span> <span>\"\"</span>) <span>set</span>(CUDA_TILE_ENABLE_BINDINGS_PYTHON <span>ON</span> <span>CACHE</span> <span>BOOL</span> <span>\"\"</span>) <span>set</span>(CUDA_TILE_ENABLE_TESTING <span>OFF</span> <span>CACHE</span> <span>BOOL</span> <span>\"\"</span>) FetchContent_MakeAvailable(cuda_tile)</pre></div> <ol start=\"3\" dir=\"auto\"> <li>Include headers from source and build directories, then link libraries as in Option 1:</li> </ol> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"include_directories(${CUDA_TILE_SOURCE_DIR}/include) include_directories(${CUDA_TILE_BINARY_DIR}/include)\"><pre><span>include_directories</span>(<span>${CUDA_TILE_SOURCE_DIR}</span>/<span>include</span>) <span>include_directories</span>(<span>${CUDA_TILE_BINARY_DIR}</span>/<span>include</span>)</pre></div> <h2 tabindex=\"-1\" dir=\"auto\">Example: Writing and Running a CUDA Tile IR Program</h2> <p dir=\"auto\">The following shows how to compile and run a simple Tile IR kernel that prints data from a pointer.</p> <p dir=\"auto\">Tile IR bytecode can be produced from an MLIR program using the <code>cuda-tile-translate</code> tool. This can be loaded directly using the CUDA driver API, which will JIT compile the program automatically. To compile ahead of time, you can use the <code>tileiras</code> tool from the CUDA Toolkit to compile the bytecode into a cubin for a particular GPU target. This example shows the latter to illustrate the extra step, but the driver launch API is the same in either case (just substitute the path to the bytecode file).</p> <h3 tabindex=\"-1\" dir=\"auto\">Prequisites</h3> <p dir=\"auto\">This example assumes you have built the CUDA Tile IR dialect tools according to the instructions above.</p> <p dir=\"auto\">You will need a supported CUDA device, CUDA Toolkit 13.1+, and a compatible driver.</p> <h3 tabindex=\"-1\" dir=\"auto\">CUDA Tile IR Program</h3> <p dir=\"auto\">Save the following into a file <code>example.mlir</code>.</p> <div data-snippet-clipboard-copy-content=\"cuda_tile.module @example_module { entry @example_kernel(%data_pr : tile<ptr<f32>>) { print &quot;Running example module\\n&quot; %offsets = iota : tile<128xi32> %data_ptr_reshaped = reshape %data_pr : tile<ptr<f32>> -> tile<1xptr<f32>> %data_ptr_broadcasted = broadcast %data_ptr_reshaped : tile<1xptr<f32>> -> tile<128xptr<f32>> %data_ptr_tensor = offset %data_ptr_broadcasted, %offsets : tile<128xptr<f32>>, tile<128xi32> -> tile<128xptr<f32>> %data, %token = load_ptr_tko weak %data_ptr_tensor : tile<128xptr<f32>> -> tile<128xf32>, token print &quot;Data: %f\\n&quot;, %data : tile<128xf32> return } }\"><pre><code>cuda_tile.module @example_module { entry @example_kernel(%data_pr : tile&lt;ptr&lt;f32&gt;&gt;) { print \"Running example module\\n\" %offsets = iota : tile&lt;128xi32&gt; %data_ptr_reshaped = reshape %data_pr : tile&lt;ptr&lt;f32&gt;&gt; -&gt; tile&lt;1xptr&lt;f32&gt;&gt; %data_ptr_broadcasted = broadcast %data_ptr_reshaped : tile&lt;1xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt; %data_ptr_tensor = offset %data_ptr_broadcasted, %offsets : tile&lt;128xptr&lt;f32&gt;&gt;, tile&lt;128xi32&gt; -&gt; tile&lt;128xptr&lt;f32&gt;&gt; %data, %token = load_ptr_tko weak %data_ptr_tensor : tile&lt;128xptr&lt;f32&gt;&gt; -&gt; tile&lt;128xf32&gt;, token print \"Data: %f\\n\", %data : tile&lt;128xf32&gt; return } } </code></pre></div> <h3 tabindex=\"-1\" dir=\"auto\">C++ Host Program</h3> <p dir=\"auto\">Save the following into a file <code>example_host.cpp</code>.</p> <div data-snippet-clipboard-copy-content=\"#include <cuda.h> #include <cuda_runtime_api.h> #include <stdio.h> #include <stdlib.h> // Macro to check for errors from CUDA driver API calls. #define CUDA_CHECK(call) \\ do { \\ CUresult err = call; \\ if (err != CUDA_SUCCESS) { \\ const char *errStr; \\ cuGetErrorString(err, &amp;errStr); \\ fprintf(stderr, &quot;CUDA error at %s:%d: %s\\n&quot;, __FILE__, __LINE__, \\ errStr); \\ exit(1); \\ } \\ } while (0) // Data tile to be passed to the kernel. float data[] = {0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385, 390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450, 455, 460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515, 520, 525, 530, 535, 540, 545, 550, 555, 560, 565, 570, 575, 580, 585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635}; int main() { // Declare and initialize CUDA driver API handles. CUdevice cuDevice; CUcontext cuContext; CUmodule cuModule; CUfunction example_kernel; CUstream stream; CUDA_CHECK(cuInit(0)); CUDA_CHECK(cuDeviceGet(&amp;cuDevice, 0)); CUDA_CHECK(cuCtxCreate(&amp;cuContext, NULL, 0, cuDevice)); CUDA_CHECK(cuStreamCreate(&amp;stream, CU_STREAM_DEFAULT)); // Load the compiled cubin file and get the entry CUDA Tile IR function. // CUDA Tile IR bytecode can also be directly loaded (JIT compilation). CUDA_CHECK(cuModuleLoad(&amp;cuModule, &quot;example.cubin&quot;)); CUDA_CHECK(cuModuleGetFunction(&amp;example_kernel, cuModule, &quot;example_kernel&quot;)); // Allocate memory on the device and copy the input data to it. CUdeviceptr data_ptr; CUDA_CHECK(cuMemAlloc(&amp;data_ptr, sizeof(data))); CUDA_CHECK(cuMemcpyHtoD(data_ptr, data, sizeof(data))); // Launch the kernel. void *kernel_args[] = {&amp;data_ptr}; CUDA_CHECK(cuLaunchKernel(example_kernel, // function 1, 1, 1, // grid dims: must be (1,1,1) 1, 1, 1, // block dims 0, // shared memory bytes: must be 0 stream, // cuda stream kernel_args, // kernel arguments NULL // extra parameters )); CUDA_CHECK(cuCtxSynchronize()); // Clean up. CUDA_CHECK(cuModuleUnload(cuModule)); CUDA_CHECK(cuCtxDestroy(cuContext)); return 0; }\"><pre><code>#include &lt;cuda.h&gt; #include &lt;cuda_runtime_api.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; // Macro to check for errors from CUDA driver API calls. #define CUDA_CHECK(call) \\ do { \\ CUresult err = call; \\ if (err != CUDA_SUCCESS) { \\ const char *errStr; \\ cuGetErrorString(err, &amp;errStr); \\ fprintf(stderr, \"CUDA error at %s:%d: %s\\n\", __FILE__, __LINE__, \\ errStr); \\ exit(1); \\ } \\ } while (0) // Data tile to be passed to the kernel. float data[] = {0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100, 105, 110, 115, 120, 125, 130, 135, 140, 145, 150, 155, 160, 165, 170, 175, 180, 185, 190, 195, 200, 205, 210, 215, 220, 225, 230, 235, 240, 245, 250, 255, 260, 265, 270, 275, 280, 285, 290, 295, 300, 305, 310, 315, 320, 325, 330, 335, 340, 345, 350, 355, 360, 365, 370, 375, 380, 385, 390, 395, 400, 405, 410, 415, 420, 425, 430, 435, 440, 445, 450, 455, 460, 465, 470, 475, 480, 485, 490, 495, 500, 505, 510, 515, 520, 525, 530, 535, 540, 545, 550, 555, 560, 565, 570, 575, 580, 585, 590, 595, 600, 605, 610, 615, 620, 625, 630, 635}; int main() { // Declare and initialize CUDA driver API handles. CUdevice cuDevice; CUcontext cuContext; CUmodule cuModule; CUfunction example_kernel; CUstream stream; CUDA_CHECK(cuInit(0)); CUDA_CHECK(cuDeviceGet(&amp;cuDevice, 0)); CUDA_CHECK(cuCtxCreate(&amp;cuContext, NULL, 0, cuDevice)); CUDA_CHECK(cuStreamCreate(&amp;stream, CU_STREAM_DEFAULT)); // Load the compiled cubin file and get the entry CUDA Tile IR function. // CUDA Tile IR bytecode can also be directly loaded (JIT compilation). CUDA_CHECK(cuModuleLoad(&amp;cuModule, \"example.cubin\")); CUDA_CHECK(cuModuleGetFunction(&amp;example_kernel, cuModule, \"example_kernel\")); // Allocate memory on the device and copy the input data to it. CUdeviceptr data_ptr; CUDA_CHECK(cuMemAlloc(&amp;data_ptr, sizeof(data))); CUDA_CHECK(cuMemcpyHtoD(data_ptr, data, sizeof(data))); // Launch the kernel. void *kernel_args[] = {&amp;data_ptr}; CUDA_CHECK(cuLaunchKernel(example_kernel, // function 1, 1, 1, // grid dims: must be (1,1,1) 1, 1, 1, // block dims 0, // shared memory bytes: must be 0 stream, // cuda stream kernel_args, // kernel arguments NULL // extra parameters )); CUDA_CHECK(cuCtxSynchronize()); // Clean up. CUDA_CHECK(cuModuleUnload(cuModule)); CUDA_CHECK(cuCtxDestroy(cuContext)); return 0; } </code></pre></div> <h3 tabindex=\"-1\" dir=\"auto\">Instructions</h3> <ol dir=\"auto\"> <li>Compile the textual mlir program to CUDA Tile IR bytecode: <code>cuda-tile-translate example.mlir --bytecode-version=13.1 --mlir-to-cudatilebc --no-implicit-module -o example.tilebc</code>.</li> <li>For AoT compilation, compile the bytecode file to a cubin: <code>tileiras --gpu-name sm_100 example.tilebc -o example.cubin</code>. <ol dir=\"auto\"> <li>Substitute <code>sm_100</code> with your supported target architecture.</li> <li>To JIT compile the bytecode at launch time, skip this step and replace <code>example.cubin</code> with <code>example.tilebc</code> in <code>host_example.cpp</code>.</li> </ol> </li> <li>Compile the host program: <code>g++ example_host.cpp -o example -I/usr/local/cuda/include -L/usr/local/cuda/lib64 -lcuda</code>. <ol dir=\"auto\"> <li>Substitute <code>g++</code> with your C++ compiler, and the paths with the correct paths to your CUDA headers and libraries.</li> </ol> </li> <li>Execute: <code>./example</code>.</li> </ol> <p dir=\"auto\">You should see the following terminal output:</p> <div data-snippet-clipboard-copy-content=\"Running example module Data: [0.000000, 5.000000, 10.000000, 15.000000, 20.000000, 25.000000, 30.000000, 35.000000, 40.000000, 45.000000, 50.000000, 55.000000, 60.000000, 65.000000, 70.000000, 75.000000, 80.000000, 85.000000, 90.000000, 95.000000, 100.000000, 105.000000, 110.000000, 115.000000, 120.000000, 125.000000, 130.000000, 135.000000, 140.000000, 145.000000, 150.000000, 155.000000, 160.000000, 165.000000, 170.000000, 175.000000, 180.000000, 185.000000, 190.000000, 195.000000, 200.000000, 205.000000, 210.000000, 215.000000, 220.000000, 225.000000, 230.000000, 235.000000, 240.000000, 245.000000, 250.000000, 255.000000, 260.000000, 265.000000, 270.000000, 275.000000, 280.000000, 285.000000, 290.000000, 295.000000, 300.000000, 305.000000, 310.000000, 315.000000, 320.000000, 325.000000, 330.000000, 335.000000, 340.000000, 345.000000, 350.000000, 355.000000, 360.000000, 365.000000, 370.000000, 375.000000, 380.000000, 385.000000, 390.000000, 395.000000, 400.000000, 405.000000, 410.000000, 415.000000, 420.000000, 425.000000, 430.000000, 435.000000, 440.000000, 445.000000, 450.000000, 455.000000, 460.000000, 465.000000, 470.000000, 475.000000, 480.000000, 485.000000, 490.000000, 495.000000, 500.000000, 505.000000, 510.000000, 515.000000, 520.000000, 525.000000, 530.000000, 535.000000, 540.000000, 545.000000, 550.000000, 555.000000, 560.000000, 565.000000, 570.000000, 575.000000, 580.000000, 585.000000, 590.000000, 595.000000, 600.000000, 605.000000, 610.000000, 615.000000, 620.000000, 625.000000, 630.000000, 635.000000]\"><pre><code>Running example module Data: [0.000000, 5.000000, 10.000000, 15.000000, 20.000000, 25.000000, 30.000000, 35.000000, 40.000000, 45.000000, 50.000000, 55.000000, 60.000000, 65.000000, 70.000000, 75.000000, 80.000000, 85.000000, 90.000000, 95.000000, 100.000000, 105.000000, 110.000000, 115.000000, 120.000000, 125.000000, 130.000000, 135.000000, 140.000000, 145.000000, 150.000000, 155.000000, 160.000000, 165.000000, 170.000000, 175.000000, 180.000000, 185.000000, 190.000000, 195.000000, 200.000000, 205.000000, 210.000000, 215.000000, 220.000000, 225.000000, 230.000000, 235.000000, 240.000000, 245.000000, 250.000000, 255.000000, 260.000000, 265.000000, 270.000000, 275.000000, 280.000000, 285.000000, 290.000000, 295.000000, 300.000000, 305.000000, 310.000000, 315.000000, 320.000000, 325.000000, 330.000000, 335.000000, 340.000000, 345.000000, 350.000000, 355.000000, 360.000000, 365.000000, 370.000000, 375.000000, 380.000000, 385.000000, 390.000000, 395.000000, 400.000000, 405.000000, 410.000000, 415.000000, 420.000000, 425.000000, 430.000000, 435.000000, 440.000000, 445.000000, 450.000000, 455.000000, 460.000000, 465.000000, 470.000000, 475.000000, 480.000000, 485.000000, 490.000000, 495.000000, 500.000000, 505.000000, 510.000000, 515.000000, 520.000000, 525.000000, 530.000000, 535.000000, 540.000000, 545.000000, 550.000000, 555.000000, 560.000000, 565.000000, 570.000000, 575.000000, 580.000000, 585.000000, 590.000000, 595.000000, 600.000000, 605.000000, 610.000000, 615.000000, 620.000000, 625.000000, 630.000000, 635.000000] </code></pre></div> <h2 tabindex=\"-1\" dir=\"auto\">Contributions and Support</h2> <p dir=\"auto\"><strong>Note: We are currently not accepting external contributions.</strong></p> <p dir=\"auto\">While CUDA Tile is an open-source project, we are not accepting external contributions at this time. The project is under active development with a focused roadmap. We encourage you to use GitHub Issues to report bugs, provide feedback, and share your experiences with CUDA Tile. Your input helps us improve the project and prioritize future development.</p> <h2 tabindex=\"-1\" dir=\"auto\">License</h2> <p dir=\"auto\">CUDA Tile IR is licensed under the <a href=\"https://llvm.org/LICENSE.txt\" rel=\"nofollow\">Apache License v2.0 with LLVM Exceptions</a>.</p>"},{"id":"https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/","title":"Seven Diabetes Patients Die Due to Undisclosed Bug in Abbott's Glucose Monitors","link":"https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/","hnCommentsUrl":"https://news.ycombinator.com/item?id=46388040","content":"<a href=\"https://news.ycombinator.com/item?id=46388040\">Comments</a>","date":1766708980000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div> <p>by on December 23, 2025 </p> <p>I <a href=\"https://sfconservancy.org/blog/2025/nov/06/juggluco-foss-continuous-glucose-montior-diabetes/\">wrote last month</a> about my diabetes diagnosis this year and my difficult choice to wear a proprietary device (called a <acronym title=\"continuous glucose monitor\">CGM</acronym>) on my arm 24/7 to continuously monitor my glucose levels. Like my friend and colleague, Karen M. Sandler ‚Äî who previously made a much higher-stakes choice to receive a proprietary implanted defibrillator to keep her safe given her genetic heart condition ‚Äî I reluctantly chose to attach proprietary hardware and software to my body.</p> <p>The device itself is quite proprietary, but fortunately the <acronym title=\"Free and Open Source Software\">FOSS</acronym> community has reverse engineered its activation and data collection protocols ‚Äî creating an Android application that does a better job than the manufacturers' proprietary ones<sup><a id=\"return-juggluco-previous-post\" href=\"#footnote-juggluco-previous-post\">0</a></sup>.</p> <p>Here in the USA, we strangely use capitalism as the center of our health care system. Two major for-profit competing brands of <acronym title=\"continuous glucose monitor\">CGM</acronym> are available here. My diabetes specialist prefers the (ironically named) Freestyle Libre Plus from Abbott. I (also rather strangely) bring a prescription for <em>electronics</em> to a pharmacy every month. On 2025-12-03, that pharmacy sent me an alarming text message (shown here). </p> <h3>Abbott Killed Seven Patients</h3> <p>After reading that text, I found <a href=\"https://www.fda.gov/medical-devices/medical-device-recalls-and-early-alerts/early-alert-glucose-monitor-sensor-issue-abbott-diabetes-care\">the USA <acronym title=\"Food and Drug Administration\">FDA</acronym> announcement</a>. My spouse cross-referenced the lot numbers while I read them off from all my Freestyle boxes<sup><a id=\"return-no-match-abbott-site\" href=\"#footnote-no-match-abbott-site\">1</a></sup>. I had indeed recently worn an impacted device!</p> <p>Only because my diabetes is so early of a stage was I relatively safe. The FDA reports that Freestyle injured over 700 people and <strong>killed seven people</strong> with this bug. Specifically, the bug caused the device to falsely report an <em>extremely low</em> glucose level. Advanced stage diabetics use low reading information to inform them that they may have too much insulin currently. The usual remedy is to eat something sugary to raise glucose in the blood. Such should be done only with great care, as a false low reading can harm and even kill the patient (who eats a high-sugar-content item while glucose in the blood is, in fact, not low).</p> <p>Proprietary software in medical devices harming patients is not new. In 1985, the <a href=\"https://en.wikipedia.org/wiki/Therac-25#Problem_description\">Therac-25 killed three people</a>. In 2020, hundreds of patients who relied on a financially troubled tech startup found their occular implants suddenly unsupported. Some patients went <a href=\"https://spectrum.ieee.org/bionic-eye-obsolete\">blind as the devices powered down without updates</a>. There are more examples that I could include here, but rereading these horrific stories is frankly more than I can take right now when I think of fellow diabetes sufferers who were ‚Äúkilled by code‚Äù recently..</p> <h3>Would FOSS Have Saved Patients' Lives?</h3> <p>It's hubris for activists to guarantee that harm would be prevented if Freestyle had publicly released the hardware specifications and the complete, corresponding source code (<abbr>CCS</abbr>). <acronym title=\"Free and Open Source Software\">FOSS</acronym> isn't immune to bugs ‚Äî even dangerous ones. However, in the centuries since the Enlightenment, we know that the scientific method <em>depends</em> on public disclosure about data and wide-reaching peer review of past work. FOSS (plus a publicly disclosed hardware design) wouid allow the millions of hardware and software engineers to peer-review the integrity, security, and safety of the devices to which patients entrust their lives. We achieve the promise of humanity when we each entrust our safety and health to our entire community ‚Äî not merely a single for-profit entity.</p> <p>We also will probably never know whether this issue was in hardware or software. The bug disclosure is incredibly vague, and it remains unclear how much investigation was done (if any) by government regulators into this problem. As a public policy and public health matter, the public <em>deserves to know</em> the technical details (software and hardware) of both the functioning device and the failed devices. NGOs should be permitted to perform their own investigations and confirmations of public safety.</p> <h3>What's Next?</h3> <p>Given that the hardware, software, and medical for-profit industries refuse to put the rights, safety and security of patients first, wrongful death lawsuits are typically the only way to hold these companies accountable. Yet, there are <em>very few</em> people who have not agreed Abbott's toxic terms of their proprietary companion application ‚Äî I guestimate that fewer than 1% of Freestyle-using patients have used Juggluco from their very start (and thus never agreed to Abbott's terms). This is significant because Abbott <a href=\"https://sfconservancy.org/videos/2025-09_Abbott-Freestyle-Libre-Plus-App-Agreement.pdf#page=78\">includes a comprehensive one-way indemnity for themselves in the terms</a>. I hope that a class action suit begins soon on this matter, but I wonder and worry that so much of the class may have signed this indemnity (which may make the road to justice bumpier).</p> <p>Finally, I want to offer that if there is anyone out there who does tear-downs of extremely tiny electronic devices, I would be thrilled to find a volunteer who would like to see if we can either extract any software components from the device, or reverse-engineer the hardware. I have saved and sanitized all of my prior <acronym title=\"continuous glucose monitors\">CGMs</acronym>. I'd gladly send one along to anyone who wants to give a try at taking them apart. (Contact SFC or <a rel=\"me\" href=\"https://fedi.copyleft.org/@bkuhn\">contact me on the Fediverse (via Mastodon)</a> if you're available to do this work.)</p> <p>For my part, I look forward (after the <a href=\"https://sfconservancy.org/vizio\">Vizio</a> trial) to sending some patches to Juggluco and also getting Juggluco available in F-Droid. Our best option in the face of these powerful medical device companies curtailing our rights is to invest our volunteer time into the edges where <acronym title=\"Free and Open Source Software\">FOSS</acronym> has resiliently worked around the constant roadblocks erected by bad actors.</p> <hr> <p><sup><a id=\"footnote-juggluco-previous-post\" href=\"#return-juggluco-previous-post\">0</a></sup>My <a href=\"https://sfconservancy.org/blog/2025/nov/06/juggluco-foss-continuous-glucose-montior-diabetes/\">prior post about <acronym title=\"continuous glucose monitors\">CGMs</acronym> discussed</a> the GPLv3'd Juggluco in more detail.</p> <p><sup><a id=\"footnote-no-match-abbott-site\" href=\"#return-no-match-abbott-site\">1</a></sup> In a fascinating turn of events, at least one of my past monitors (of which I fortitously saved all the boxes with the lot/serial number on them) is listed in <a href=\"https://www.fda.gov/media/189900/download?attachment\">the FDA's spreadsheet</a> as recalled lot, yet the serial number is listed as ‚Äú safe to use‚Äù on <a href=\"https://www.freestylecheck.com/us-en/product-lookup.html\">Abbott's webform</a> ü§î ‚Ä¶ I'm left wondering how I can trust Abbott to write reliable software stuck into my arm if they can't even write a web form that cross-references serial numbers to lots correctly üò¨. </p> <p><a href=\"https://sfconservancy.org/blog/2025/dec/23/seven-abbott-freestyle-libre-cgm-patients-dead/\">[permalink]</a></p> </div></div>"},{"id":"https://news.ycombinator.com/item?id=46385197","title":"Ask HN: What skills do you want to develop or improve in 2026?","link":"https://news.ycombinator.com/item?id=46385197","hnCommentsUrl":"https://news.ycombinator.com/item?id=46385197","content":"<a href=\"https://news.ycombinator.com/item?id=46385197\">Comments</a>","date":1766678937000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div id=\"bigbox\"><table><tbody><tr id=\"46385197\"><td><span><a href=\"https://news.ycombinator.com/item?id=46385197\">Ask HN: What skills do you want to develop or improve in 2026?</a></span></td></tr><tr><td><span><span id=\"score_46385197\">81 points</span> by <a href=\"https://news.ycombinator.com/user?id=meridion\">meridion</a> <span title=\"2025-12-25T16:08:57 1766678937\"><a href=\"https://news.ycombinator.com/item?id=46385197\">6 hours ago</a></span> | <a href=\"https://news.ycombinator.com/hide?id=46385197&amp;goto=item%3Fid%3D46385197\">hide</a> | <a href=\"https://hn.algolia.com/?query=Ask%20HN%3A%20What%20skills%20do%20you%20want%20to%20develop%20or%20improve%20in%202026%3F&amp;type=story&amp;dateRange=all&amp;sort=byDate&amp;storyText=false&amp;prefix&amp;page=0\">past</a> | <a href=\"https://news.ycombinator.com/fave?id=46385197&amp;auth=0e432687d48b87f40ea878fb7c05fe2874ed8e2c\">favorite</a> | <a href=\"https://news.ycombinator.com/item?id=46385197\">97&nbsp;comments</a></span></td></tr><tr><td><div><p>Thread for 2025: <a href=\"https://news.ycombinator.com/item?id=42509408\">https://news.ycombinator.com/item?id=42509408</a></p><p>Thread for 2024: <a href=\"https://news.ycombinator.com/item?id=38782613\">https://news.ycombinator.com/item?id=38782613</a></p><p>Thread for 2023: <a href=\"https://news.ycombinator.com/item?id=33873800\">https://news.ycombinator.com/item?id=33873800</a></p><p>Here are mine:</p><p>Technical skills:</p><p>- Among my last year's goals was to take on VR dev, which sadly I did not get to. Punting it to 2026. I'm thinking to get the Samsung Galaxy XR and experiment with some VR apps and learn the fundamentals of spatial computing. As an Android mobile developer, that feels like a natural extension.</p><p>- Complete the \"UCSanDiegoX: Computer Graphics II: Rendering\" computer graphics course. I did the first course in the series and found it enlightening (no pun intended)</p><p>- Create an e2e project that earns money as a side gig. It's time to put my product and technical knowledge to practice and actually build something people want.</p><p>- Leverage AI across all my endeavors. AI tools are here to stay and the more I know how to use them effectively, the better. The speed boost in learning a new framework/concept is phenomenal.</p><p>Non-technical skills:</p><p>- Expand my social circle - the unstable tech climate made me realize the importance of maintaining a healthy social network. My goal is to connect with more people both inside my company and outside, by both proactively reaching out and going to meetups in my area. In fact, I invite fellow NYC-based HN-ers to contact me at cybercreampuff at yahoo dot com, in case you want to meet up!</p></div></td></tr></tbody></table><br> </div></div>"},{"id":"http://miod.online.fr/software/openbsd/stories/udl.html","title":"When a driver challenges the kernel's assumptions","link":"http://miod.online.fr/software/openbsd/stories/udl.html","hnCommentsUrl":"https://news.ycombinator.com/item?id=46388059","content":"<a href=\"https://news.ycombinator.com/item?id=46388059\">Comments</a>","date":1766709135000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<div id=\"readability-page-1\" class=\"page\"><div id=\"content\"> <p> Unix-based systems have been around for more than 50 years now. Although the best design ideas still prevail to this day, the evolution of the computing industry has forced operating system designers to rethink the way they work, multiple times over time. </p><p> From a device driver point of view, the most important change was the change from fixed, compile-time hardware configuration, enumerated upon boot and never changing afterwards, to a more dynamic model, where devices can come and go: storage devices first, with the first hotplug-capable SCSI controllers in the first half of the 1990s, and complete devices shortly later, first with the introduction of the PCMCIA bus on laptops, then with USB and Firewire, which were not limited to laptops. </p><p> While PCMCIA support in open source operating systems had lingered for a few years before being integrated (both in Linux with \"pcmcia-cs\" and in FreeBSD with the \"laptop package\"), by the time USB support was being worked on, the required changes to accept/allow that devices may show up or disappear at any time had been completed and tested, and the kernel had no excuse not to cope with removable devices. </p><p> Today's story is the story of a <a href=\"https://man.openbsd.org/udl\">device driver</a> which caused some kernel assumptions to no longer stand, and the work done to remediate this situation, letting the kernel cope with the new world order. </p><hr> <p> This story starts on the 12th of march 2009. <b>Theo de Raadt</b> is travelling to Japan to visit fellow OpenBSD developer <b>Ryan McBride</b>, best known for implementing the CARP network high-availability protocol in OpenBSD, who has been living in Japan for a few years already. </p><p> Every time he goes to Japan, Theo never misses the opportunity to visit Akihabara in Tokyo, in order to find new computing devices to play with. </p><p> Among the devices he picks this time, is a small display, the size of a smartphone, with an usb cable, shown here running an X server under OpenBSD for the first time, a few months later. <i>(picture courtesy of Marcus Glocker)</i> <br> <img src=\"http://miod.online.fr/software/openbsd/stories/udl.jpg\" width=\"640\" height=\"480\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"> </p><p> The company making these USB displays is called <a href=\"https://web.archive.org/web/20090331070312/http://www.displaylink.com/\">DisplayLink</a>. </p><p> DisplayLink provides binary-only drivers for Microsoft Windows and Apple Mac OSX, and nothing more. This caused some frustration in the free software world, and <b>Florian Echtler</b>, with the help of <b>Chris Hodges</b>, worked on reverse engineering the device, with the intent to get it working under Linux. All of their work is nowadays described at <a href=\"https://floe.butterbrot.org/matrix/hacking/dlnk/\">https://floe.butterbrot.org/matrix/hacking/dlnk/</a>. </p><p> Of course, at this point, there is no support for these devices in OpenBSD either, and they attach as generic USB devices when connected: </p><div id=\"pre\"> <pre>&lt;deraadt&gt; ugen0 at uhub0 port 3 \"DisplayLink LCD-8000U\" rev 2.00/0.02 addr 2 &lt;deraadt&gt; Picked up that too </pre> </div> <p> Moments later, <b>Matthieu Herrb</b> points to Florian Echtler's information: </p><div id=\"pre\"> <pre>&lt;matthieu&gt; &lt;http://floe.butterbrot.org/displaylink/doku.php&gt; </pre> </div><p> (which nowadays redirects to <a href=\"https://github.com/floe/tubecable/tree/master/doc\">https://github.com/floe/tubecable/tree/master/doc</a>) </p><p> The DisplayLink device gets passed from Theo de Raadt to developer <b>Claudio Jeker</b>, who lives in Switzerland close to another developer who has been recently working on USB devices, and will be the hero of this story: <b><a href=\"https://nazgul.ch/dev_openbsd.html\">Marcus Glocker</a></b>. </p><p> Once told he will be given the device, Marcus does what everyone in his place would have done: he asks the manufacturer politely for documentation on april 6th. </p><p> There was no contact information at DisplayLink, only a \"contact us\" webform in which you could enter your name and email address, and your message. </p><div id=\"pre\"> <pre>Name: Marcus Glocker Message: Dear Ladies and Gentleman, We are thinking about developing an OpenSource driver for your DL-120/DL-160 chipsets. This would allow several OpenSource projects (e.g. NetBSD, FreeBSD, OpenBSD, Linux, X.org) to add support for DisplayLink based USB LCD devices, and therefore extend your potential user base. Therefore we would like to talk with you about getting basic documentation for your chipsets. It would be nice if the right person for this topic could contact me for further communication on the e-mail address stated in this form. Regards, Marcus Glocker, OpenBSD Developer </pre> </div> <p> The answer was as unhelpful as possible: </p><div id=\"pre\"> <pre>Date: Mon, 6 Apr 2009 12:49:42 +0100 From: Jason Slaughter (DisplayLink) To: Marcus Glocker Cc: Bernie Thompson (DisplayLink) Subject: Opensource Driver for OpenBSD Hello Marcus, thank you for your email. DisplayLink USB graphics are a software graphics card: graphics functions are performed on the CPU and then compressed and sent across the USB cable to be decompressed in a DisplayLink DL-120/DL-160 chip on the other side. This means that - unlike most PC hardware - getting basic documentation about the chip would not provide enough information to implement a driver for BSD. However, DisplayLink is current in the process of putting together a library that will provide the functions necessary to drive a DisplayLink USB graphics chips, and it is our intention to release this library under an open source license in the very near future. This library would provide you with all of the information necessary to develop an x.org compatible driver for DisplayLink USB graphics devices on *BSD. The best point of contact for you might be Bernie Thompson (cc'd), our VP of Software Platforms based in Seattle, Washington. We could start with a conversation on email, but Bernie and I would also be happy to have a quick phone call with you to help you understand how DisplayLink graphics work and what our plans are for supporting open source platforms. I would also be curious to get your thoughts on how DisplayLink device support might fit into BSD. Thank you, Jason </pre> </div> <p> At the same time, Marcus was sharing the results of this discussion with the other OpenBSD developers on our private chatroom. </p><div id=\"pre\"> <pre>&lt;mglock&gt; wow. &lt;kettenis&gt; wow? &lt;mglock&gt; DisplayLink TM seems to be very communactive. &lt;mglock&gt; asked the for specs for their DL-120/DL-160 chips, and got a detailed answer withing 4 hours. &lt;mglock&gt; they want to call me :-) &lt;oga&gt; they're actually responding? &lt;oga&gt; wow... &lt;oga&gt; I'd heard they were quite reticent. Someone on the xorg lists is rev engineering them for that reason. Not so much progress so far &lt;mglock&gt; interessting. &lt;oga&gt; Well, i asked him if he was reverse engineering, he said yes. I'm sure I asked if he'd asked for docs, but the reply from him doesn't include that bit. &lt;oga&gt; If you can get specifications freely, I'd be interested in a copy &lt;oga&gt; those things are pretty cool, in an oddball kinda way &lt;mglock&gt; sure. keep you informed. lets see how far we come. [...] &lt;mglocker&gt; just wrote back to the DisplayLink guys. lets see what happens. </pre> </div> <p> The day after: </p><div id=\"pre\"> <pre>&lt;mglocker&gt; re &lt;mglocker&gt; got the DisplayLink device from claudio. &lt;mglocker&gt; and an answer from DisplayLink themselfs. &lt;mglocker&gt; we will get no docs. &lt;kettenis&gt; bastards! &lt;mglocker&gt; but the seem to release the opensource library soon. based on libusb. &lt;mglocker&gt; library will be LGPL licensed ... </pre> </div> <p> However, Marcus does not remain inactive waiting for a hypothetical code release from DisplayLink. After weeks of trials, he reaches a point where he has a terminal (text-mode emulation) running on it, albeit slowly, and talks about it on may 1st, leading to a productive brainstorming session with a few other OpenBSD developers: </p><div id=\"pre\"> <pre>&lt;mglocker&gt; hmm, looks bad for the USB display driver for wscons. all that many little USB bulk transfers which are necessary to draw the fonts make the display rendering slow. &lt;mglocker&gt; i think there is no way to improve it further. &lt;oga&gt; mglocker: make a shadow area and blit the whole lot in one go &lt;oga&gt; so make a line of text in one go the blit all of it &lt;oga&gt; and use blits to move lines of text around for scrolling &lt;mglocker&gt; nah, you need to be able to draw one character. &lt;oga&gt; hmmm &lt;oga&gt; did they release that library by the way? &lt;deraadt&gt; there must be memory on the thing, to pre-load the fonts there &lt;mglocker&gt; even if you preload the fonts, it doesn't help. to bit blit one character you also need to setup a bulk xfer. &lt;oga&gt; oh, offscreen memory if it has, then it's just blitting chars. &lt;oga&gt; oh jesus. &lt;mglocker&gt; it's the same amount of bulk xfers in the end. &lt;mglocker&gt; yes, i tried exactly that. &lt;oga&gt; how slow is slow? &lt;mglocker&gt; it is exactly as slow as before. &lt;oga&gt; I mean how slow is the output all told? &lt;mglocker&gt; i think it's not the content of the bulk xfer making it slow, but the little peaces of bulk xfers. &lt;mglocker&gt; ah. &lt;mglocker&gt; on my amd64 it's like a 19200 modem connection or such :-) &lt;deraadt&gt; how do they make it fast, then? &lt;oga&gt; what info do you have on it? &lt;drahn&gt; can multiple 'blits' be combined? &lt;oga&gt; all I heard about was some libarary that they will release, and the reveng stuff &lt;sthen&gt; if you have a shadow buffer, can't you update multiple chars in one go? &lt;mglocker&gt; you could. but then i would need to \"collect\" lines? &lt;sthen&gt; yep. &lt;mglocker&gt; shudder. &lt;mglocker&gt; sounds ugly. &lt;oga&gt; welcome to graphics, mate. &lt;mglocker&gt; heh &lt;mglocker&gt; i think i quit that area ;-) &lt;drahn&gt; a timeout to gather up several or polling waiting for the previous one to complete? &lt;oga&gt; what information do you have on the hardware itself? &lt;mglocker&gt; http://floe.butterbrot.org/displaylink/doku.php &lt;mglocker&gt; that's what we have. &lt;oga&gt; oh, that's the reveng stuff? &lt;oga&gt; yeah, seen it. &lt;mglocker&gt; there is compression, which doesn't seem to work yet. but i'm not sure if the compression stuff would help us for wscons. &lt;mglocker&gt; since again, you need to setup a bulk xfer for each character. &lt;oga&gt; good for bulk data moves. less good for char-by-char &lt;mglocker&gt; and the thing which make it slow seems to be the setup overhead, not the content itself. &lt;mglocker&gt; yes. &lt;mglocker&gt; i don't know how the driver on windows works, but i don't guess they do something like char-by-char, as we do for wscons. &lt;oga&gt; is this a problem for scrolling, or for typing? &lt;oga&gt; scrolling you can coalesce as long as the timeout is short. &lt;mglocker&gt; the scrolling seems fine. &lt;oga&gt; and it'd be a LOT better than char-by-char. &lt;oga&gt; so it's just typing? &lt;mglocker&gt; the problem is for rendering the chars. e.g. \"ls -l\". &lt;oga&gt; if you put a timeout say (out of my arse) 10ms. all chars that come in then go out at the same time &lt;oga&gt; no one types that fast, so you see char by char, but ls gets done in one go &lt;mglocker&gt; that would help for sure. &lt;oga&gt; try that. one transfer per char is not going to help you &lt;mglocker&gt; the question is how to implement this in a nice manner ... sigh. &lt;oga&gt; several ways present themselves &lt;oga&gt; font in offscreen memory, when you get a char make blit command for that char to where you want it. multi commands in one bulkt transfer (which is apparently ok) &lt;mglocker&gt; and then send it after the timeout is done (if there is something)? &lt;oga&gt; when the timeout fires, launch off all you've got &lt;oga&gt; exactly &lt;oga&gt; just queue up commands and put htem all in one bulk &lt;oga&gt; the timeout will need tuning to be a good average between latency and bandwidth though &lt;drahn&gt; scrolls are a bit extra info in the queue &lt;mglocker&gt; putting them in one bulk would improve it for sure. &lt;oga&gt; give it a try, and mail me the diff so I can see what you're doing &lt;oga&gt; i'm curious, even though I don't understand the usb stack at all &lt;mglocker&gt; the usb part is not difficult for this device, really. &lt;oga&gt; oh good. means I might get it. &lt;mglocker&gt; \"ls -l /etc\" takes about 5 seconds now to render. that's not nice ... &lt;mglocker&gt; ok, thanks for the hints. maybe i can get something together which helps ... &lt;oga&gt; for wsfb you'll want to provide a mmap area and sync that as well. that will suck though &lt;oga&gt; horribly &lt;oga&gt; it'll need a proper driver to be usabel for X &lt;mglocker&gt; yes. there are different opinions for the X driver. &lt;mglocker&gt; some say, use wsfb, some say you need to write a own accel driver for X :-) &lt;oga&gt; wsfb iirc mmaps the framebuffer, so you'd need to shadow it and update every X times a second &lt;oga&gt; you'd need the compression at least for it to not suck &lt;mglocker&gt; exactly. &lt;mglocker&gt; jup. and the compression part is missing from the rev. eng. as it looks. &lt;oga&gt; it'd help to know more about the hardware it really would &lt;oga&gt; without compression wsfb will be FAR too slow &lt;mglocker&gt; i believe so, too, now. &lt;oga&gt; no mmap over usb :( &lt;mglocker&gt; no way. &lt;miod&gt; mglocker: can't you copy the font image to offscreen video memory and do screen-to-screen blt to display chars? &lt;miod&gt; that's the canonical trick. &lt;oga&gt; that was my suggestion, with batching to keep from too many bulks. &lt;miod&gt; ah &lt;mglocker&gt; miod: i thought about that, too, but the bulk xfers keep the same. &lt;miod&gt; (not caught up with logs yet) &lt;mglocker&gt; one per bitblit. &lt;mglocker&gt; i've even tested it. &lt;mglocker&gt; it doesn't help at all. it's exactly as slow as before. &lt;miod&gt; can you queue the xfers and schedule a batch of them 25 times per second? &lt;deraadt&gt; you can only request one blt per bulk? &lt;oga&gt; the rev eng says you can do more, hence the batching &lt;deraadt&gt; sounds like a retarded protocol if you can only do one blt per bulk &lt;mglocker&gt; miod: that's what oga suggested. &lt;mglocker&gt; yes you can queue more than one bitblit. &lt;mglocker&gt; but that would be i would need to \"collect\" chars. what oga said. &lt;mglocker&gt; and fire them on a timeout. </pre> </div> <p> After going back to the drawing board, the driver performance improves, and it becomes worth commiting to the OpenBSD source code repository, so that people can try it and developers can work on improving it. </p><p> The <a href=\"https://github.com/openbsd/src/commit/c197399b252cb241ef20f393eb34bd395fb76b66\">first stab of a driver</a> hits the tree on may 9th. </p><p> Three days later, possibly related to that commit, but we'll never know... </p><div id=\"pre\"> <pre>Date: Tue, 12 May 2009 20:08:44 +0100 From: Jason Slaughter (DisplayLink) To: Marcus Glocker Cc: Bernie Thompson (DisplayLink) Hello Marcus, Just an update on this: our source code library will be going live this Friday and you'll be able to find it by going to http://displaylink.org around Friday afternoon. We would be happy to send you a few DL-160 based USB to DVI graphics adapters if you think you would get good use out of them. If so, let me know how many you'd like and send me your mailing address. With any luck they'll get to you before the code goes live on displaylink.org. Thank you, Jason </pre> </div><p> which leads to this conversation at OpenBSD headquarters: </p><div id=\"pre\"> <pre>&lt;mglocker&gt; oho. DisplayLink is answering again. &lt;deraadt&gt; heh &lt;mglocker&gt; the will release the opensource driver this friday. &lt;mglocker&gt; and they want to send me DL-160 based USB to DVI graphic adapters. &lt;oga&gt; the library you mean? &lt;mglocker&gt; jup. &lt;oga&gt; i'm sure i found a dl-160 going for 50 quid on amazon the other day &lt;oga&gt; out of stock though &lt;mglocker&gt; so, who wants to have such a toy? &lt;oga&gt; I'd love one. &lt;mglocker&gt; ok, one for you, one for me, anyone else? &lt;mglocker&gt; Florian Echtler will be glad to hear this. &lt;mglocker&gt; because the compression stuff is hard to crack. &lt;oga&gt; I wonder how complete the library will be &lt;oga&gt; what license, by the way? &lt;mglocker&gt; pretty complete i hope ;-) &lt;oga&gt; hope is definitely the word. &lt;mglocker&gt; they wrote me about the license. some GPL shiz if i remember right ... &lt;oga&gt; well it's a libusb library anyway, so it'll want a rewrite &lt;oga&gt; GPL? if they're targetting X then they should know X uses MIT. &lt;mglocker&gt; *shrug* &lt;mglocker&gt; after the last conversation i don't know how much they really know about X ... &lt;mglocker&gt; probably they put the library out and hope somebody will write the X driver for them ... &lt;oga&gt; time permitting i'll probably write one &lt;oga&gt; it'll be either drm or wscons based depending on what I learn about the hardware &lt;oga&gt; s/wscons/wsfb-style &lt;mglocker&gt; oga: if you going to start on the X driver, we should sync with Florian maybe. it would be probably good if not three people start to write on the X driver when the library has been released I guess ... &lt;oga&gt; yeah. &lt;oga&gt; if we're doing stuff with wscons for the driver though, then it won't work on linux. &lt;oga&gt; I don't know how fbdev works. &lt;mglocker&gt; and he started some sourceforge page today, to find out about the compression stuff more :-) i'll forward him the mail from displaylink now. </pre> </div> <p> Three more days pass, and indeed, on may 15th... </p><div id=\"pre\"> <pre>&lt;mglock&gt; re &lt;mglock&gt; http://www.freedesktop.org/wiki/Software/libdlo &lt;mglock&gt; the library is online. &lt;mglock&gt; read first feedback from the displaylink rev. eng. mailling-list. &lt;mglock&gt; it seems to be useless. &lt;mglock&gt; since the whole compression stuff is missing. &lt;mglock&gt; need to look at it myself later. </pre> </div><p> (the libdlo url is now <a href=\"https://libdlo.freedesktop.org/wiki/\">https://libdlo.freedesktop.org/wiki/</a></p><a>) <p> 5 hours later: </p><div id=\"pre\"> <pre>&lt;oga&gt; mglock: displaylink rev. eng have a mailing list? &lt;oga&gt; where? &lt;mglock&gt; erm, yes. &lt;mglock&gt; https://lists.sourceforge.net/lists/listinfo/displaylink-devel &lt;oga&gt; mglock: thanks &lt;oga&gt; hmmm, glancing at the library it is not that useful &lt;oga&gt; disappointing </pre> </div> <p> 2 hours later: </p><div id=\"pre\"> <pre>&lt;oga&gt; mein gott! libdlo is ugly &lt;oga&gt; ERR_GOTO(function(args)) ... </pre> </div> <p> and 2 more hours later: </p><div id=\"pre\"> <pre>&lt;mglocker&gt; i've setup another mail to displaylink asking them why they didn't include the compression stuff in their library :-) </pre> </div> <p> On the same day, on the displaylink-devel list: </p><div id=\"pre\"> <pre>Date: 2009-05-15 13:59:30 From: Florian Echtler To: displaylink-devel mailing list Subject: Re: [Displaylink-devel] News from Displaylink &gt; Sigh, pretty disappointing. LGPL licensed first and then it doesn't &gt; contain the compression stuff. What do the think how people can &gt; write an usable X.Org driver with this? I gonna ask them. All right, this is getting a bit ridiculous. Looking in dlo_usb.c, they labeled the \"set encryption\" request as \"select channel\", and the \"null-key\" for disabling encryption is called STD_CHANNEL. Okay, maybe just terminology. However, in dlo_data.h, there's suddenly always an DLO_MODE_ENABLE_* sequence (never seen that before), and the DLO_MODE_DATA stuff looks totally random. Hey, look, in mode_select(), it's always sending the ENABLE sequence to the \"select channel\" command, then the binary blob and then DLO_MODE_POSTAMBLE. Hmm, this POSTAMBLE looks just like the STD_CHANNEL default key! Oh, what a surprise, the register sets for the modes are _still_ encrypted - never mind that we can decrypt this since Christmas last year. Displaylink, I really don't get this. I'll have a closer look at the way the stride registers are implemented; this is one of the smaller parts which I still would like to figure out, but in general, this library is absolutely useless to the wider opensource community. It's obviously designed for some embedded shops which want to use it for non-realtime stuff like LCD signs etc., <b>but it's just ridiculous to still try and obfuscate parts of an opensource library.</b> Florian -- 0666 - Filemode of the Beast </pre> </div> (emphasis mine) <p> Two more weeks pass, during which Florian Echtler continues his reverse engineering efforts and figures out the compression scheme used by the DisplayLink chip. </p><div id=\"pre\"> <pre>&lt;mglocker&gt; for those who are interessted in the displaylink stuff ... the compression has been cracked. florian echtler did a library and showed it to the displaylink guys ;-) &lt;mglocker&gt; http://lists.freedesktop.org/archives/libdlo/2009-May/000092.html &lt;mglocker&gt; http://floe.butterbrot.org/external/tubecable-0.1.tar.bz2 &lt;mglocker&gt; ~mglocker/dldemo2_mglocker.tar.gz &lt;mglocker&gt; if you want to have a kind of C-style demo for the compression. </pre> </div> <p> Minor fixes to the udl driver occur on june 1st, then there is not much activity until Marcus resumes working on it, in order to add compression support. </p><p> On august 14th, he mentions the need for a compression table which could be stored on the filesystem, rather than in the kernel image: </p><div id=\"pre\"> <pre>&lt;mglocker&gt; anyone mind if we would keep a 300kB huffman table in /etc/firmware? &lt;miod&gt; that's not really firmware... can't you generate the table at runtime? is this for udl evilness? &lt;mglocker&gt; jup, for the huffman pixel difference compression. i could store the table into an *.h file, but i'm not sure if this is what we want. &lt;mglocker&gt; i don't think we can \"generate\" the table. &lt;miod&gt; but yes, although it's not technically firmware, it's loadable data, so why not &lt;mglocker&gt; yeah. it's probably worth an exception in that case. &lt;mglocker&gt; i can draw some compressed stuff already, but some issues left to fix. </pre> </div> </a><p><a> And on august 25th, </a><a href=\"https://github.com/openbsd/src/commit/2fa8dc63a726975bf998789c994a878960127963\">compression support</a> is added to the driver. </p><p> As more and more developers are trying the driver on as many platforms as possible, we start hitting situations where heavy screen adjustments would cause the output to stall, or some display updates to get lost. </p><p> Investigating these showed that the device was overflowed with requests, and needed time to process all the pending operations until further requests could be made. </p><p> This was not very different from a serial line link, where the serial chip has a small (and sometimes nonexistent) FIFO queue, and no more characters can be transmitted when it gets full, until the characters get transmitted over the wire. </p><p> For serial links, the kernel handles this nicely, by forcing writers to sleep when the FIFO gets full, and waking them when space becomes available in the FIFO. </p><p> But for display terminal emulations, there was no such thing, because it was assumed that every display operation (output a character, draw the cursor shape, scroll the display in any direction) could be done without having to wait. </p><p> And in fact, until the DisplayLink driver came into use, this was the case: display drivers either had full access to the display memory and could do the required changes directly in memory or, as e.g. for the <a href=\"https://www.openbsd.org/vax.html\">Vax</a> <a href=\"https://man.openbsd.org/OpenBSD-5.9/vax/gpx\">gpx</a> driver [which I will write the story of soon...], by instructing the graphics controller to perform these changes. </p><p> But here, over the USB bus, bandwidth has to be shared with all the other USB devices on the controller, and the USB controller itself might not be able to queue enough requests for the udl needs - in other words, this is similar to the serial FIFO. </p><p> The graphics display being, from the kernel point of view, a special case of serial line, we could benefit from the FIFO behaviour, even though this had never been the case. </p><p> So a cheap and easy way to solve the \"display gets overflowed with requests\" problem would be to simply let the display operation routines (draw the cursor, paint a character...), which until now where `void` operations, return a value letting their caller know whether they had been able to perform the requested operation or not. The DisplayLink specific implementation of these requests could then return failure when the device FIFO is full, and let the upper layers know that they need to wait and force the process currently writing to the display, to sleep. </p><p> This would be a large, but mostly mechanical, change to all the display drivers in the OpenBSD source tree. </p><p> On august 30th, I suggested going in this direction: </p><div id=\"pre\"> <pre>Date: Sun, 30 Aug 2009 20:42:29 +0000 From: Miod Vallat To: private mailing list Subject: wsdisplay asynchronous processing After discussing udl(4) behaviour with mglocker@, we came to the following thoughts which I believe are worth sharing. wscons - and its wsdisplay output part - has been written 10 years ago with tga(4) and vga(4) style bitmapped devices in mind, with synchronous operations. This assumption was true, until we started to support video devices on which the video memory is not directly accessible. Examples of this are cfxga(4), gpx(4) on vax, and udl(4). The first two such drivers have been coerced to fit the synchronous model, because sending commands to the hardware was fast and did only involve the device registers. udl(4), however, is completely alien to this. Commands are sent with USB pipes, there might be other devices on the USB bus, and processing is asynchronous in nature. Now, if you look at wsdisplay, there are two conditions upon which wsdisplay will want to update the display contents: - userland writes to the wsdisplay terminal. wsdisplay is invoked from the tty layer, and has a process context, so it can sleep. - kernel writes to the console device. wsdisplay can not assume anything. So if we are able to flag devices such as udl(4) as not being able to be a console output device, then it will know that it can sleep at the wsdisplay level. Why am I talking about sleeping? Because udl(4) is currently the only wsdisplay(4) driver unable to behave synchronously. In order to complete its operation, it needs to get fifo resources and whatnot, and if none of them are available, there won't be any new resources available until the usb stack has a chance to run: udl(4) needs to sleep. Now if udl(4) is allowed to sleep, and the driver&lt;-&gt;wsdisplay interfaces are modified so that a driver can return <tt>EAGAIN</tt> if it is not a console device, then wsdisplay(4) can sleep. Problem solved. </pre> </div> <p> and since the best way to figure out if such a proposal is worth doing is to write a proof-of-concept, the next day I shared a diff with Marcus. </p><p> But that diff was larger and trickier that what my previous message implied. </p><p> The reason for that, is that the graphics terminal is not simply a \"line printer on a CRT\". <i>(and, writing this in 2025, I suppose some of my younger readers will not be familiar with <a href=\"https://en.wikipedia.org/wiki/Cathode_ray_tube\">Cathode Ray Tube</a> displays)</i> </p><p> An important improvement of CRT displays above line printers is that most CRT terminals have an addressable cursor position; this opens the road towards fancy displays (nowadays with colours!), first to implement <a href=\"https://en.wikipedia.org/wiki/3270_emulator\">IBM 3270 terminal emulators</a>, later for more generic needs, thanks to the <a href=\"https://man.openbsd.org/man5/termcap.5\">termcap</a> and/or <a href=\"https://man.openbsd.org/terminfo.5\">terminfo</a> abstractions, and eventually the <a href=\"https://man.openbsd.org/curses\">curses</a> library. </p><p> The OpenBSD workstation console terminal emulation tries to mimic two well-known console devices, the <a href=\"https://en.wikipedia.org/wiki/VT220\">Digital VT220 terminal</a> (minus several seldom used features difficult to implement cleanly, such as double-width characters) on most platforms, and the Sun console (with extra features such as colour) on <a href=\"https://www.openbsd.org/sparc.html\">sparc</a> and <a href=\"https://www.openbsd.org/sparc64.html\">sparc64</a> ports. (I tried to have the <a href=\"https://www.openbsd.org/luna88k.html\">luna88k</a> port, which defaults to black on white display like the Sun systems, to use the sun terminal emulation, but got outpowered). </p><p> Both these terminal emulations have in common that they recognize specific escape sequences, e.g. to change the cursor position, change output colours, or other specific operations. Also, simply outputting a regular character expands to at least three display operations: hide the cursor, draw the character, and draw the cursor in its new position; and if the character had been the last on the last line, the display needs to scroll, which involves one more operation. </p><p> If display drivers were able to fail any operations, as allowed in my proposal, we could end up with a particular output operation processed only partly. But the tty layer in the kernel has no way to know that a character has been, sort of, \"partially output\". In this layer, a character has either been output, or needs to be output, and nothing in-between. Any finer-grained state needs to be maintained in a lower layer, such as the vt220 emulation code. </p><p> So, in addition to allowing display drivers to fail operations, I wrapped the terminal emulation processing into what I called an \"abort state\", remembering at which point we encountered a failure, so that attempting to output a character would skip the operations which had succeeded already. </p><p> With my changes, a character output would only be \"validated\" if all the display operations it would cause had been successful. If not, the process writing to the display would be forced to sleep, until the display driver reports it is able to process requests again. Processing of the display operations would note which operations had been successful, so as to skip them and only perform the operations which had failed or had no chance to start earlier. </p><div id=\"pre\"> <pre>Date: Mon, 31 Aug 2009 22:00:27 +0000 From: Miod Vallat To: Marcus Glocker Subject: early wscons `ok to stall' diff This should be enough for an i386 or amd64 kernel to compile. Many frame buffer drivers still need to be modified due to interface and prototypes changes. How does it work? Well, this adds an error path from the display driver to the tty layer. So we have this path: 1. void wsdisplay_emulops routines now return values. The driver implementing them can return 0 if it did the work, or nonzero (preferrably EAGAIN) if it couldn't. Note that generic rasops routines, accessing frame buffer memory, never fail and always return zero. 2. the return values of the emulops are now tested in the wsdisplay emulation code. This is the horrible part with a lot of changes, which will need careful review. 3. I designed this so that, when the emulops return an error, the emulation state machine is moved back to a sane state, allowing the operation to be tried again later. This involves undoing logical cursor moves, and other internal state changes. Note the code currently assumes an operation involving several emulops can fail in the middle and be restarted from the beginning - this is wrong, e.g. when scrolling the tty, since we copy rows and then clear the bottom row. If the copy works but the clearing fails, we'll restart with the copy. I am aware of this and working on an `abort state' part of the emulation state machine (which is currently the FALSE_ABORT bit in the per-emulation flags, but needs to move into its own thing). (This is why this is an alpha diff...) 4. The error condition detected by the emulation causes the tty write to abort early. The MI wsdisplay code will now know how many chars have been processed, and if it detected an early abort, it will not try to write more characters, even if there are any left in the tty queue. 5. The same function already has logic to schedule a timeout if more tty data is pending. This timeout will try to feed the driver more work, but until it fires the driver can hopefully get some interrupts and gather resources to do so. What is left to do: 1. Update all MD frame buffer drivers (mechanical). 2. Finish the abort state design and correctly recover from a partial operation. 3. Update udl to return EAGAIN in the emulops. Known problems which won't be fixed soon: 1. Some operations do not come from the tty layer, but by keyboard events (emulation reset sequence), ioctl or timeouts (screen burner). We do not necessarily have a process context there, so sleeping is not an option. I need to extend some interfaces for the affected routines to know whether the caller can recover automatically (tty context), or not, and if not, whether it's ok to sleep or not. 2. There is no way, yet, for the driver which has returned EAGAIN to cause the tty timeout to be triggered earlier (i.e. as soon as it regains resources). I'll think about it eventually. Miod (and now time for some zzz) </pre> </div> <p> The next day, I received the best testimonial ever for a diff: </p><div id=\"pre\"> <pre>&lt;mglock&gt; hi from udl over wscons with initial EAGAIN support :-) &lt;mglock&gt; miod is evil. </pre> </div> <p> After more testing, on september 1st, a new version of that diff was shared: </p><div id=\"pre\"> <pre>Date: Tue, 1 Sep 2009 20:19:02 +0000 From: Miod Vallat To: private mailing list Subject: Re: wsdisplay asynchronous processing In case people are interested, I have a monster diff (which will be split in 4 different pieces), which implements error path from the display drivers up to the tty layer, so that the driver can cause tty output to stop if it is overflowed. The tty layer (well, the wsdisplay tty code) will then nicely recover from this, and everything is fine. This diff has the disavantage of adding about 1KB of code to the kernel (for kernels with vt100 emulation), so this might be a problem for the floppies. And there is no way to disable this if SMALL_KERNEL. Unless you want to dive in your own sea of macro hell filled with sharks. Due the large size of the diff (about 200KB, affecting 51 files), I will not append it to this mail. People interested in it can find it in cvs:~miod/wscons-stall2.vari Note that, for it to be really useful on udl(4) devices - since they are the reason for this work - you need another diff from mglocker@, adding the necessary EAGAIN code in udl(4). $ wc wscons-stall2.vari 7017 26187 194603 wscons-stall2.vari Miod </pre> </div> <p> The monster diff started with the following description: </p><div id=\"pre\"> <pre>This diff is large because many frame buffer drivers need to be modified due to interface and prototype changes. How does it work? Well, this adds an error path from the display driver to the tty layer. So we have this path: 1. void wsdisplay_emulops routines now return values. The driver implementing them can return 0 if it did the work, or nonzero (preferrably EAGAIN or EINTR) if it couldn't. Note that generic rasops routines, accessing frame buffer memory, never fail and always return zero. 2. the return values of the emulops are now tested in the wsdisplay emulation code. This is the horrible part with a lot of changes, which will need careful review. 3. I designed this so that, when the emulops return an error, the emulation state machine is moved back to a sane state, allowing the operation to be tried again later. This involves undoing logical cursor moves, and other internal state changes. Note there might be bugs in the undoing so far, I need to review this carefully. And test too (-: There are comments in wsemulvar.h trying to explain how I keep track of failures occuring in the middle of a `character' (from the tty layer point of view) output. 4. The error condition detected by the emulation causes the tty write to abort early. The MI wsdisplay code will now know how many chars have been processed, and if it detected an early abort, it will not try to write more characters, even if there are any left in the tty queue. 5. The same function already has logic to schedule a timeout if more tty data is pending. This timeout will try to feed the driver more work, but until it fires the driver can hopefully get some interrupts and gather resources to do so. Note that it is not necessary to implement a faster output resume path (e.g. if the driver gets an interrupt and frees resources), as the timeout will run only 8ms later (1/128 second). Keep in mind the human persistance of vision is about 1/25 second, so in the blink of an eye the wsdisplay code can resume stalled output several times. Problems left with this code: 1. You may notice resetop() does not check for emulops failure. This because this is an out-of-tty-layer processing (but ioctl issued to the tty device). I know how to make it able to recover, but this will need a few more emulops prototypes changes, and I would like to keep the focus of this diff minimal (har, har), i.e. trying to only address one problem. A later diff will address that area. 2. The same comments apply to the screen burner code. Again, I have plans for this. How I intend to split this work in individual commits: 1. internal wsemul changes to change the state machine functions from returning u_int to returning void (and updating emul state structs directly), so that they can later be changed again to return errors. (no functional change, but little code growth) 2. emulops prototype changes (and all the rasops / driver part of this diff). (again, no functional change, but little code growth) 3. wsemul_ops change of output() to return the number of chars consumed, and the corresponding logic in wsdisplaystart(), with the emulation code returning the number of chars it has been given (again, no functional change, but little code growth) 4. the error path handling in the emulation code, i.e. the evil part of this diff. </pre> </div> <p> Of course, testing exposed a few bugs in that diff, which led to a new version: </p><div id=\"pre\"> <pre>Date: Wed, 2 Sep 2009 16:17:06 +0000 From: Miod Vallat To: private mailing list Subject: Re: wsdisplay asynchronous processing New diff fixing a few bugs in the previous diff (description of changes at the head of the new diff). cvs:~miod/wscons-stall3.vari $ wc wscons-stall3.vari 7074 26494 196861 wscons-stall23vari Miod </pre> </div> <p> <i>And while writing this, I am dissatisfied with past me. It was not a wise idea to put the details in a file which is now long gone, and I should have put them both in the file and in the email. But I did not think that future me would want to tell this story, years later.</i> </p><p> <i> Fortunately, it turns out that I had sent that diff directly to Marcus minutes later:</i> </p><div id=\"pre\"> <pre>Date: Wed, 2 Sep 2009 16:13:51 +0000 From: Miod Vallat To: Marcus Glocker Subject: latest wscons diff This is diff #3. Changes since last diff: - rearrange changes to wsdisplaystart() so as not to introduce a new goto. - minor simplification in wsemul_sun when backing out state changes because of failed scrollup operation. - rearrange some double-wide array updates in vt100 to be able to correctly recover from operations aborted in the middle. This double width feature is something noone uses anyway. - wrap the COPYCOLS and ERASECOLS operations in vt100 within WSEMULOP so that they will not be reissued by mistake if failure occurs after they're done. This ought to have been in the previous diff but I forgot to do this. </pre> </div> <p> And some time later: </p><div id=\"pre\"> <pre>Date: Thu, 3 Sep 2009 22:00:06 +0000 From: Miod Vallat To: Marcus Glocker Subject: Re: udl Even better, with a diff that still compiles after the untested last minute change. This is diff #4. Changes since last diff: - removed udl.c changes, get them from mglocker@ - fixed an unitialized variable in wsemul_vt100_output() causing cursor image display to sometimes be skipped. - extended the abort state with four different states (ok, failed to display the cursor image, failed to jump scroll, failed a regular operation) instead of two (ok/fail) and having the `fail cursor' a particular state of failure. This allows me to make things a bit more clean (arguably) and two fix two important bugs: + after a regular failure, jump scroll would be attempted before the other operation; if the failure had happened after N operations we would then skip N operations during the jump scroll (usually causing the scroll not to happen, so lines would be overwritten instead of scrolling). + after a jump scroll failure, we need to retry it with the same number of lines as the failed operation, so that the copy/erase parts of the scrolling operation are consistent with each other. </pre> </div> <p> Eventually Marcus confirms it works: </p><div id=\"pre\"> <pre>From: Miod Vallat To: Marcus Glocker Subject: Re: udl &gt; This diff works pretty fine for me! I can't spot any bugs anymore yet, &gt; even when running in a screen session with crazzy apps like irssi and &gt; mutt. Excellent. </pre> </div> <p> Having been able to wrap these operations in specfic macros to hide the note-and-restart logic, this allowed installation media kernels (which would not embed the DisplayLink driver) to use the previous \"nothing can fail\" logic, in order not to grow these kernels and still allow them to fit on 3\"¬Ω floppy installation media. This work eventually got commited on september 5th, <a href=\"https://github.com/openbsd/src/commit/2b541da856699c3b1d730730df6cc39c8c4d0a42\">in</a> <a href=\"https://github.com/openbsd/src/commit/072953e3c89278fb6ae80e298ac40d5bd3f6e394\">multiple</a> <a href=\"https://github.com/openbsd/src/commit/0ba8d49cb814e83f28fa162a6bc07ea9a7d700db\">steps</a> and <a href=\"https://github.com/openbsd/src/commit/345431b5cb528a4ba29ef930fa8b7784131ff6cc\">then</a> <a href=\"https://github.com/openbsd/src/commit/ae9d593cbfe550e923daa0a3eb694eb628de2158\">some</a>... </p><p> ...and udl made use of it <a href=\"https://github.com/openbsd/src/commit/8c7540bbae17d285d3147ca5eddb883d929e2496\">immediately</a>. </p><p> On september 11th, this allowed to nicely fix a <a href=\"https://github.com/openbsd/src/commit/fc15257d66be80ee50687172bf0bd06f4d0d71b7\">diresome situation</a>. </p><p> And since I am only human, a <a href=\"https://github.com/openbsd/src/commit/95ca511cdf8bd172a0789d83d2c5dc6b1974fd15\">small bugfix</a> was needed on the 14th. </p><p> After these changes, there have been no problem reports with DisplayLink devices. </p><p> Later this month, an X server, based upon the \"damage\" extension which describes areas which need to be redrawn, was also added to the OpenBSD source tree, and support for the DisplayLink devices was now complete. </p><p> This allowed \"serial console-only\" platforms such as the <a href=\"https://www.openbsd.org/armish.html\">armish</a> and <a href=\"https://www.openbsd.org/landisk.html\">landisk</a> ports to use a graphics console and run an X server. </p><p> And even if there had been no X server support, the DisplayLink driver forced the console code to face new challenges and solve them in a way which will benefit future drivers. </p><hr> <p> Today, DisplayLink still exists as part of <a href=\"https://www.synaptics.com/products/displaylink-graphics\">Synaptics</a>, and their most recent chips are supported neither by Linux nor by OpenBSD with open source drivers. Synpatics provides a binary driver for Ubuntu, which to this day hasn't been reverse engineered, yet. </p><p> The usefulness of these devices has apparently gone away, people do not seem to be interested by these devices anymore those days. </p></div></div>"},{"id":"https://github.com/DanexCodr/Coderive","title":"Show HN: Coderive ‚Äì Iterating through 1 Quintillion Inside a Loop in just 50ms","link":"https://github.com/DanexCodr/Coderive","hnCommentsUrl":"https://news.ycombinator.com/item?id=46351178","content":"<a href=\"https://news.ycombinator.com/item?id=46351178\">Comments</a>","date":1766376019000,"feedTitle":"Hacker News","isHackerNews":true,"isGoogleNews":false,"fullContent":"<p><a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/DanexCodr/Coderive/main/assets/1762666234889.jpg\"><img src=\"https://raw.githubusercontent.com/DanexCodr/Coderive/main/assets/1762666234889.jpg\" alt=\"Coderive Logo\" width=\"60%\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a> </p> <hr> <h2 tabindex=\"-1\" dir=\"auto\">Table of Contents</h2> <ol dir=\"auto\"> <li><a href=\"#1-introduction\">Introduction</a></li> <li><a href=\"#2-technical-architecture\">Technical Architecture</a></li> <li><a href=\"#3-language-features\">Language Features</a></li> <li><a href=\"#4-performance-validation\">Performance Validation</a></li> <li><a href=\"#5-getting-started\">Getting Started</a></li> <li><a href=\"#6-current-status\">Current Status</a></li> <li><a href=\"#7-license\">License</a></li> <li><a href=\"#8-contact\">Contact</a></li> </ol> <h2 tabindex=\"-1\" dir=\"auto\">1. Introduction</h2> <p dir=\"auto\">We present <strong>Coderive v0.4.0</strong>, a mobile-first general programming language designed for <strong>safe, fast, and clear</strong> coding. Coderive features a parser system written in java (manual recursive backtracking) and dual compilation pipeline (bytecode + native code generation). Built entirely on mobile devices, Coderive proves that serious compiler development can happen outside traditional environments.</p> <p dir=\"auto\"> <a target=\"_blank\" rel=\"noopener noreferrer nofollow\" href=\"https://raw.githubusercontent.com/DanexCodr/Coderive/main/assets/quantifier_estimation.jpg\"><img width=\"90%\" src=\"https://raw.githubusercontent.com/DanexCodr/Coderive/main/assets/quantifier_estimation.jpg\" style=\"max-width: 100%; height: auto; max-height: 300px; object-fit: contain; border-radius: 6px; margin: 10px 0;\"></a> </p> <h2 tabindex=\"-1\" dir=\"auto\">2. Technical Architecture</h2> <hr> <p dir=\"auto\"><strong>Compiler Pipeline: Efficient Code Generation</strong></p> <ul dir=\"auto\"> <li><strong>Dual Compilation:</strong> Simultaneous bytecode and native code generation</li> <li><strong>Multi-Target Support:</strong> ARM64 and x86_64 code generation from single codebase</li> <li><strong>Mobile-First Design:</strong> Built and tested primarily on Android devices</li> </ul> <hr> <p dir=\"auto\"><strong>Development Environment: Constraint-Driven Innovation</strong></p> <p dir=\"auto\">The language was developed under the constraint of mobile-only development:</p> <ul dir=\"auto\"> <li><strong>Java NIDE:</strong> Fast Java 7 compiler for Android</li> <li><strong>Quickedit:</strong> High-performance mobile code editor</li> <li><strong>Termux:</strong> Comprehensive Linux environment</li> <li><strong>AI Assistants:</strong> DeepSeek and Gemini for accelerated debugging</li> </ul> <hr> <h2 tabindex=\"-1\" dir=\"auto\">3. Language Features</h2> <h3 tabindex=\"-1\" dir=\"auto\">Core Innovations</h3> <p dir=\"auto\"><strong>Quantifier-First Logic Design</strong> Coderive replaces traditional boolean operators with expressive quantifiers:</p> <markdown-accessiblity-table><table> <thead> <tr> <th>Traditional</th> <th>Coderive</th> </tr> </thead> <tbody> <tr> <td><code>A &amp;&amp; B &amp;&amp; C</code></td> <td><code>all[A, B, C]</code></td> </tr> <tr> <td><code>A || B || C</code></td> <td><code>any[A, B, C]</code></td> </tr> <tr> <td><code>A &amp;&amp; (B || C)</code></td> <td><code>all[A, any[B, C]]</code></td> </tr> </tbody> </table></markdown-accessiblity-table> <p dir=\"auto\"><strong>Multi-Return Slot System</strong></p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"share Calculator { local calculate(int a, int b) :: result: int, operation: text { ~> a + b, &quot;addition&quot; # Slot assignments } }\"><pre><span>share</span> <span>Calculator</span> { <span>local</span> <span>calculate</span>(<span>int</span> <span>a</span>, <span>int</span> <span>b</span>) :: <span>result</span>: <span>int</span>, <span>operation</span>: <span>text</span> { <span>~</span><span>&gt;</span> <span>a</span> <span>+</span> <span>b</span>, <span>\"addition\"</span> <span># Slot assignments</span> } }</pre></div> <p dir=\"auto\"><strong>Smart For-Loops</strong></p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"for i by *2 in 1 to 10 { # Complex step patterns # Loop body }\"><pre><span>for</span> <span>i</span> <span>by</span> <span>*</span><span>2</span> <span>in</span> <span>1</span> <span>to</span> <span>10</span> { <span># Complex step patterns</span> <span># Loop body</span> }</pre></div> <p dir=\"auto\"><strong>Language Example</strong></p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"unit sample.program get { lang.Math } share InteractiveDemo { local calculate(a: int, b: int, op: text) :: formula: int, operation: text { if all[a >= 0, b >= 0] { if op == any[&quot;+&quot;, &quot;-&quot;, &quot;*&quot;] { ~> a + b, &quot;valid operation&quot; } } ~> 0, &quot;invalid&quot; } }\"><pre><span>unit</span> <span>sample</span>.<span>program</span> <span>get</span> { <span>lang</span>.<span>Math</span> } <span>share</span> <span>InteractiveDemo</span> { <span>local</span> <span>calculate</span>(<span>a</span>: <span>int</span>, <span>b</span>: <span>int</span>, <span>op</span>: <span>text</span>) :: <span>formula</span>: <span>int</span>, <span>operation</span>: <span>text</span> { <span>if</span> <span>all</span>[<span>a</span> <span>&gt;=</span> <span>0</span>, <span>b</span> <span>&gt;=</span> <span>0</span>] { <span>if</span> <span>op</span> <span>==</span> <span>any</span>[<span>\"+\"</span>, <span>\"-\"</span>, <span>\"*\"</span>] { <span>~</span><span>&gt;</span> <span>a</span> <span>+</span> <span>b</span>, <span>\"valid operation\"</span> } } <span>~</span><span>&gt;</span> <span>0</span>, <span>\"invalid\"</span> } }</pre></div> <h2 tabindex=\"-1\" dir=\"auto\">4. Compiler</h2> <p dir=\"auto\">The system has an on going TAC Compiler for efficient execution across both interpreter and native compilation targets.</p> <h2 tabindex=\"-1\" dir=\"auto\">5. Getting Started</h2> <p dir=\"auto\"><strong>System Requirements</strong></p> <p dir=\"auto\">¬∑ Java 7 or later</p> <p dir=\"auto\">¬∑ Linux environment (Termux recommended for mobile)</p> <p dir=\"auto\"><strong>Quick Start</strong></p> <div dir=\"auto\" data-snippet-clipboard-copy-content=\"# Run interpreter java -jar coderive.jar program.cod # Compile to native java -jar coderive.jar --native program.cod\"><pre><span><span>#</span> Run interpreter</span> java -jar coderive.jar program.cod <span><span>#</span> Compile to native</span> java -jar coderive.jar --native program.cod</pre></div> <h2 tabindex=\"-1\" dir=\"auto\">6. Current Status</h2> <div dir=\"auto\"> <markdown-accessiblity-table><table> <thead> <tr> <th>Component</th> <th>Status</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td>Interpreter</td> <td>‚úÖ Complete</td> <td>Full language features</td> </tr> <tr> <td>Native Code Generation</td> <td>‚úÖ Complete</td> <td>ARM64/x86_64 support</td> </tr> <tr> <td>TAC Compiler</td> <td>üîß In Progress</td> <td>Enhanced implementation</td> </tr> </tbody> </table></markdown-accessiblity-table> </div> <h2 tabindex=\"-1\" dir=\"auto\">7. License</h2> <p dir=\"auto\">This project is licensed under the <a href=\"https://github.com/DanexCodr/Coderive/blob/main/LICENSE\">MIT License</a>.</p> <h2 tabindex=\"-1\" dir=\"auto\">8. Contact</h2> <p dir=\"auto\">Have questions or want to contribute?</p> <p dir=\"auto\">Join our community discussions:</p> <p dir=\"auto\">¬∑ <a href=\"https://github.com/DanexCodr/Coderive/discussions\">GitHub Discussions</a> - Ask questions and share ideas</p> <p dir=\"auto\">¬∑ <a href=\"https://github.com/DanexCodr/Coderive/issues\">GitHub Issues</a> - Report bugs and problems</p> <p dir=\"auto\">¬∑ Developer's Email: <a href=\"mailto:danisonnunez001@gmail.com\">danisonnunez001@gmail.com</a></p> <hr> <p><em>Built with passion on mobile devices ‚Äî proving innovation knows no hardware boundaries.</em> </p>"}]}